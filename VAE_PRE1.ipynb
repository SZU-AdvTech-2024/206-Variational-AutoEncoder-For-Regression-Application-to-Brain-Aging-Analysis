{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5923520-3d86-47ff-ad8a-55df4232cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.models as models\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59793afc-4ded-4135-8384-df65a139cc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Classes: ['-250', '-249', '-248', '-247', '-246', '-245', '-244', '-243', '-242', '-241', '-240', '-239', '-238', '-237', '-236', '-235', '-234', '-233', '-232', '-231', '-230', '-229', '-228', '-227', '-226', '-225', '-224', '-223', '-222', '-221', '-220', '-219', '-218', '-217', '-216', '-215', '-214', '-213', '-212', '-211', '-210', '-209', '-208', '-207', '-206', '-205', '-204', '-203', '-202', '-201', '-200', '-199', '-198', '-197', '-196', '-195', '-194', '-193', '-192', '-191', '-190', '-189', '-188', '-187', '-186', '-185', '-184', '-183', '-182', '-181', '-180', '-179', '-178', '-177', '-176', '-175', '-174', '-173', '-172', '-171', '-170', '-169', '-168', '-167', '-166', '-165', '-164', '-163', '-162', '-161', '-160', '-159', '-158', '-157', '-156', '-155', '-154', '-153', '-152', '-151', '-150', '-149', '-148', '-147', '-146', '-145', '-144', '-143', '-142', '-141', '-140', '-139', '-138', '-137', '-136', '-135', '-134', '-133', '-132', '-131', '-130', '-129', '-128', '-127', '-126', '-125', '-124', '-123', '-122', '-121', '-120', '-119', '-118', '-117', '-116', '-115', '-114', '-113', '-112', '-111', '-110', '-109', '-108', '-107', '-106', '-105', '-104', '-103', '-102', '-101', '-100', '-99', '-98', '-97', '-96', '-95', '-94', '-93', '-92', '-91', '-90', '-89', '-88', '-87', '-86', '-85', '-84', '-83', '-82', '-81', '-80', '-79', '-78', '-77', '-76', '-75', '-74', '-73', '-72', '-71', '-70', '-69', '-68', '-67', '-66', '-65', '-64', '-63', '-62', '-61', '-60', '-59', '-58', '-57', '-56', '-55', '-54', '-53', '-52', '-51', '-50', '-49', '-48', '-47', '-46', '-45', '-44', '-43', '-42', '-41', '-40', '-39', '-38', '-37', '-36', '-35', '-34', '-33', '-32', '-31', '-30', '-29', '-28', '-27', '-26', '-25', '-24', '-23', '-22', '-21', '-20', '-19', '-18', '-17', '-16', '-15', '-14', '-13', '-12', '-11', '-10', '-9', '-8', '-7', '-6', '-5', '-4', '-3', '-2', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250']\n",
      "训练集大小: 20040\n",
      "测试集大小: 5010\n"
     ]
    }
   ],
   "source": [
    "# 设置数据集路径\n",
    "total_dir = \"/Users/fcccasa/Downloads/R/img\"\n",
    "\n",
    "# 定义数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "total_data = datasets.ImageFolder(total_dir, transform)\n",
    "\n",
    "# 获取类别并按数字顺序排序\n",
    "def sort_key(class_name):\n",
    "    # 尝试将类别名称转换为整数，若失败则返回原字符串\n",
    "    try:\n",
    "        return int(class_name)  # 如果类别是数字，则按数字排序\n",
    "    except ValueError:\n",
    "        return class_name  # 如果类别是非数字，则按字母排序\n",
    "\n",
    "sorted_classes = sorted(total_data.classes, key=sort_key)\n",
    "\n",
    "# 更新 ImageFolder 的 class_to_idx 字典\n",
    "total_data.class_to_idx = {cls: idx for idx, cls in enumerate(sorted_classes)}\n",
    "\n",
    "print(\"Sorted Classes:\", sorted_classes)\n",
    "\n",
    "# 使用排序后的类名创建训练集和测试集索引\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# 遍历数据集中的每个类别（文件夹）\n",
    "for class_idx in range(len(sorted_classes)):\n",
    "    # 获取当前类别所有图像的索引\n",
    "    class_indices = [i for i, label in enumerate(total_data.targets) if label == class_idx]\n",
    "    \n",
    "    # 分割：前40个图像为训练集，剩下的为测试集\n",
    "    train_indices.extend(class_indices[:40])\n",
    "    test_indices.extend(class_indices[40:])\n",
    "\n",
    "# 使用索引创建训练集和测试集\n",
    "train_dataset = torch.utils.data.Subset(total_data, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(total_data, test_indices)\n",
    "\n",
    "# 打印训练集和测试集的大小\n",
    "print(f'训练集大小: {len(train_dataset)}')\n",
    "print(f'测试集大小: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f864c727-bb31-43c5-988d-62be8f0aaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=40, shuffle=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "667388f8-0d51-4394-8237-4a54da9ad25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "num_classes = 1\n",
    "\n",
    "MAX_EPOCH = 20     \n",
    "LR = 0.001         \n",
    "log_interval = 10    # 每隔 1 个 训练批次（或 epoch）记录一次训练日志。 表示训练过程中，控制打印训练信息的频率。\n",
    "val_interval = 1    # 每隔 1 个 epoch 进行一次验证集的评估。验证集的评估是为了监控模型在验证集上的表现，从而避免过拟合。\n",
    "classes = 1         # 分类任务的类别数为 2。\n",
    "start_epoch = -1     \n",
    "lr_decay_step = 10   # 表示学习率的衰减步长。 每隔一定的 epoch（如 1）对学习率进行衰减，通常是为了使模型在后期更稳定地收敛。\n",
    "                    # 可结合优化器的学习率调度策略（如 StepLR）使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbfc89b8-b6c6-45cb-9819-57384df65e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = device = torch.device(\"mps\")\n",
    "res_model = models.resnet18(pretrained=True)  # 使用预训练权重\n",
    "# 获取 ResNet 的最后一层输入特征数\n",
    "num_ftrs = res_model.fc.in_features  # 获取fc层的输入特征数\n",
    "res_model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),           # 将输入特征维度映射到4096\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)                    # 输出为1，进行回归预测任务\n",
    ")\n",
    "print(num_ftrs)\n",
    "res_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b42f2e5-0b44-4c2c-bbac-1f7e28a75212",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "140f3afe-9016-403c-826f-d100780e275d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(res_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b27dec8-da9c-445d-a202-2ace741bceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(48227) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[000/020] Iteration[010/501] Loss: 238.8234\n",
      "Training: Epoch[000/020] Iteration[020/501] Loss: 208.3223\n",
      "Training: Epoch[000/020] Iteration[030/501] Loss: 187.8736\n",
      "Training: Epoch[000/020] Iteration[040/501] Loss: 146.2350\n",
      "Training: Epoch[000/020] Iteration[050/501] Loss: 81.2386\n",
      "Training: Epoch[000/020] Iteration[060/501] Loss: 50.6772\n",
      "Training: Epoch[000/020] Iteration[070/501] Loss: 43.5607\n",
      "Training: Epoch[000/020] Iteration[080/501] Loss: 45.5455\n",
      "Training: Epoch[000/020] Iteration[090/501] Loss: 40.0129\n",
      "Training: Epoch[000/020] Iteration[100/501] Loss: 34.9232\n",
      "Training: Epoch[000/020] Iteration[110/501] Loss: 32.0094\n",
      "Training: Epoch[000/020] Iteration[120/501] Loss: 38.4783\n",
      "Training: Epoch[000/020] Iteration[130/501] Loss: 32.1578\n",
      "Training: Epoch[000/020] Iteration[140/501] Loss: 30.7788\n",
      "Training: Epoch[000/020] Iteration[150/501] Loss: 33.4160\n",
      "Training: Epoch[000/020] Iteration[160/501] Loss: 26.5570\n",
      "Training: Epoch[000/020] Iteration[170/501] Loss: 23.9600\n",
      "Training: Epoch[000/020] Iteration[180/501] Loss: 27.8293\n",
      "Training: Epoch[000/020] Iteration[190/501] Loss: 31.1418\n",
      "Training: Epoch[000/020] Iteration[200/501] Loss: 32.0184\n",
      "Training: Epoch[000/020] Iteration[210/501] Loss: 26.8587\n",
      "Training: Epoch[000/020] Iteration[220/501] Loss: 28.1053\n",
      "Training: Epoch[000/020] Iteration[230/501] Loss: 24.3804\n",
      "Training: Epoch[000/020] Iteration[240/501] Loss: 25.7295\n",
      "Training: Epoch[000/020] Iteration[250/501] Loss: 28.8126\n",
      "Training: Epoch[000/020] Iteration[260/501] Loss: 29.4857\n",
      "Training: Epoch[000/020] Iteration[270/501] Loss: 21.5771\n",
      "Training: Epoch[000/020] Iteration[280/501] Loss: 25.5008\n",
      "Training: Epoch[000/020] Iteration[290/501] Loss: 27.2795\n",
      "Training: Epoch[000/020] Iteration[300/501] Loss: 31.6328\n",
      "Training: Epoch[000/020] Iteration[310/501] Loss: 25.9707\n",
      "Training: Epoch[000/020] Iteration[320/501] Loss: 26.7169\n",
      "Training: Epoch[000/020] Iteration[330/501] Loss: 25.6471\n",
      "Training: Epoch[000/020] Iteration[340/501] Loss: 29.6686\n",
      "Training: Epoch[000/020] Iteration[350/501] Loss: 28.9804\n",
      "Training: Epoch[000/020] Iteration[360/501] Loss: 24.2977\n",
      "Training: Epoch[000/020] Iteration[370/501] Loss: 29.0414\n",
      "Training: Epoch[000/020] Iteration[380/501] Loss: 29.0803\n",
      "Training: Epoch[000/020] Iteration[390/501] Loss: 23.7049\n",
      "Training: Epoch[000/020] Iteration[400/501] Loss: 27.4044\n",
      "Training: Epoch[000/020] Iteration[410/501] Loss: 23.2604\n",
      "Training: Epoch[000/020] Iteration[420/501] Loss: 26.7032\n",
      "Training: Epoch[000/020] Iteration[430/501] Loss: 19.9617\n",
      "Training: Epoch[000/020] Iteration[440/501] Loss: 23.4722\n",
      "Training: Epoch[000/020] Iteration[450/501] Loss: 23.3463\n",
      "Training: Epoch[000/020] Iteration[460/501] Loss: 27.2488\n",
      "Training: Epoch[000/020] Iteration[470/501] Loss: 27.4930\n",
      "Training: Epoch[000/020] Iteration[480/501] Loss: 26.7887\n",
      "Training: Epoch[000/020] Iteration[490/501] Loss: 21.4201\n",
      "Training: Epoch[000/020] Iteration[500/501] Loss: 21.5123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(48800) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[000/020] Iteration[501/501] Loss: 20.9481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(48897) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[001/020] Iteration[010/501] Loss: 22.0532\n",
      "Training: Epoch[001/020] Iteration[020/501] Loss: 27.7960\n",
      "Training: Epoch[001/020] Iteration[030/501] Loss: 18.9813\n",
      "Training: Epoch[001/020] Iteration[040/501] Loss: 25.1819\n",
      "Training: Epoch[001/020] Iteration[050/501] Loss: 20.7939\n",
      "Training: Epoch[001/020] Iteration[060/501] Loss: 22.0902\n",
      "Training: Epoch[001/020] Iteration[070/501] Loss: 18.3003\n",
      "Training: Epoch[001/020] Iteration[080/501] Loss: 21.3319\n",
      "Training: Epoch[001/020] Iteration[090/501] Loss: 25.1870\n",
      "Training: Epoch[001/020] Iteration[100/501] Loss: 24.5501\n",
      "Training: Epoch[001/020] Iteration[110/501] Loss: 25.8687\n",
      "Training: Epoch[001/020] Iteration[120/501] Loss: 22.0448\n",
      "Training: Epoch[001/020] Iteration[130/501] Loss: 20.6532\n",
      "Training: Epoch[001/020] Iteration[140/501] Loss: 23.2834\n",
      "Training: Epoch[001/020] Iteration[150/501] Loss: 26.0089\n",
      "Training: Epoch[001/020] Iteration[160/501] Loss: 21.1507\n",
      "Training: Epoch[001/020] Iteration[170/501] Loss: 22.9160\n",
      "Training: Epoch[001/020] Iteration[180/501] Loss: 18.2804\n",
      "Training: Epoch[001/020] Iteration[190/501] Loss: 17.8350\n",
      "Training: Epoch[001/020] Iteration[200/501] Loss: 16.6834\n",
      "Training: Epoch[001/020] Iteration[210/501] Loss: 17.7804\n",
      "Training: Epoch[001/020] Iteration[220/501] Loss: 18.9396\n",
      "Training: Epoch[001/020] Iteration[230/501] Loss: 15.6780\n",
      "Training: Epoch[001/020] Iteration[240/501] Loss: 21.8717\n",
      "Training: Epoch[001/020] Iteration[250/501] Loss: 17.2012\n",
      "Training: Epoch[001/020] Iteration[260/501] Loss: 18.5241\n",
      "Training: Epoch[001/020] Iteration[270/501] Loss: 18.7844\n",
      "Training: Epoch[001/020] Iteration[280/501] Loss: 16.8035\n",
      "Training: Epoch[001/020] Iteration[290/501] Loss: 14.9670\n",
      "Training: Epoch[001/020] Iteration[300/501] Loss: 13.7668\n",
      "Training: Epoch[001/020] Iteration[310/501] Loss: 18.5072\n",
      "Training: Epoch[001/020] Iteration[320/501] Loss: 20.7428\n",
      "Training: Epoch[001/020] Iteration[330/501] Loss: 16.2738\n",
      "Training: Epoch[001/020] Iteration[340/501] Loss: 13.4987\n",
      "Training: Epoch[001/020] Iteration[350/501] Loss: 17.6226\n",
      "Training: Epoch[001/020] Iteration[360/501] Loss: 15.8784\n",
      "Training: Epoch[001/020] Iteration[370/501] Loss: 16.3523\n",
      "Training: Epoch[001/020] Iteration[380/501] Loss: 17.0743\n",
      "Training: Epoch[001/020] Iteration[390/501] Loss: 19.4752\n",
      "Training: Epoch[001/020] Iteration[400/501] Loss: 17.6825\n",
      "Training: Epoch[001/020] Iteration[410/501] Loss: 17.2940\n",
      "Training: Epoch[001/020] Iteration[420/501] Loss: 14.1978\n",
      "Training: Epoch[001/020] Iteration[430/501] Loss: 16.6154\n",
      "Training: Epoch[001/020] Iteration[440/501] Loss: 20.8419\n",
      "Training: Epoch[001/020] Iteration[450/501] Loss: 17.8782\n",
      "Training: Epoch[001/020] Iteration[460/501] Loss: 18.4681\n",
      "Training: Epoch[001/020] Iteration[470/501] Loss: 16.8598\n",
      "Training: Epoch[001/020] Iteration[480/501] Loss: 17.2151\n",
      "Training: Epoch[001/020] Iteration[490/501] Loss: 14.6611\n",
      "Training: Epoch[001/020] Iteration[500/501] Loss: 17.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49521) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[001/020] Iteration[501/501] Loss: 16.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(49587) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[002/020] Iteration[010/501] Loss: 15.4716\n",
      "Training: Epoch[002/020] Iteration[020/501] Loss: 16.9876\n",
      "Training: Epoch[002/020] Iteration[030/501] Loss: 16.4757\n",
      "Training: Epoch[002/020] Iteration[040/501] Loss: 13.3934\n",
      "Training: Epoch[002/020] Iteration[050/501] Loss: 19.9617\n",
      "Training: Epoch[002/020] Iteration[060/501] Loss: 16.5978\n",
      "Training: Epoch[002/020] Iteration[070/501] Loss: 17.4290\n",
      "Training: Epoch[002/020] Iteration[080/501] Loss: 19.2206\n",
      "Training: Epoch[002/020] Iteration[090/501] Loss: 15.3631\n",
      "Training: Epoch[002/020] Iteration[100/501] Loss: 16.6155\n",
      "Training: Epoch[002/020] Iteration[110/501] Loss: 16.4806\n",
      "Training: Epoch[002/020] Iteration[120/501] Loss: 18.0277\n",
      "Training: Epoch[002/020] Iteration[130/501] Loss: 14.2841\n",
      "Training: Epoch[002/020] Iteration[140/501] Loss: 16.1954\n",
      "Training: Epoch[002/020] Iteration[150/501] Loss: 14.4718\n",
      "Training: Epoch[002/020] Iteration[160/501] Loss: 14.1210\n",
      "Training: Epoch[002/020] Iteration[170/501] Loss: 14.9896\n",
      "Training: Epoch[002/020] Iteration[180/501] Loss: 16.2815\n",
      "Training: Epoch[002/020] Iteration[190/501] Loss: 14.6725\n",
      "Training: Epoch[002/020] Iteration[200/501] Loss: 14.4633\n",
      "Training: Epoch[002/020] Iteration[210/501] Loss: 14.4146\n",
      "Training: Epoch[002/020] Iteration[220/501] Loss: 17.4815\n",
      "Training: Epoch[002/020] Iteration[230/501] Loss: 18.3695\n",
      "Training: Epoch[002/020] Iteration[240/501] Loss: 11.8917\n",
      "Training: Epoch[002/020] Iteration[250/501] Loss: 15.7969\n",
      "Training: Epoch[002/020] Iteration[260/501] Loss: 15.5166\n",
      "Training: Epoch[002/020] Iteration[270/501] Loss: 15.2443\n",
      "Training: Epoch[002/020] Iteration[280/501] Loss: 16.2618\n",
      "Training: Epoch[002/020] Iteration[290/501] Loss: 18.8561\n",
      "Training: Epoch[002/020] Iteration[300/501] Loss: 15.2459\n",
      "Training: Epoch[002/020] Iteration[310/501] Loss: 16.7419\n",
      "Training: Epoch[002/020] Iteration[320/501] Loss: 14.2188\n",
      "Training: Epoch[002/020] Iteration[330/501] Loss: 15.5419\n",
      "Training: Epoch[002/020] Iteration[340/501] Loss: 12.7659\n",
      "Training: Epoch[002/020] Iteration[350/501] Loss: 13.8223\n",
      "Training: Epoch[002/020] Iteration[360/501] Loss: 13.6913\n",
      "Training: Epoch[002/020] Iteration[370/501] Loss: 12.0388\n",
      "Training: Epoch[002/020] Iteration[380/501] Loss: 13.7280\n",
      "Training: Epoch[002/020] Iteration[390/501] Loss: 13.1076\n",
      "Training: Epoch[002/020] Iteration[400/501] Loss: 13.9981\n",
      "Training: Epoch[002/020] Iteration[410/501] Loss: 15.2825\n",
      "Training: Epoch[002/020] Iteration[420/501] Loss: 14.9880\n",
      "Training: Epoch[002/020] Iteration[430/501] Loss: 11.4524\n",
      "Training: Epoch[002/020] Iteration[440/501] Loss: 13.2887\n",
      "Training: Epoch[002/020] Iteration[450/501] Loss: 13.6757\n",
      "Training: Epoch[002/020] Iteration[460/501] Loss: 15.2680\n",
      "Training: Epoch[002/020] Iteration[470/501] Loss: 14.1080\n",
      "Training: Epoch[002/020] Iteration[480/501] Loss: 15.3837\n",
      "Training: Epoch[002/020] Iteration[490/501] Loss: 15.5288\n",
      "Training: Epoch[002/020] Iteration[500/501] Loss: 15.3464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(50121) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[002/020] Iteration[501/501] Loss: 14.7543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(50184) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[003/020] Iteration[010/501] Loss: 13.8210\n",
      "Training: Epoch[003/020] Iteration[020/501] Loss: 16.5366\n",
      "Training: Epoch[003/020] Iteration[030/501] Loss: 17.1545\n",
      "Training: Epoch[003/020] Iteration[040/501] Loss: 12.2355\n",
      "Training: Epoch[003/020] Iteration[050/501] Loss: 14.7445\n",
      "Training: Epoch[003/020] Iteration[060/501] Loss: 12.2521\n",
      "Training: Epoch[003/020] Iteration[070/501] Loss: 13.1738\n",
      "Training: Epoch[003/020] Iteration[080/501] Loss: 11.8454\n",
      "Training: Epoch[003/020] Iteration[090/501] Loss: 11.4668\n",
      "Training: Epoch[003/020] Iteration[100/501] Loss: 15.9831\n",
      "Training: Epoch[003/020] Iteration[110/501] Loss: 14.0516\n",
      "Training: Epoch[003/020] Iteration[120/501] Loss: 13.9923\n",
      "Training: Epoch[003/020] Iteration[130/501] Loss: 13.0644\n",
      "Training: Epoch[003/020] Iteration[140/501] Loss: 11.0713\n",
      "Training: Epoch[003/020] Iteration[150/501] Loss: 12.9820\n",
      "Training: Epoch[003/020] Iteration[160/501] Loss: 10.6824\n",
      "Training: Epoch[003/020] Iteration[170/501] Loss: 14.0987\n",
      "Training: Epoch[003/020] Iteration[180/501] Loss: 12.5978\n",
      "Training: Epoch[003/020] Iteration[190/501] Loss: 15.2074\n",
      "Training: Epoch[003/020] Iteration[200/501] Loss: 13.3905\n",
      "Training: Epoch[003/020] Iteration[210/501] Loss: 9.6785\n",
      "Training: Epoch[003/020] Iteration[220/501] Loss: 10.5016\n",
      "Training: Epoch[003/020] Iteration[230/501] Loss: 14.2317\n",
      "Training: Epoch[003/020] Iteration[240/501] Loss: 14.1472\n",
      "Training: Epoch[003/020] Iteration[250/501] Loss: 13.7170\n",
      "Training: Epoch[003/020] Iteration[260/501] Loss: 11.5263\n",
      "Training: Epoch[003/020] Iteration[270/501] Loss: 13.2296\n",
      "Training: Epoch[003/020] Iteration[280/501] Loss: 14.1226\n",
      "Training: Epoch[003/020] Iteration[290/501] Loss: 12.5780\n",
      "Training: Epoch[003/020] Iteration[300/501] Loss: 12.8347\n",
      "Training: Epoch[003/020] Iteration[310/501] Loss: 11.5552\n",
      "Training: Epoch[003/020] Iteration[320/501] Loss: 14.5650\n",
      "Training: Epoch[003/020] Iteration[330/501] Loss: 10.5293\n",
      "Training: Epoch[003/020] Iteration[340/501] Loss: 10.4800\n",
      "Training: Epoch[003/020] Iteration[350/501] Loss: 12.2214\n",
      "Training: Epoch[003/020] Iteration[360/501] Loss: 12.2765\n",
      "Training: Epoch[003/020] Iteration[370/501] Loss: 13.0863\n",
      "Training: Epoch[003/020] Iteration[380/501] Loss: 13.3163\n",
      "Training: Epoch[003/020] Iteration[390/501] Loss: 13.5429\n",
      "Training: Epoch[003/020] Iteration[400/501] Loss: 15.4400\n",
      "Training: Epoch[003/020] Iteration[410/501] Loss: 11.0617\n",
      "Training: Epoch[003/020] Iteration[420/501] Loss: 13.9447\n",
      "Training: Epoch[003/020] Iteration[430/501] Loss: 12.3877\n",
      "Training: Epoch[003/020] Iteration[440/501] Loss: 14.3823\n",
      "Training: Epoch[003/020] Iteration[450/501] Loss: 12.3181\n",
      "Training: Epoch[003/020] Iteration[460/501] Loss: 14.3778\n",
      "Training: Epoch[003/020] Iteration[470/501] Loss: 13.6340\n",
      "Training: Epoch[003/020] Iteration[480/501] Loss: 10.9610\n",
      "Training: Epoch[003/020] Iteration[490/501] Loss: 11.3593\n",
      "Training: Epoch[003/020] Iteration[500/501] Loss: 11.9684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(50753) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[003/020] Iteration[501/501] Loss: 12.7043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(50814) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[004/020] Iteration[010/501] Loss: 11.7257\n",
      "Training: Epoch[004/020] Iteration[020/501] Loss: 7.7379\n",
      "Training: Epoch[004/020] Iteration[030/501] Loss: 11.4048\n",
      "Training: Epoch[004/020] Iteration[040/501] Loss: 11.2533\n",
      "Training: Epoch[004/020] Iteration[050/501] Loss: 10.9073\n",
      "Training: Epoch[004/020] Iteration[060/501] Loss: 10.8476\n",
      "Training: Epoch[004/020] Iteration[070/501] Loss: 11.4002\n",
      "Training: Epoch[004/020] Iteration[080/501] Loss: 11.9448\n",
      "Training: Epoch[004/020] Iteration[090/501] Loss: 13.1945\n",
      "Training: Epoch[004/020] Iteration[100/501] Loss: 11.4955\n",
      "Training: Epoch[004/020] Iteration[110/501] Loss: 12.6401\n",
      "Training: Epoch[004/020] Iteration[120/501] Loss: 10.2714\n",
      "Training: Epoch[004/020] Iteration[130/501] Loss: 11.4772\n",
      "Training: Epoch[004/020] Iteration[140/501] Loss: 10.4336\n",
      "Training: Epoch[004/020] Iteration[150/501] Loss: 14.3970\n",
      "Training: Epoch[004/020] Iteration[160/501] Loss: 13.4623\n",
      "Training: Epoch[004/020] Iteration[170/501] Loss: 11.9761\n",
      "Training: Epoch[004/020] Iteration[180/501] Loss: 14.6404\n",
      "Training: Epoch[004/020] Iteration[190/501] Loss: 13.0590\n",
      "Training: Epoch[004/020] Iteration[200/501] Loss: 12.2503\n",
      "Training: Epoch[004/020] Iteration[210/501] Loss: 12.4554\n",
      "Training: Epoch[004/020] Iteration[220/501] Loss: 13.1355\n",
      "Training: Epoch[004/020] Iteration[230/501] Loss: 10.8101\n",
      "Training: Epoch[004/020] Iteration[240/501] Loss: 12.7018\n",
      "Training: Epoch[004/020] Iteration[250/501] Loss: 10.2599\n",
      "Training: Epoch[004/020] Iteration[260/501] Loss: 9.1618\n",
      "Training: Epoch[004/020] Iteration[270/501] Loss: 9.4096\n",
      "Training: Epoch[004/020] Iteration[280/501] Loss: 10.1332\n",
      "Training: Epoch[004/020] Iteration[290/501] Loss: 10.3258\n",
      "Training: Epoch[004/020] Iteration[300/501] Loss: 11.3689\n",
      "Training: Epoch[004/020] Iteration[310/501] Loss: 13.2091\n",
      "Training: Epoch[004/020] Iteration[320/501] Loss: 11.5239\n",
      "Training: Epoch[004/020] Iteration[330/501] Loss: 13.2336\n",
      "Training: Epoch[004/020] Iteration[340/501] Loss: 11.7690\n",
      "Training: Epoch[004/020] Iteration[350/501] Loss: 12.3049\n",
      "Training: Epoch[004/020] Iteration[360/501] Loss: 12.1110\n",
      "Training: Epoch[004/020] Iteration[370/501] Loss: 13.1996\n",
      "Training: Epoch[004/020] Iteration[380/501] Loss: 10.6718\n",
      "Training: Epoch[004/020] Iteration[390/501] Loss: 11.3361\n",
      "Training: Epoch[004/020] Iteration[400/501] Loss: 10.5410\n",
      "Training: Epoch[004/020] Iteration[410/501] Loss: 13.2638\n",
      "Training: Epoch[004/020] Iteration[420/501] Loss: 12.5483\n",
      "Training: Epoch[004/020] Iteration[430/501] Loss: 11.8325\n",
      "Training: Epoch[004/020] Iteration[440/501] Loss: 10.6531\n",
      "Training: Epoch[004/020] Iteration[450/501] Loss: 9.0701\n",
      "Training: Epoch[004/020] Iteration[460/501] Loss: 12.1961\n",
      "Training: Epoch[004/020] Iteration[470/501] Loss: 11.7380\n",
      "Training: Epoch[004/020] Iteration[480/501] Loss: 13.4583\n",
      "Training: Epoch[004/020] Iteration[490/501] Loss: 11.2895\n",
      "Training: Epoch[004/020] Iteration[500/501] Loss: 11.3410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(51314) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[004/020] Iteration[501/501] Loss: 12.5175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(51378) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[005/020] Iteration[010/501] Loss: 12.6548\n",
      "Training: Epoch[005/020] Iteration[020/501] Loss: 9.8428\n",
      "Training: Epoch[005/020] Iteration[030/501] Loss: 7.2569\n",
      "Training: Epoch[005/020] Iteration[040/501] Loss: 12.8381\n",
      "Training: Epoch[005/020] Iteration[050/501] Loss: 15.2437\n",
      "Training: Epoch[005/020] Iteration[060/501] Loss: 13.4848\n",
      "Training: Epoch[005/020] Iteration[070/501] Loss: 18.0235\n",
      "Training: Epoch[005/020] Iteration[080/501] Loss: 14.5941\n",
      "Training: Epoch[005/020] Iteration[090/501] Loss: 11.5216\n",
      "Training: Epoch[005/020] Iteration[100/501] Loss: 10.1568\n",
      "Training: Epoch[005/020] Iteration[110/501] Loss: 10.9613\n",
      "Training: Epoch[005/020] Iteration[120/501] Loss: 9.1112\n",
      "Training: Epoch[005/020] Iteration[130/501] Loss: 11.2066\n",
      "Training: Epoch[005/020] Iteration[140/501] Loss: 12.5009\n",
      "Training: Epoch[005/020] Iteration[150/501] Loss: 12.7571\n",
      "Training: Epoch[005/020] Iteration[160/501] Loss: 10.1717\n",
      "Training: Epoch[005/020] Iteration[170/501] Loss: 11.1188\n",
      "Training: Epoch[005/020] Iteration[180/501] Loss: 13.0450\n",
      "Training: Epoch[005/020] Iteration[190/501] Loss: 15.7824\n",
      "Training: Epoch[005/020] Iteration[200/501] Loss: 12.4548\n",
      "Training: Epoch[005/020] Iteration[210/501] Loss: 13.3296\n",
      "Training: Epoch[005/020] Iteration[220/501] Loss: 8.7856\n",
      "Training: Epoch[005/020] Iteration[230/501] Loss: 9.3331\n",
      "Training: Epoch[005/020] Iteration[240/501] Loss: 6.6306\n",
      "Training: Epoch[005/020] Iteration[250/501] Loss: 9.2873\n",
      "Training: Epoch[005/020] Iteration[260/501] Loss: 6.7805\n",
      "Training: Epoch[005/020] Iteration[270/501] Loss: 9.8428\n",
      "Training: Epoch[005/020] Iteration[280/501] Loss: 17.1860\n",
      "Training: Epoch[005/020] Iteration[290/501] Loss: 10.2019\n",
      "Training: Epoch[005/020] Iteration[300/501] Loss: 9.9368\n",
      "Training: Epoch[005/020] Iteration[310/501] Loss: 9.4411\n",
      "Training: Epoch[005/020] Iteration[320/501] Loss: 11.2167\n",
      "Training: Epoch[005/020] Iteration[330/501] Loss: 10.1647\n",
      "Training: Epoch[005/020] Iteration[340/501] Loss: 9.8902\n",
      "Training: Epoch[005/020] Iteration[350/501] Loss: 8.1900\n",
      "Training: Epoch[005/020] Iteration[360/501] Loss: 9.8370\n",
      "Training: Epoch[005/020] Iteration[370/501] Loss: 11.8766\n",
      "Training: Epoch[005/020] Iteration[380/501] Loss: 14.4094\n",
      "Training: Epoch[005/020] Iteration[390/501] Loss: 9.8236\n",
      "Training: Epoch[005/020] Iteration[400/501] Loss: 10.6306\n",
      "Training: Epoch[005/020] Iteration[410/501] Loss: 10.2058\n",
      "Training: Epoch[005/020] Iteration[420/501] Loss: 11.1115\n",
      "Training: Epoch[005/020] Iteration[430/501] Loss: 9.7611\n",
      "Training: Epoch[005/020] Iteration[440/501] Loss: 10.5685\n",
      "Training: Epoch[005/020] Iteration[450/501] Loss: 13.8831\n",
      "Training: Epoch[005/020] Iteration[460/501] Loss: 11.2707\n",
      "Training: Epoch[005/020] Iteration[470/501] Loss: 11.2847\n",
      "Training: Epoch[005/020] Iteration[480/501] Loss: 11.3283\n",
      "Training: Epoch[005/020] Iteration[490/501] Loss: 9.9181\n",
      "Training: Epoch[005/020] Iteration[500/501] Loss: 11.6762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(51850) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[005/020] Iteration[501/501] Loss: 10.8250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(51921) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[006/020] Iteration[010/501] Loss: 10.8015\n",
      "Training: Epoch[006/020] Iteration[020/501] Loss: 9.1599\n",
      "Training: Epoch[006/020] Iteration[030/501] Loss: 11.9721\n",
      "Training: Epoch[006/020] Iteration[040/501] Loss: 10.8907\n",
      "Training: Epoch[006/020] Iteration[050/501] Loss: 9.4159\n",
      "Training: Epoch[006/020] Iteration[060/501] Loss: 9.0384\n",
      "Training: Epoch[006/020] Iteration[070/501] Loss: 7.9468\n",
      "Training: Epoch[006/020] Iteration[080/501] Loss: 10.9773\n",
      "Training: Epoch[006/020] Iteration[090/501] Loss: 9.8638\n",
      "Training: Epoch[006/020] Iteration[100/501] Loss: 9.4479\n",
      "Training: Epoch[006/020] Iteration[110/501] Loss: 9.2185\n",
      "Training: Epoch[006/020] Iteration[120/501] Loss: 9.7100\n",
      "Training: Epoch[006/020] Iteration[130/501] Loss: 9.6613\n",
      "Training: Epoch[006/020] Iteration[140/501] Loss: 6.9416\n",
      "Training: Epoch[006/020] Iteration[150/501] Loss: 9.5139\n",
      "Training: Epoch[006/020] Iteration[160/501] Loss: 12.2623\n",
      "Training: Epoch[006/020] Iteration[170/501] Loss: 9.9908\n",
      "Training: Epoch[006/020] Iteration[180/501] Loss: 8.8906\n",
      "Training: Epoch[006/020] Iteration[190/501] Loss: 7.7202\n",
      "Training: Epoch[006/020] Iteration[200/501] Loss: 8.6847\n",
      "Training: Epoch[006/020] Iteration[210/501] Loss: 8.1376\n",
      "Training: Epoch[006/020] Iteration[220/501] Loss: 8.3817\n",
      "Training: Epoch[006/020] Iteration[230/501] Loss: 9.7266\n",
      "Training: Epoch[006/020] Iteration[240/501] Loss: 7.7167\n",
      "Training: Epoch[006/020] Iteration[250/501] Loss: 8.7527\n",
      "Training: Epoch[006/020] Iteration[260/501] Loss: 7.3859\n",
      "Training: Epoch[006/020] Iteration[270/501] Loss: 7.6497\n",
      "Training: Epoch[006/020] Iteration[280/501] Loss: 9.3410\n",
      "Training: Epoch[006/020] Iteration[290/501] Loss: 6.5553\n",
      "Training: Epoch[006/020] Iteration[300/501] Loss: 7.6306\n",
      "Training: Epoch[006/020] Iteration[310/501] Loss: 9.9001\n",
      "Training: Epoch[006/020] Iteration[320/501] Loss: 7.7345\n",
      "Training: Epoch[006/020] Iteration[330/501] Loss: 8.2468\n",
      "Training: Epoch[006/020] Iteration[340/501] Loss: 7.4366\n",
      "Training: Epoch[006/020] Iteration[350/501] Loss: 8.2000\n",
      "Training: Epoch[006/020] Iteration[360/501] Loss: 12.1993\n",
      "Training: Epoch[006/020] Iteration[370/501] Loss: 11.4383\n",
      "Training: Epoch[006/020] Iteration[380/501] Loss: 8.4934\n",
      "Training: Epoch[006/020] Iteration[390/501] Loss: 10.2246\n",
      "Training: Epoch[006/020] Iteration[400/501] Loss: 8.1493\n",
      "Training: Epoch[006/020] Iteration[410/501] Loss: 10.2717\n",
      "Training: Epoch[006/020] Iteration[420/501] Loss: 10.5260\n",
      "Training: Epoch[006/020] Iteration[430/501] Loss: 8.5268\n",
      "Training: Epoch[006/020] Iteration[440/501] Loss: 9.9256\n",
      "Training: Epoch[006/020] Iteration[450/501] Loss: 12.3535\n",
      "Training: Epoch[006/020] Iteration[460/501] Loss: 11.6485\n",
      "Training: Epoch[006/020] Iteration[470/501] Loss: 9.7207\n",
      "Training: Epoch[006/020] Iteration[480/501] Loss: 9.5172\n",
      "Training: Epoch[006/020] Iteration[490/501] Loss: 9.0871\n",
      "Training: Epoch[006/020] Iteration[500/501] Loss: 12.7295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(52400) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[006/020] Iteration[501/501] Loss: 9.2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(52461) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[007/020] Iteration[010/501] Loss: 6.7900\n",
      "Training: Epoch[007/020] Iteration[020/501] Loss: 7.5766\n",
      "Training: Epoch[007/020] Iteration[030/501] Loss: 8.8937\n",
      "Training: Epoch[007/020] Iteration[040/501] Loss: 8.9897\n",
      "Training: Epoch[007/020] Iteration[050/501] Loss: 10.2976\n",
      "Training: Epoch[007/020] Iteration[060/501] Loss: 12.5946\n",
      "Training: Epoch[007/020] Iteration[070/501] Loss: 8.8265\n",
      "Training: Epoch[007/020] Iteration[080/501] Loss: 9.3521\n",
      "Training: Epoch[007/020] Iteration[090/501] Loss: 7.2001\n",
      "Training: Epoch[007/020] Iteration[100/501] Loss: 8.3544\n",
      "Training: Epoch[007/020] Iteration[110/501] Loss: 7.4457\n",
      "Training: Epoch[007/020] Iteration[120/501] Loss: 7.7033\n",
      "Training: Epoch[007/020] Iteration[130/501] Loss: 9.1867\n",
      "Training: Epoch[007/020] Iteration[140/501] Loss: 7.5564\n",
      "Training: Epoch[007/020] Iteration[150/501] Loss: 10.0516\n",
      "Training: Epoch[007/020] Iteration[160/501] Loss: 8.2025\n",
      "Training: Epoch[007/020] Iteration[170/501] Loss: 14.5456\n",
      "Training: Epoch[007/020] Iteration[180/501] Loss: 11.0367\n",
      "Training: Epoch[007/020] Iteration[190/501] Loss: 7.2476\n",
      "Training: Epoch[007/020] Iteration[200/501] Loss: 9.4875\n",
      "Training: Epoch[007/020] Iteration[210/501] Loss: 8.9003\n",
      "Training: Epoch[007/020] Iteration[220/501] Loss: 6.1053\n",
      "Training: Epoch[007/020] Iteration[230/501] Loss: 8.1413\n",
      "Training: Epoch[007/020] Iteration[240/501] Loss: 9.6091\n",
      "Training: Epoch[007/020] Iteration[250/501] Loss: 9.6811\n",
      "Training: Epoch[007/020] Iteration[260/501] Loss: 10.4961\n",
      "Training: Epoch[007/020] Iteration[270/501] Loss: 11.0737\n",
      "Training: Epoch[007/020] Iteration[280/501] Loss: 12.7865\n",
      "Training: Epoch[007/020] Iteration[290/501] Loss: 8.0093\n",
      "Training: Epoch[007/020] Iteration[300/501] Loss: 9.4834\n",
      "Training: Epoch[007/020] Iteration[310/501] Loss: 9.2340\n",
      "Training: Epoch[007/020] Iteration[320/501] Loss: 9.8633\n",
      "Training: Epoch[007/020] Iteration[330/501] Loss: 8.6061\n",
      "Training: Epoch[007/020] Iteration[340/501] Loss: 8.7480\n",
      "Training: Epoch[007/020] Iteration[350/501] Loss: 8.7512\n",
      "Training: Epoch[007/020] Iteration[360/501] Loss: 7.4622\n",
      "Training: Epoch[007/020] Iteration[370/501] Loss: 7.6402\n",
      "Training: Epoch[007/020] Iteration[380/501] Loss: 9.9349\n",
      "Training: Epoch[007/020] Iteration[390/501] Loss: 10.3779\n",
      "Training: Epoch[007/020] Iteration[400/501] Loss: 9.2473\n",
      "Training: Epoch[007/020] Iteration[410/501] Loss: 8.5809\n",
      "Training: Epoch[007/020] Iteration[420/501] Loss: 7.9068\n",
      "Training: Epoch[007/020] Iteration[430/501] Loss: 8.2498\n",
      "Training: Epoch[007/020] Iteration[440/501] Loss: 8.6613\n",
      "Training: Epoch[007/020] Iteration[450/501] Loss: 10.8005\n",
      "Training: Epoch[007/020] Iteration[460/501] Loss: 9.1080\n",
      "Training: Epoch[007/020] Iteration[470/501] Loss: 7.5818\n",
      "Training: Epoch[007/020] Iteration[480/501] Loss: 8.8772\n",
      "Training: Epoch[007/020] Iteration[490/501] Loss: 8.2948\n",
      "Training: Epoch[007/020] Iteration[500/501] Loss: 9.1762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(52922) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[007/020] Iteration[501/501] Loss: 11.5732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(52982) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[008/020] Iteration[010/501] Loss: 9.4077\n",
      "Training: Epoch[008/020] Iteration[020/501] Loss: 10.6807\n",
      "Training: Epoch[008/020] Iteration[030/501] Loss: 7.3281\n",
      "Training: Epoch[008/020] Iteration[040/501] Loss: 9.3661\n",
      "Training: Epoch[008/020] Iteration[050/501] Loss: 10.5932\n",
      "Training: Epoch[008/020] Iteration[060/501] Loss: 8.5461\n",
      "Training: Epoch[008/020] Iteration[070/501] Loss: 9.1463\n",
      "Training: Epoch[008/020] Iteration[080/501] Loss: 7.1961\n",
      "Training: Epoch[008/020] Iteration[090/501] Loss: 11.7326\n",
      "Training: Epoch[008/020] Iteration[100/501] Loss: 11.6934\n",
      "Training: Epoch[008/020] Iteration[110/501] Loss: 8.5837\n",
      "Training: Epoch[008/020] Iteration[120/501] Loss: 8.8147\n",
      "Training: Epoch[008/020] Iteration[130/501] Loss: 6.4637\n",
      "Training: Epoch[008/020] Iteration[140/501] Loss: 6.0278\n",
      "Training: Epoch[008/020] Iteration[150/501] Loss: 8.3779\n",
      "Training: Epoch[008/020] Iteration[160/501] Loss: 8.7051\n",
      "Training: Epoch[008/020] Iteration[170/501] Loss: 8.6559\n",
      "Training: Epoch[008/020] Iteration[180/501] Loss: 10.0338\n",
      "Training: Epoch[008/020] Iteration[190/501] Loss: 7.3075\n",
      "Training: Epoch[008/020] Iteration[200/501] Loss: 8.2756\n",
      "Training: Epoch[008/020] Iteration[210/501] Loss: 8.6517\n",
      "Training: Epoch[008/020] Iteration[220/501] Loss: 8.0597\n",
      "Training: Epoch[008/020] Iteration[230/501] Loss: 7.9998\n",
      "Training: Epoch[008/020] Iteration[240/501] Loss: 9.1071\n",
      "Training: Epoch[008/020] Iteration[250/501] Loss: 6.5825\n",
      "Training: Epoch[008/020] Iteration[260/501] Loss: 9.5159\n",
      "Training: Epoch[008/020] Iteration[270/501] Loss: 11.6607\n",
      "Training: Epoch[008/020] Iteration[280/501] Loss: 8.5784\n",
      "Training: Epoch[008/020] Iteration[290/501] Loss: 8.8372\n",
      "Training: Epoch[008/020] Iteration[300/501] Loss: 9.1470\n",
      "Training: Epoch[008/020] Iteration[310/501] Loss: 9.3537\n",
      "Training: Epoch[008/020] Iteration[320/501] Loss: 8.2412\n",
      "Training: Epoch[008/020] Iteration[330/501] Loss: 10.4725\n",
      "Training: Epoch[008/020] Iteration[340/501] Loss: 9.4055\n",
      "Training: Epoch[008/020] Iteration[350/501] Loss: 8.6837\n",
      "Training: Epoch[008/020] Iteration[360/501] Loss: 8.4140\n",
      "Training: Epoch[008/020] Iteration[370/501] Loss: 7.5985\n",
      "Training: Epoch[008/020] Iteration[380/501] Loss: 7.9894\n",
      "Training: Epoch[008/020] Iteration[390/501] Loss: 8.6802\n",
      "Training: Epoch[008/020] Iteration[400/501] Loss: 7.0175\n",
      "Training: Epoch[008/020] Iteration[410/501] Loss: 6.3501\n",
      "Training: Epoch[008/020] Iteration[420/501] Loss: 9.1016\n",
      "Training: Epoch[008/020] Iteration[430/501] Loss: 8.7585\n",
      "Training: Epoch[008/020] Iteration[440/501] Loss: 9.1591\n",
      "Training: Epoch[008/020] Iteration[450/501] Loss: 9.5124\n",
      "Training: Epoch[008/020] Iteration[460/501] Loss: 7.7078\n",
      "Training: Epoch[008/020] Iteration[470/501] Loss: 9.1963\n",
      "Training: Epoch[008/020] Iteration[480/501] Loss: 9.8567\n",
      "Training: Epoch[008/020] Iteration[490/501] Loss: 6.8086\n",
      "Training: Epoch[008/020] Iteration[500/501] Loss: 9.5639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(53449) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[008/020] Iteration[501/501] Loss: 8.7671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(53532) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[009/020] Iteration[010/501] Loss: 9.9748\n",
      "Training: Epoch[009/020] Iteration[020/501] Loss: 7.2753\n",
      "Training: Epoch[009/020] Iteration[030/501] Loss: 8.9928\n",
      "Training: Epoch[009/020] Iteration[040/501] Loss: 7.1571\n",
      "Training: Epoch[009/020] Iteration[050/501] Loss: 8.3721\n",
      "Training: Epoch[009/020] Iteration[060/501] Loss: 7.2343\n",
      "Training: Epoch[009/020] Iteration[070/501] Loss: 7.4626\n",
      "Training: Epoch[009/020] Iteration[080/501] Loss: 10.4263\n",
      "Training: Epoch[009/020] Iteration[090/501] Loss: 6.5545\n",
      "Training: Epoch[009/020] Iteration[100/501] Loss: 8.7681\n",
      "Training: Epoch[009/020] Iteration[110/501] Loss: 8.6162\n",
      "Training: Epoch[009/020] Iteration[120/501] Loss: 7.8678\n",
      "Training: Epoch[009/020] Iteration[130/501] Loss: 8.8307\n",
      "Training: Epoch[009/020] Iteration[140/501] Loss: 7.2141\n",
      "Training: Epoch[009/020] Iteration[150/501] Loss: 7.3186\n",
      "Training: Epoch[009/020] Iteration[160/501] Loss: 8.3244\n",
      "Training: Epoch[009/020] Iteration[170/501] Loss: 7.6228\n",
      "Training: Epoch[009/020] Iteration[180/501] Loss: 8.6299\n",
      "Training: Epoch[009/020] Iteration[190/501] Loss: 7.3601\n",
      "Training: Epoch[009/020] Iteration[200/501] Loss: 8.6511\n",
      "Training: Epoch[009/020] Iteration[210/501] Loss: 7.5357\n",
      "Training: Epoch[009/020] Iteration[220/501] Loss: 9.2581\n",
      "Training: Epoch[009/020] Iteration[230/501] Loss: 9.2841\n",
      "Training: Epoch[009/020] Iteration[240/501] Loss: 8.5008\n",
      "Training: Epoch[009/020] Iteration[250/501] Loss: 6.4044\n",
      "Training: Epoch[009/020] Iteration[260/501] Loss: 8.9242\n",
      "Training: Epoch[009/020] Iteration[270/501] Loss: 5.5569\n",
      "Training: Epoch[009/020] Iteration[280/501] Loss: 10.4933\n",
      "Training: Epoch[009/020] Iteration[290/501] Loss: 10.4599\n",
      "Training: Epoch[009/020] Iteration[300/501] Loss: 6.6608\n",
      "Training: Epoch[009/020] Iteration[310/501] Loss: 9.3187\n",
      "Training: Epoch[009/020] Iteration[320/501] Loss: 5.0931\n",
      "Training: Epoch[009/020] Iteration[330/501] Loss: 8.0785\n",
      "Training: Epoch[009/020] Iteration[340/501] Loss: 7.6031\n",
      "Training: Epoch[009/020] Iteration[350/501] Loss: 8.3979\n",
      "Training: Epoch[009/020] Iteration[360/501] Loss: 7.5580\n",
      "Training: Epoch[009/020] Iteration[370/501] Loss: 7.1805\n",
      "Training: Epoch[009/020] Iteration[380/501] Loss: 7.1174\n",
      "Training: Epoch[009/020] Iteration[390/501] Loss: 7.5384\n",
      "Training: Epoch[009/020] Iteration[400/501] Loss: 6.9511\n",
      "Training: Epoch[009/020] Iteration[410/501] Loss: 7.5939\n",
      "Training: Epoch[009/020] Iteration[420/501] Loss: 9.1120\n",
      "Training: Epoch[009/020] Iteration[430/501] Loss: 11.4389\n",
      "Training: Epoch[009/020] Iteration[440/501] Loss: 7.8858\n",
      "Training: Epoch[009/020] Iteration[450/501] Loss: 7.5795\n",
      "Training: Epoch[009/020] Iteration[460/501] Loss: 6.9482\n",
      "Training: Epoch[009/020] Iteration[470/501] Loss: 8.0664\n",
      "Training: Epoch[009/020] Iteration[480/501] Loss: 7.7301\n",
      "Training: Epoch[009/020] Iteration[490/501] Loss: 7.6460\n",
      "Training: Epoch[009/020] Iteration[500/501] Loss: 7.3189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(53996) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[009/020] Iteration[501/501] Loss: 13.8604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(54059) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[010/020] Iteration[010/501] Loss: 6.5754\n",
      "Training: Epoch[010/020] Iteration[020/501] Loss: 5.1443\n",
      "Training: Epoch[010/020] Iteration[030/501] Loss: 6.4053\n",
      "Training: Epoch[010/020] Iteration[040/501] Loss: 5.7604\n",
      "Training: Epoch[010/020] Iteration[050/501] Loss: 4.8653\n",
      "Training: Epoch[010/020] Iteration[060/501] Loss: 6.1179\n",
      "Training: Epoch[010/020] Iteration[070/501] Loss: 5.0736\n",
      "Training: Epoch[010/020] Iteration[080/501] Loss: 4.9657\n",
      "Training: Epoch[010/020] Iteration[090/501] Loss: 6.3988\n",
      "Training: Epoch[010/020] Iteration[100/501] Loss: 4.7283\n",
      "Training: Epoch[010/020] Iteration[110/501] Loss: 7.6278\n",
      "Training: Epoch[010/020] Iteration[120/501] Loss: 6.0997\n",
      "Training: Epoch[010/020] Iteration[130/501] Loss: 5.2862\n",
      "Training: Epoch[010/020] Iteration[140/501] Loss: 4.4694\n",
      "Training: Epoch[010/020] Iteration[150/501] Loss: 4.7654\n",
      "Training: Epoch[010/020] Iteration[160/501] Loss: 6.3224\n",
      "Training: Epoch[010/020] Iteration[170/501] Loss: 7.2690\n",
      "Training: Epoch[010/020] Iteration[180/501] Loss: 5.9957\n",
      "Training: Epoch[010/020] Iteration[190/501] Loss: 4.3153\n",
      "Training: Epoch[010/020] Iteration[200/501] Loss: 4.7253\n",
      "Training: Epoch[010/020] Iteration[210/501] Loss: 5.2621\n",
      "Training: Epoch[010/020] Iteration[220/501] Loss: 3.7490\n",
      "Training: Epoch[010/020] Iteration[230/501] Loss: 4.5399\n",
      "Training: Epoch[010/020] Iteration[240/501] Loss: 5.2329\n",
      "Training: Epoch[010/020] Iteration[250/501] Loss: 5.7702\n",
      "Training: Epoch[010/020] Iteration[260/501] Loss: 5.5422\n",
      "Training: Epoch[010/020] Iteration[270/501] Loss: 5.7816\n",
      "Training: Epoch[010/020] Iteration[280/501] Loss: 2.7872\n",
      "Training: Epoch[010/020] Iteration[290/501] Loss: 4.1017\n",
      "Training: Epoch[010/020] Iteration[300/501] Loss: 5.6491\n",
      "Training: Epoch[010/020] Iteration[310/501] Loss: 4.5168\n",
      "Training: Epoch[010/020] Iteration[320/501] Loss: 4.6119\n",
      "Training: Epoch[010/020] Iteration[330/501] Loss: 3.6463\n",
      "Training: Epoch[010/020] Iteration[340/501] Loss: 5.8569\n",
      "Training: Epoch[010/020] Iteration[350/501] Loss: 4.1578\n",
      "Training: Epoch[010/020] Iteration[360/501] Loss: 3.8310\n",
      "Training: Epoch[010/020] Iteration[370/501] Loss: 5.3108\n",
      "Training: Epoch[010/020] Iteration[380/501] Loss: 4.3139\n",
      "Training: Epoch[010/020] Iteration[390/501] Loss: 4.2369\n",
      "Training: Epoch[010/020] Iteration[400/501] Loss: 4.7875\n",
      "Training: Epoch[010/020] Iteration[410/501] Loss: 4.1811\n",
      "Training: Epoch[010/020] Iteration[420/501] Loss: 5.6538\n",
      "Training: Epoch[010/020] Iteration[430/501] Loss: 5.2806\n",
      "Training: Epoch[010/020] Iteration[440/501] Loss: 3.3451\n",
      "Training: Epoch[010/020] Iteration[450/501] Loss: 5.5297\n",
      "Training: Epoch[010/020] Iteration[460/501] Loss: 6.3023\n",
      "Training: Epoch[010/020] Iteration[470/501] Loss: 4.7726\n",
      "Training: Epoch[010/020] Iteration[480/501] Loss: 8.2052\n",
      "Training: Epoch[010/020] Iteration[490/501] Loss: 4.8439\n",
      "Training: Epoch[010/020] Iteration[500/501] Loss: 4.8642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(54532) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[010/020] Iteration[501/501] Loss: 7.5477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(54589) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[011/020] Iteration[010/501] Loss: 4.6608\n",
      "Training: Epoch[011/020] Iteration[020/501] Loss: 4.7584\n",
      "Training: Epoch[011/020] Iteration[030/501] Loss: 3.9805\n",
      "Training: Epoch[011/020] Iteration[040/501] Loss: 4.0690\n",
      "Training: Epoch[011/020] Iteration[050/501] Loss: 4.4010\n",
      "Training: Epoch[011/020] Iteration[060/501] Loss: 5.2539\n",
      "Training: Epoch[011/020] Iteration[070/501] Loss: 3.5625\n",
      "Training: Epoch[011/020] Iteration[080/501] Loss: 6.5291\n",
      "Training: Epoch[011/020] Iteration[090/501] Loss: 3.5330\n",
      "Training: Epoch[011/020] Iteration[100/501] Loss: 4.3120\n",
      "Training: Epoch[011/020] Iteration[110/501] Loss: 5.2259\n",
      "Training: Epoch[011/020] Iteration[120/501] Loss: 3.6978\n",
      "Training: Epoch[011/020] Iteration[130/501] Loss: 4.3054\n",
      "Training: Epoch[011/020] Iteration[140/501] Loss: 3.8206\n",
      "Training: Epoch[011/020] Iteration[150/501] Loss: 5.2437\n",
      "Training: Epoch[011/020] Iteration[160/501] Loss: 5.4507\n",
      "Training: Epoch[011/020] Iteration[170/501] Loss: 4.4434\n",
      "Training: Epoch[011/020] Iteration[180/501] Loss: 3.3242\n",
      "Training: Epoch[011/020] Iteration[190/501] Loss: 3.6672\n",
      "Training: Epoch[011/020] Iteration[200/501] Loss: 4.4257\n",
      "Training: Epoch[011/020] Iteration[210/501] Loss: 4.8108\n",
      "Training: Epoch[011/020] Iteration[220/501] Loss: 4.6468\n",
      "Training: Epoch[011/020] Iteration[230/501] Loss: 5.3695\n",
      "Training: Epoch[011/020] Iteration[240/501] Loss: 4.8762\n",
      "Training: Epoch[011/020] Iteration[250/501] Loss: 3.4200\n",
      "Training: Epoch[011/020] Iteration[260/501] Loss: 2.9531\n",
      "Training: Epoch[011/020] Iteration[270/501] Loss: 4.1571\n",
      "Training: Epoch[011/020] Iteration[280/501] Loss: 4.7753\n",
      "Training: Epoch[011/020] Iteration[290/501] Loss: 4.5098\n",
      "Training: Epoch[011/020] Iteration[300/501] Loss: 4.4742\n",
      "Training: Epoch[011/020] Iteration[310/501] Loss: 4.8286\n",
      "Training: Epoch[011/020] Iteration[320/501] Loss: 3.2193\n",
      "Training: Epoch[011/020] Iteration[330/501] Loss: 4.2129\n",
      "Training: Epoch[011/020] Iteration[340/501] Loss: 4.9679\n",
      "Training: Epoch[011/020] Iteration[350/501] Loss: 3.9735\n",
      "Training: Epoch[011/020] Iteration[360/501] Loss: 4.8931\n",
      "Training: Epoch[011/020] Iteration[370/501] Loss: 4.7417\n",
      "Training: Epoch[011/020] Iteration[380/501] Loss: 5.0042\n",
      "Training: Epoch[011/020] Iteration[390/501] Loss: 4.1951\n",
      "Training: Epoch[011/020] Iteration[400/501] Loss: 7.2374\n",
      "Training: Epoch[011/020] Iteration[410/501] Loss: 4.8013\n",
      "Training: Epoch[011/020] Iteration[420/501] Loss: 3.8138\n",
      "Training: Epoch[011/020] Iteration[430/501] Loss: 4.5981\n",
      "Training: Epoch[011/020] Iteration[440/501] Loss: 5.1675\n",
      "Training: Epoch[011/020] Iteration[450/501] Loss: 4.3051\n",
      "Training: Epoch[011/020] Iteration[460/501] Loss: 3.8242\n",
      "Training: Epoch[011/020] Iteration[470/501] Loss: 3.5827\n",
      "Training: Epoch[011/020] Iteration[480/501] Loss: 5.4398\n",
      "Training: Epoch[011/020] Iteration[490/501] Loss: 4.4753\n",
      "Training: Epoch[011/020] Iteration[500/501] Loss: 3.6040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(55072) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[011/020] Iteration[501/501] Loss: 7.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(55147) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[012/020] Iteration[010/501] Loss: 5.2503\n",
      "Training: Epoch[012/020] Iteration[020/501] Loss: 3.8035\n",
      "Training: Epoch[012/020] Iteration[030/501] Loss: 4.8196\n",
      "Training: Epoch[012/020] Iteration[040/501] Loss: 4.3112\n",
      "Training: Epoch[012/020] Iteration[050/501] Loss: 3.4615\n",
      "Training: Epoch[012/020] Iteration[060/501] Loss: 4.1532\n",
      "Training: Epoch[012/020] Iteration[070/501] Loss: 4.9905\n",
      "Training: Epoch[012/020] Iteration[080/501] Loss: 3.5606\n",
      "Training: Epoch[012/020] Iteration[090/501] Loss: 3.6137\n",
      "Training: Epoch[012/020] Iteration[100/501] Loss: 4.1462\n",
      "Training: Epoch[012/020] Iteration[110/501] Loss: 3.6810\n",
      "Training: Epoch[012/020] Iteration[120/501] Loss: 4.1270\n",
      "Training: Epoch[012/020] Iteration[130/501] Loss: 4.0659\n",
      "Training: Epoch[012/020] Iteration[140/501] Loss: 4.5517\n",
      "Training: Epoch[012/020] Iteration[150/501] Loss: 4.9447\n",
      "Training: Epoch[012/020] Iteration[160/501] Loss: 3.6297\n",
      "Training: Epoch[012/020] Iteration[170/501] Loss: 3.2257\n",
      "Training: Epoch[012/020] Iteration[180/501] Loss: 3.9058\n",
      "Training: Epoch[012/020] Iteration[190/501] Loss: 3.8361\n",
      "Training: Epoch[012/020] Iteration[200/501] Loss: 4.9561\n",
      "Training: Epoch[012/020] Iteration[210/501] Loss: 4.8724\n",
      "Training: Epoch[012/020] Iteration[220/501] Loss: 4.4357\n",
      "Training: Epoch[012/020] Iteration[230/501] Loss: 3.6138\n",
      "Training: Epoch[012/020] Iteration[240/501] Loss: 3.1087\n",
      "Training: Epoch[012/020] Iteration[250/501] Loss: 4.9442\n",
      "Training: Epoch[012/020] Iteration[260/501] Loss: 4.1991\n",
      "Training: Epoch[012/020] Iteration[270/501] Loss: 3.3012\n",
      "Training: Epoch[012/020] Iteration[280/501] Loss: 3.6646\n",
      "Training: Epoch[012/020] Iteration[290/501] Loss: 3.4022\n",
      "Training: Epoch[012/020] Iteration[300/501] Loss: 3.6759\n",
      "Training: Epoch[012/020] Iteration[310/501] Loss: 3.2291\n",
      "Training: Epoch[012/020] Iteration[320/501] Loss: 4.7246\n",
      "Training: Epoch[012/020] Iteration[330/501] Loss: 5.8362\n",
      "Training: Epoch[012/020] Iteration[340/501] Loss: 4.3940\n",
      "Training: Epoch[012/020] Iteration[350/501] Loss: 3.7239\n",
      "Training: Epoch[012/020] Iteration[360/501] Loss: 3.4055\n",
      "Training: Epoch[012/020] Iteration[370/501] Loss: 4.0917\n",
      "Training: Epoch[012/020] Iteration[380/501] Loss: 3.9076\n",
      "Training: Epoch[012/020] Iteration[390/501] Loss: 4.6385\n",
      "Training: Epoch[012/020] Iteration[400/501] Loss: 5.0109\n",
      "Training: Epoch[012/020] Iteration[410/501] Loss: 3.1835\n",
      "Training: Epoch[012/020] Iteration[420/501] Loss: 3.6026\n",
      "Training: Epoch[012/020] Iteration[430/501] Loss: 4.3986\n",
      "Training: Epoch[012/020] Iteration[440/501] Loss: 3.9809\n",
      "Training: Epoch[012/020] Iteration[450/501] Loss: 4.2697\n",
      "Training: Epoch[012/020] Iteration[460/501] Loss: 3.5442\n",
      "Training: Epoch[012/020] Iteration[470/501] Loss: 4.2497\n",
      "Training: Epoch[012/020] Iteration[480/501] Loss: 4.1032\n",
      "Training: Epoch[012/020] Iteration[490/501] Loss: 4.8937\n",
      "Training: Epoch[012/020] Iteration[500/501] Loss: 5.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(55693) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[012/020] Iteration[501/501] Loss: 7.1818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(55780) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[013/020] Iteration[010/501] Loss: 4.3410\n",
      "Training: Epoch[013/020] Iteration[020/501] Loss: 4.2721\n",
      "Training: Epoch[013/020] Iteration[030/501] Loss: 4.4973\n",
      "Training: Epoch[013/020] Iteration[040/501] Loss: 4.3514\n",
      "Training: Epoch[013/020] Iteration[050/501] Loss: 4.4605\n",
      "Training: Epoch[013/020] Iteration[060/501] Loss: 2.8226\n",
      "Training: Epoch[013/020] Iteration[070/501] Loss: 4.1230\n",
      "Training: Epoch[013/020] Iteration[080/501] Loss: 3.6081\n",
      "Training: Epoch[013/020] Iteration[090/501] Loss: 3.8821\n",
      "Training: Epoch[013/020] Iteration[100/501] Loss: 3.2377\n",
      "Training: Epoch[013/020] Iteration[110/501] Loss: 4.3251\n",
      "Training: Epoch[013/020] Iteration[120/501] Loss: 4.3427\n",
      "Training: Epoch[013/020] Iteration[130/501] Loss: 4.9099\n",
      "Training: Epoch[013/020] Iteration[140/501] Loss: 4.7477\n",
      "Training: Epoch[013/020] Iteration[150/501] Loss: 3.8844\n",
      "Training: Epoch[013/020] Iteration[160/501] Loss: 4.0496\n",
      "Training: Epoch[013/020] Iteration[170/501] Loss: 3.4048\n",
      "Training: Epoch[013/020] Iteration[180/501] Loss: 3.9980\n",
      "Training: Epoch[013/020] Iteration[190/501] Loss: 4.3684\n",
      "Training: Epoch[013/020] Iteration[200/501] Loss: 3.8649\n",
      "Training: Epoch[013/020] Iteration[210/501] Loss: 3.7710\n",
      "Training: Epoch[013/020] Iteration[220/501] Loss: 3.7539\n",
      "Training: Epoch[013/020] Iteration[230/501] Loss: 2.7230\n",
      "Training: Epoch[013/020] Iteration[240/501] Loss: 3.1128\n",
      "Training: Epoch[013/020] Iteration[250/501] Loss: 3.1008\n",
      "Training: Epoch[013/020] Iteration[260/501] Loss: 3.9017\n",
      "Training: Epoch[013/020] Iteration[270/501] Loss: 3.6348\n",
      "Training: Epoch[013/020] Iteration[280/501] Loss: 4.2430\n",
      "Training: Epoch[013/020] Iteration[290/501] Loss: 4.0364\n",
      "Training: Epoch[013/020] Iteration[300/501] Loss: 3.4046\n",
      "Training: Epoch[013/020] Iteration[310/501] Loss: 3.9807\n",
      "Training: Epoch[013/020] Iteration[320/501] Loss: 3.3537\n",
      "Training: Epoch[013/020] Iteration[330/501] Loss: 4.3860\n",
      "Training: Epoch[013/020] Iteration[340/501] Loss: 3.4729\n",
      "Training: Epoch[013/020] Iteration[350/501] Loss: 4.9345\n",
      "Training: Epoch[013/020] Iteration[360/501] Loss: 3.1365\n",
      "Training: Epoch[013/020] Iteration[370/501] Loss: 4.0013\n",
      "Training: Epoch[013/020] Iteration[380/501] Loss: 4.2430\n",
      "Training: Epoch[013/020] Iteration[390/501] Loss: 3.8886\n",
      "Training: Epoch[013/020] Iteration[400/501] Loss: 4.2982\n",
      "Training: Epoch[013/020] Iteration[410/501] Loss: 3.3192\n",
      "Training: Epoch[013/020] Iteration[420/501] Loss: 3.7294\n",
      "Training: Epoch[013/020] Iteration[430/501] Loss: 4.9489\n",
      "Training: Epoch[013/020] Iteration[440/501] Loss: 3.6074\n",
      "Training: Epoch[013/020] Iteration[450/501] Loss: 3.2483\n",
      "Training: Epoch[013/020] Iteration[460/501] Loss: 3.2492\n",
      "Training: Epoch[013/020] Iteration[470/501] Loss: 5.0012\n",
      "Training: Epoch[013/020] Iteration[480/501] Loss: 4.4566\n",
      "Training: Epoch[013/020] Iteration[490/501] Loss: 4.1236\n",
      "Training: Epoch[013/020] Iteration[500/501] Loss: 4.3858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(56301) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[013/020] Iteration[501/501] Loss: 6.9892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(56395) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[014/020] Iteration[010/501] Loss: 4.3521\n",
      "Training: Epoch[014/020] Iteration[020/501] Loss: 3.5666\n",
      "Training: Epoch[014/020] Iteration[030/501] Loss: 3.4853\n",
      "Training: Epoch[014/020] Iteration[040/501] Loss: 3.8524\n",
      "Training: Epoch[014/020] Iteration[050/501] Loss: 2.9791\n",
      "Training: Epoch[014/020] Iteration[060/501] Loss: 3.2978\n",
      "Training: Epoch[014/020] Iteration[070/501] Loss: 3.3511\n",
      "Training: Epoch[014/020] Iteration[080/501] Loss: 3.9908\n",
      "Training: Epoch[014/020] Iteration[090/501] Loss: 4.5615\n",
      "Training: Epoch[014/020] Iteration[100/501] Loss: 3.3901\n",
      "Training: Epoch[014/020] Iteration[110/501] Loss: 4.5843\n",
      "Training: Epoch[014/020] Iteration[120/501] Loss: 3.7425\n",
      "Training: Epoch[014/020] Iteration[130/501] Loss: 3.4506\n",
      "Training: Epoch[014/020] Iteration[140/501] Loss: 4.2193\n",
      "Training: Epoch[014/020] Iteration[150/501] Loss: 2.8520\n",
      "Training: Epoch[014/020] Iteration[160/501] Loss: 4.4028\n",
      "Training: Epoch[014/020] Iteration[170/501] Loss: 2.8071\n",
      "Training: Epoch[014/020] Iteration[180/501] Loss: 3.4752\n",
      "Training: Epoch[014/020] Iteration[190/501] Loss: 3.5434\n",
      "Training: Epoch[014/020] Iteration[200/501] Loss: 3.1326\n",
      "Training: Epoch[014/020] Iteration[210/501] Loss: 3.7142\n",
      "Training: Epoch[014/020] Iteration[220/501] Loss: 3.0283\n",
      "Training: Epoch[014/020] Iteration[230/501] Loss: 3.5196\n",
      "Training: Epoch[014/020] Iteration[240/501] Loss: 3.7223\n",
      "Training: Epoch[014/020] Iteration[250/501] Loss: 3.7223\n",
      "Training: Epoch[014/020] Iteration[260/501] Loss: 3.2521\n",
      "Training: Epoch[014/020] Iteration[270/501] Loss: 4.5032\n",
      "Training: Epoch[014/020] Iteration[280/501] Loss: 3.9737\n",
      "Training: Epoch[014/020] Iteration[290/501] Loss: 4.8806\n",
      "Training: Epoch[014/020] Iteration[300/501] Loss: 5.1632\n",
      "Training: Epoch[014/020] Iteration[310/501] Loss: 4.3370\n",
      "Training: Epoch[014/020] Iteration[320/501] Loss: 4.2124\n",
      "Training: Epoch[014/020] Iteration[330/501] Loss: 4.1825\n",
      "Training: Epoch[014/020] Iteration[340/501] Loss: 4.2463\n",
      "Training: Epoch[014/020] Iteration[350/501] Loss: 3.6469\n",
      "Training: Epoch[014/020] Iteration[360/501] Loss: 3.1742\n",
      "Training: Epoch[014/020] Iteration[370/501] Loss: 3.4850\n",
      "Training: Epoch[014/020] Iteration[380/501] Loss: 3.9148\n",
      "Training: Epoch[014/020] Iteration[390/501] Loss: 2.5981\n",
      "Training: Epoch[014/020] Iteration[400/501] Loss: 3.6578\n",
      "Training: Epoch[014/020] Iteration[410/501] Loss: 3.6056\n",
      "Training: Epoch[014/020] Iteration[420/501] Loss: 2.7116\n",
      "Training: Epoch[014/020] Iteration[430/501] Loss: 2.8782\n",
      "Training: Epoch[014/020] Iteration[440/501] Loss: 3.3348\n",
      "Training: Epoch[014/020] Iteration[450/501] Loss: 3.8781\n",
      "Training: Epoch[014/020] Iteration[460/501] Loss: 3.7508\n",
      "Training: Epoch[014/020] Iteration[470/501] Loss: 4.2802\n",
      "Training: Epoch[014/020] Iteration[480/501] Loss: 3.5361\n",
      "Training: Epoch[014/020] Iteration[490/501] Loss: 3.7845\n",
      "Training: Epoch[014/020] Iteration[500/501] Loss: 4.7016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(56907) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[014/020] Iteration[501/501] Loss: 6.4247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(56969) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[015/020] Iteration[010/501] Loss: 3.5053\n",
      "Training: Epoch[015/020] Iteration[020/501] Loss: 3.5310\n",
      "Training: Epoch[015/020] Iteration[030/501] Loss: 3.4255\n",
      "Training: Epoch[015/020] Iteration[040/501] Loss: 3.3116\n",
      "Training: Epoch[015/020] Iteration[050/501] Loss: 2.7160\n",
      "Training: Epoch[015/020] Iteration[060/501] Loss: 3.3035\n",
      "Training: Epoch[015/020] Iteration[070/501] Loss: 3.3299\n",
      "Training: Epoch[015/020] Iteration[080/501] Loss: 3.5936\n",
      "Training: Epoch[015/020] Iteration[090/501] Loss: 3.7078\n",
      "Training: Epoch[015/020] Iteration[100/501] Loss: 2.9790\n",
      "Training: Epoch[015/020] Iteration[110/501] Loss: 5.0209\n",
      "Training: Epoch[015/020] Iteration[120/501] Loss: 3.4171\n",
      "Training: Epoch[015/020] Iteration[130/501] Loss: 3.4965\n",
      "Training: Epoch[015/020] Iteration[140/501] Loss: 3.4526\n",
      "Training: Epoch[015/020] Iteration[150/501] Loss: 3.3740\n",
      "Training: Epoch[015/020] Iteration[160/501] Loss: 3.6174\n",
      "Training: Epoch[015/020] Iteration[170/501] Loss: 4.8849\n",
      "Training: Epoch[015/020] Iteration[180/501] Loss: 4.5697\n",
      "Training: Epoch[015/020] Iteration[190/501] Loss: 3.8552\n",
      "Training: Epoch[015/020] Iteration[200/501] Loss: 4.3938\n",
      "Training: Epoch[015/020] Iteration[210/501] Loss: 2.7786\n",
      "Training: Epoch[015/020] Iteration[220/501] Loss: 3.8458\n",
      "Training: Epoch[015/020] Iteration[230/501] Loss: 3.1672\n",
      "Training: Epoch[015/020] Iteration[240/501] Loss: 4.2628\n",
      "Training: Epoch[015/020] Iteration[250/501] Loss: 3.8374\n",
      "Training: Epoch[015/020] Iteration[260/501] Loss: 3.3863\n",
      "Training: Epoch[015/020] Iteration[270/501] Loss: 3.3390\n",
      "Training: Epoch[015/020] Iteration[280/501] Loss: 4.4492\n",
      "Training: Epoch[015/020] Iteration[290/501] Loss: 3.2022\n",
      "Training: Epoch[015/020] Iteration[300/501] Loss: 3.1295\n",
      "Training: Epoch[015/020] Iteration[310/501] Loss: 2.5406\n",
      "Training: Epoch[015/020] Iteration[320/501] Loss: 2.8628\n",
      "Training: Epoch[015/020] Iteration[330/501] Loss: 3.5051\n",
      "Training: Epoch[015/020] Iteration[340/501] Loss: 3.0206\n",
      "Training: Epoch[015/020] Iteration[350/501] Loss: 3.2665\n",
      "Training: Epoch[015/020] Iteration[360/501] Loss: 2.8475\n",
      "Training: Epoch[015/020] Iteration[370/501] Loss: 3.2068\n",
      "Training: Epoch[015/020] Iteration[380/501] Loss: 2.6201\n",
      "Training: Epoch[015/020] Iteration[390/501] Loss: 3.8645\n",
      "Training: Epoch[015/020] Iteration[400/501] Loss: 3.1595\n",
      "Training: Epoch[015/020] Iteration[410/501] Loss: 3.2955\n",
      "Training: Epoch[015/020] Iteration[420/501] Loss: 3.6594\n",
      "Training: Epoch[015/020] Iteration[430/501] Loss: 4.5217\n",
      "Training: Epoch[015/020] Iteration[440/501] Loss: 5.2055\n",
      "Training: Epoch[015/020] Iteration[450/501] Loss: 2.9925\n",
      "Training: Epoch[015/020] Iteration[460/501] Loss: 3.2299\n",
      "Training: Epoch[015/020] Iteration[470/501] Loss: 3.0849\n",
      "Training: Epoch[015/020] Iteration[480/501] Loss: 3.7183\n",
      "Training: Epoch[015/020] Iteration[490/501] Loss: 2.9778\n",
      "Training: Epoch[015/020] Iteration[500/501] Loss: 4.5833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(57429) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[015/020] Iteration[501/501] Loss: 7.1975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(57507) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[016/020] Iteration[010/501] Loss: 2.5582\n",
      "Training: Epoch[016/020] Iteration[020/501] Loss: 3.2896\n",
      "Training: Epoch[016/020] Iteration[030/501] Loss: 3.8831\n",
      "Training: Epoch[016/020] Iteration[040/501] Loss: 3.4168\n",
      "Training: Epoch[016/020] Iteration[050/501] Loss: 2.8053\n",
      "Training: Epoch[016/020] Iteration[060/501] Loss: 2.8472\n",
      "Training: Epoch[016/020] Iteration[070/501] Loss: 3.4587\n",
      "Training: Epoch[016/020] Iteration[080/501] Loss: 3.7815\n",
      "Training: Epoch[016/020] Iteration[090/501] Loss: 3.3504\n",
      "Training: Epoch[016/020] Iteration[100/501] Loss: 3.2981\n",
      "Training: Epoch[016/020] Iteration[110/501] Loss: 2.7838\n",
      "Training: Epoch[016/020] Iteration[120/501] Loss: 2.9577\n",
      "Training: Epoch[016/020] Iteration[130/501] Loss: 3.0303\n",
      "Training: Epoch[016/020] Iteration[140/501] Loss: 3.9144\n",
      "Training: Epoch[016/020] Iteration[150/501] Loss: 3.5087\n",
      "Training: Epoch[016/020] Iteration[160/501] Loss: 3.3001\n",
      "Training: Epoch[016/020] Iteration[170/501] Loss: 3.4337\n",
      "Training: Epoch[016/020] Iteration[180/501] Loss: 6.2296\n",
      "Training: Epoch[016/020] Iteration[190/501] Loss: 3.7570\n",
      "Training: Epoch[016/020] Iteration[200/501] Loss: 3.6763\n",
      "Training: Epoch[016/020] Iteration[210/501] Loss: 3.9727\n",
      "Training: Epoch[016/020] Iteration[220/501] Loss: 3.0419\n",
      "Training: Epoch[016/020] Iteration[230/501] Loss: 2.8591\n",
      "Training: Epoch[016/020] Iteration[240/501] Loss: 3.8716\n",
      "Training: Epoch[016/020] Iteration[250/501] Loss: 3.6036\n",
      "Training: Epoch[016/020] Iteration[260/501] Loss: 3.4306\n",
      "Training: Epoch[016/020] Iteration[270/501] Loss: 3.1121\n",
      "Training: Epoch[016/020] Iteration[280/501] Loss: 3.1206\n",
      "Training: Epoch[016/020] Iteration[290/501] Loss: 3.5089\n",
      "Training: Epoch[016/020] Iteration[300/501] Loss: 2.9511\n",
      "Training: Epoch[016/020] Iteration[310/501] Loss: 3.2597\n",
      "Training: Epoch[016/020] Iteration[320/501] Loss: 4.7419\n",
      "Training: Epoch[016/020] Iteration[330/501] Loss: 3.8805\n",
      "Training: Epoch[016/020] Iteration[340/501] Loss: 2.9465\n",
      "Training: Epoch[016/020] Iteration[350/501] Loss: 4.4648\n",
      "Training: Epoch[016/020] Iteration[360/501] Loss: 2.9993\n",
      "Training: Epoch[016/020] Iteration[370/501] Loss: 3.0550\n",
      "Training: Epoch[016/020] Iteration[380/501] Loss: 3.7868\n",
      "Training: Epoch[016/020] Iteration[390/501] Loss: 2.8895\n",
      "Training: Epoch[016/020] Iteration[400/501] Loss: 3.8181\n",
      "Training: Epoch[016/020] Iteration[410/501] Loss: 4.1891\n",
      "Training: Epoch[016/020] Iteration[420/501] Loss: 3.1475\n",
      "Training: Epoch[016/020] Iteration[430/501] Loss: 2.9943\n",
      "Training: Epoch[016/020] Iteration[440/501] Loss: 2.6652\n",
      "Training: Epoch[016/020] Iteration[450/501] Loss: 2.5948\n",
      "Training: Epoch[016/020] Iteration[460/501] Loss: 2.6776\n",
      "Training: Epoch[016/020] Iteration[470/501] Loss: 2.9965\n",
      "Training: Epoch[016/020] Iteration[480/501] Loss: 3.9705\n",
      "Training: Epoch[016/020] Iteration[490/501] Loss: 2.8432\n",
      "Training: Epoch[016/020] Iteration[500/501] Loss: 2.8098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(58017) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[016/020] Iteration[501/501] Loss: 7.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(58088) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[017/020] Iteration[010/501] Loss: 2.8337\n",
      "Training: Epoch[017/020] Iteration[020/501] Loss: 4.0240\n",
      "Training: Epoch[017/020] Iteration[030/501] Loss: 2.4415\n",
      "Training: Epoch[017/020] Iteration[040/501] Loss: 3.0053\n",
      "Training: Epoch[017/020] Iteration[050/501] Loss: 3.0911\n",
      "Training: Epoch[017/020] Iteration[060/501] Loss: 2.8292\n",
      "Training: Epoch[017/020] Iteration[070/501] Loss: 3.2767\n",
      "Training: Epoch[017/020] Iteration[080/501] Loss: 3.1337\n",
      "Training: Epoch[017/020] Iteration[090/501] Loss: 2.9678\n",
      "Training: Epoch[017/020] Iteration[100/501] Loss: 2.3185\n",
      "Training: Epoch[017/020] Iteration[110/501] Loss: 2.9826\n",
      "Training: Epoch[017/020] Iteration[120/501] Loss: 3.4830\n",
      "Training: Epoch[017/020] Iteration[130/501] Loss: 3.4522\n",
      "Training: Epoch[017/020] Iteration[140/501] Loss: 2.8644\n",
      "Training: Epoch[017/020] Iteration[150/501] Loss: 2.6267\n",
      "Training: Epoch[017/020] Iteration[160/501] Loss: 3.7542\n",
      "Training: Epoch[017/020] Iteration[170/501] Loss: 2.4798\n",
      "Training: Epoch[017/020] Iteration[180/501] Loss: 2.9963\n",
      "Training: Epoch[017/020] Iteration[190/501] Loss: 3.4533\n",
      "Training: Epoch[017/020] Iteration[200/501] Loss: 3.0101\n",
      "Training: Epoch[017/020] Iteration[210/501] Loss: 3.2863\n",
      "Training: Epoch[017/020] Iteration[220/501] Loss: 4.5563\n",
      "Training: Epoch[017/020] Iteration[230/501] Loss: 2.7836\n",
      "Training: Epoch[017/020] Iteration[240/501] Loss: 3.4469\n",
      "Training: Epoch[017/020] Iteration[250/501] Loss: 4.0621\n",
      "Training: Epoch[017/020] Iteration[260/501] Loss: 2.7597\n",
      "Training: Epoch[017/020] Iteration[270/501] Loss: 4.1979\n",
      "Training: Epoch[017/020] Iteration[280/501] Loss: 2.8393\n",
      "Training: Epoch[017/020] Iteration[290/501] Loss: 3.1770\n",
      "Training: Epoch[017/020] Iteration[300/501] Loss: 2.9008\n",
      "Training: Epoch[017/020] Iteration[310/501] Loss: 3.5847\n",
      "Training: Epoch[017/020] Iteration[320/501] Loss: 3.6335\n",
      "Training: Epoch[017/020] Iteration[330/501] Loss: 3.9412\n",
      "Training: Epoch[017/020] Iteration[340/501] Loss: 3.4260\n",
      "Training: Epoch[017/020] Iteration[350/501] Loss: 3.3876\n",
      "Training: Epoch[017/020] Iteration[360/501] Loss: 3.4492\n",
      "Training: Epoch[017/020] Iteration[370/501] Loss: 3.4077\n",
      "Training: Epoch[017/020] Iteration[380/501] Loss: 2.7048\n",
      "Training: Epoch[017/020] Iteration[390/501] Loss: 2.6383\n",
      "Training: Epoch[017/020] Iteration[400/501] Loss: 2.4454\n",
      "Training: Epoch[017/020] Iteration[410/501] Loss: 3.2391\n",
      "Training: Epoch[017/020] Iteration[420/501] Loss: 2.7105\n",
      "Training: Epoch[017/020] Iteration[430/501] Loss: 2.4687\n",
      "Training: Epoch[017/020] Iteration[440/501] Loss: 3.0718\n",
      "Training: Epoch[017/020] Iteration[450/501] Loss: 3.1764\n",
      "Training: Epoch[017/020] Iteration[460/501] Loss: 2.9837\n",
      "Training: Epoch[017/020] Iteration[470/501] Loss: 4.7971\n",
      "Training: Epoch[017/020] Iteration[480/501] Loss: 3.3527\n",
      "Training: Epoch[017/020] Iteration[490/501] Loss: 4.3112\n",
      "Training: Epoch[017/020] Iteration[500/501] Loss: 3.4305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(58542) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[017/020] Iteration[501/501] Loss: 6.5909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(58605) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[018/020] Iteration[010/501] Loss: 3.0944\n",
      "Training: Epoch[018/020] Iteration[020/501] Loss: 2.8589\n",
      "Training: Epoch[018/020] Iteration[030/501] Loss: 3.0793\n",
      "Training: Epoch[018/020] Iteration[040/501] Loss: 5.0700\n",
      "Training: Epoch[018/020] Iteration[050/501] Loss: 3.3699\n",
      "Training: Epoch[018/020] Iteration[060/501] Loss: 3.5569\n",
      "Training: Epoch[018/020] Iteration[070/501] Loss: 2.1733\n",
      "Training: Epoch[018/020] Iteration[080/501] Loss: 2.9779\n",
      "Training: Epoch[018/020] Iteration[090/501] Loss: 3.2136\n",
      "Training: Epoch[018/020] Iteration[100/501] Loss: 3.2622\n",
      "Training: Epoch[018/020] Iteration[110/501] Loss: 3.2349\n",
      "Training: Epoch[018/020] Iteration[120/501] Loss: 3.2590\n",
      "Training: Epoch[018/020] Iteration[130/501] Loss: 2.8514\n",
      "Training: Epoch[018/020] Iteration[140/501] Loss: 2.7069\n",
      "Training: Epoch[018/020] Iteration[150/501] Loss: 3.1759\n",
      "Training: Epoch[018/020] Iteration[160/501] Loss: 2.7986\n",
      "Training: Epoch[018/020] Iteration[170/501] Loss: 3.2043\n",
      "Training: Epoch[018/020] Iteration[180/501] Loss: 2.7117\n",
      "Training: Epoch[018/020] Iteration[190/501] Loss: 3.0496\n",
      "Training: Epoch[018/020] Iteration[200/501] Loss: 2.6246\n",
      "Training: Epoch[018/020] Iteration[210/501] Loss: 2.5356\n",
      "Training: Epoch[018/020] Iteration[220/501] Loss: 2.7604\n",
      "Training: Epoch[018/020] Iteration[230/501] Loss: 3.2370\n",
      "Training: Epoch[018/020] Iteration[240/501] Loss: 3.1779\n",
      "Training: Epoch[018/020] Iteration[250/501] Loss: 2.9616\n",
      "Training: Epoch[018/020] Iteration[260/501] Loss: 2.6581\n",
      "Training: Epoch[018/020] Iteration[270/501] Loss: 2.4740\n",
      "Training: Epoch[018/020] Iteration[280/501] Loss: 3.0991\n",
      "Training: Epoch[018/020] Iteration[290/501] Loss: 4.2504\n",
      "Training: Epoch[018/020] Iteration[300/501] Loss: 2.4314\n",
      "Training: Epoch[018/020] Iteration[310/501] Loss: 3.2339\n",
      "Training: Epoch[018/020] Iteration[320/501] Loss: 2.3239\n",
      "Training: Epoch[018/020] Iteration[330/501] Loss: 3.6420\n",
      "Training: Epoch[018/020] Iteration[340/501] Loss: 3.0740\n",
      "Training: Epoch[018/020] Iteration[350/501] Loss: 2.5025\n",
      "Training: Epoch[018/020] Iteration[360/501] Loss: 2.9177\n",
      "Training: Epoch[018/020] Iteration[370/501] Loss: 2.7487\n",
      "Training: Epoch[018/020] Iteration[380/501] Loss: 2.6772\n",
      "Training: Epoch[018/020] Iteration[390/501] Loss: 3.8046\n",
      "Training: Epoch[018/020] Iteration[400/501] Loss: 2.5265\n",
      "Training: Epoch[018/020] Iteration[410/501] Loss: 2.9069\n",
      "Training: Epoch[018/020] Iteration[420/501] Loss: 2.5078\n",
      "Training: Epoch[018/020] Iteration[430/501] Loss: 2.7523\n",
      "Training: Epoch[018/020] Iteration[440/501] Loss: 3.5185\n",
      "Training: Epoch[018/020] Iteration[450/501] Loss: 2.8278\n",
      "Training: Epoch[018/020] Iteration[460/501] Loss: 2.7016\n",
      "Training: Epoch[018/020] Iteration[470/501] Loss: 3.7604\n",
      "Training: Epoch[018/020] Iteration[480/501] Loss: 3.2218\n",
      "Training: Epoch[018/020] Iteration[490/501] Loss: 3.5293\n",
      "Training: Epoch[018/020] Iteration[500/501] Loss: 2.8491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(59035) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[018/020] Iteration[501/501] Loss: 6.1188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(59087) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[019/020] Iteration[010/501] Loss: 2.3275\n",
      "Training: Epoch[019/020] Iteration[020/501] Loss: 2.7709\n",
      "Training: Epoch[019/020] Iteration[030/501] Loss: 2.8152\n",
      "Training: Epoch[019/020] Iteration[040/501] Loss: 3.2274\n",
      "Training: Epoch[019/020] Iteration[050/501] Loss: 2.4177\n",
      "Training: Epoch[019/020] Iteration[060/501] Loss: 2.9212\n",
      "Training: Epoch[019/020] Iteration[070/501] Loss: 2.3044\n",
      "Training: Epoch[019/020] Iteration[080/501] Loss: 3.1531\n",
      "Training: Epoch[019/020] Iteration[090/501] Loss: 3.0878\n",
      "Training: Epoch[019/020] Iteration[100/501] Loss: 3.3386\n",
      "Training: Epoch[019/020] Iteration[110/501] Loss: 3.2825\n",
      "Training: Epoch[019/020] Iteration[120/501] Loss: 2.5395\n",
      "Training: Epoch[019/020] Iteration[130/501] Loss: 3.1956\n",
      "Training: Epoch[019/020] Iteration[140/501] Loss: 3.9969\n",
      "Training: Epoch[019/020] Iteration[150/501] Loss: 2.5841\n",
      "Training: Epoch[019/020] Iteration[160/501] Loss: 3.4169\n",
      "Training: Epoch[019/020] Iteration[170/501] Loss: 2.9861\n",
      "Training: Epoch[019/020] Iteration[180/501] Loss: 2.6389\n",
      "Training: Epoch[019/020] Iteration[190/501] Loss: 2.6622\n",
      "Training: Epoch[019/020] Iteration[200/501] Loss: 2.6750\n",
      "Training: Epoch[019/020] Iteration[210/501] Loss: 3.1160\n",
      "Training: Epoch[019/020] Iteration[220/501] Loss: 3.8000\n",
      "Training: Epoch[019/020] Iteration[230/501] Loss: 3.0306\n",
      "Training: Epoch[019/020] Iteration[240/501] Loss: 3.3447\n",
      "Training: Epoch[019/020] Iteration[250/501] Loss: 3.0371\n",
      "Training: Epoch[019/020] Iteration[260/501] Loss: 2.8827\n",
      "Training: Epoch[019/020] Iteration[270/501] Loss: 2.5714\n",
      "Training: Epoch[019/020] Iteration[280/501] Loss: 3.0724\n",
      "Training: Epoch[019/020] Iteration[290/501] Loss: 3.4499\n",
      "Training: Epoch[019/020] Iteration[300/501] Loss: 3.1820\n",
      "Training: Epoch[019/020] Iteration[310/501] Loss: 3.0192\n",
      "Training: Epoch[019/020] Iteration[320/501] Loss: 3.0580\n",
      "Training: Epoch[019/020] Iteration[330/501] Loss: 3.4696\n",
      "Training: Epoch[019/020] Iteration[340/501] Loss: 2.3566\n",
      "Training: Epoch[019/020] Iteration[350/501] Loss: 2.5847\n",
      "Training: Epoch[019/020] Iteration[360/501] Loss: 2.7487\n",
      "Training: Epoch[019/020] Iteration[370/501] Loss: 2.7272\n",
      "Training: Epoch[019/020] Iteration[380/501] Loss: 3.0929\n",
      "Training: Epoch[019/020] Iteration[390/501] Loss: 3.8551\n",
      "Training: Epoch[019/020] Iteration[400/501] Loss: 2.9929\n",
      "Training: Epoch[019/020] Iteration[410/501] Loss: 2.6705\n",
      "Training: Epoch[019/020] Iteration[420/501] Loss: 2.3895\n",
      "Training: Epoch[019/020] Iteration[430/501] Loss: 2.6767\n",
      "Training: Epoch[019/020] Iteration[440/501] Loss: 2.7481\n",
      "Training: Epoch[019/020] Iteration[450/501] Loss: 3.4855\n",
      "Training: Epoch[019/020] Iteration[460/501] Loss: 3.6002\n",
      "Training: Epoch[019/020] Iteration[470/501] Loss: 3.3296\n",
      "Training: Epoch[019/020] Iteration[480/501] Loss: 2.3313\n",
      "Training: Epoch[019/020] Iteration[490/501] Loss: 3.1543\n",
      "Training: Epoch[019/020] Iteration[500/501] Loss: 4.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(59537) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid: Epoch[019/020] Iteration[501/501] Loss: 6.5877\n"
     ]
    }
   ],
   "source": [
    "train_curve = list() \n",
    "valid_curve = list()\n",
    "\n",
    "for epoch in range(start_epoch + 1, MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    res_model.train() # 将模型切换到训练模式，启用 dropout 等操作。\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # 遍历训练数据加载器，data 包含一个批次的 inputs（输入图像）和 labels（对应的标签）。\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = res_model(inputs)\n",
    "\n",
    "        # 调整标签的形状\n",
    "        labels = labels.view(-1, 1)  # 确保标签的形状与输出一致\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        # 清空上一批次的梯度，防止累积。\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # 通过自动求导计算梯度。\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # 使用优化器根据计算出的梯度更新模型参数。\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()        # 获取当前批次的损失值。\n",
    "        train_curve.append(loss.item()) # 将当前批次的损失值记录到 train_curve。\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_dataloader), loss_mean))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率 调用学习率调度器，根据设置调整当前学习率（如按一定步长下降）。\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        res_model.eval() \n",
    "        \n",
    "        # 禁用自动求导，减少内存占用，加速计算。\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(test_dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = res_model(inputs)\n",
    "               \n",
    "                # 调整标签的形状\n",
    "                labels = labels.view(-1, 1)  # 确保标签的形状与输出一致\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            loss_val_mean = loss_val / len(test_dataloader)\n",
    "            valid_curve.append(loss_val_mean)\n",
    "            print(\"Valid: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(test_dataloader), loss_val_mean))\n",
    "\n",
    "        res_model.train()  # 将模型切换回训练模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e6e845c-15f6-4296-bbe9-1792f1882aad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_dataloader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations\n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499116ed-0095-4c54-9407-2606209c8f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256e64c0-7244-4968-9eec-edbbb4ff9b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72631cd1-8d5c-40a0-8707-9bf042c31348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
