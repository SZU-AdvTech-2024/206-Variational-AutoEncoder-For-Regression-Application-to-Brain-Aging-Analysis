{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1f4610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.models as models\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843b3455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Classes: ['-250', '-249', '-248', '-247', '-246', '-245', '-244', '-243', '-242', '-241', '-240', '-239', '-238', '-237', '-236', '-235', '-234', '-233', '-232', '-231', '-230', '-229', '-228', '-227', '-226', '-225', '-224', '-223', '-222', '-221', '-220', '-219', '-218', '-217', '-216', '-215', '-214', '-213', '-212', '-211', '-210', '-209', '-208', '-207', '-206', '-205', '-204', '-203', '-202', '-201', '-200', '-199', '-198', '-197', '-196', '-195', '-194', '-193', '-192', '-191', '-190', '-189', '-188', '-187', '-186', '-185', '-184', '-183', '-182', '-181', '-180', '-179', '-178', '-177', '-176', '-175', '-174', '-173', '-172', '-171', '-170', '-169', '-168', '-167', '-166', '-165', '-164', '-163', '-162', '-161', '-160', '-159', '-158', '-157', '-156', '-155', '-154', '-153', '-152', '-151', '-150', '-149', '-148', '-147', '-146', '-145', '-144', '-143', '-142', '-141', '-140', '-139', '-138', '-137', '-136', '-135', '-134', '-133', '-132', '-131', '-130', '-129', '-128', '-127', '-126', '-125', '-124', '-123', '-122', '-121', '-120', '-119', '-118', '-117', '-116', '-115', '-114', '-113', '-112', '-111', '-110', '-109', '-108', '-107', '-106', '-105', '-104', '-103', '-102', '-101', '-100', '-99', '-98', '-97', '-96', '-95', '-94', '-93', '-92', '-91', '-90', '-89', '-88', '-87', '-86', '-85', '-84', '-83', '-82', '-81', '-80', '-79', '-78', '-77', '-76', '-75', '-74', '-73', '-72', '-71', '-70', '-69', '-68', '-67', '-66', '-65', '-64', '-63', '-62', '-61', '-60', '-59', '-58', '-57', '-56', '-55', '-54', '-53', '-52', '-51', '-50', '-49', '-48', '-47', '-46', '-45', '-44', '-43', '-42', '-41', '-40', '-39', '-38', '-37', '-36', '-35', '-34', '-33', '-32', '-31', '-30', '-29', '-28', '-27', '-26', '-25', '-24', '-23', '-22', '-21', '-20', '-19', '-18', '-17', '-16', '-15', '-14', '-13', '-12', '-11', '-10', '-9', '-8', '-7', '-6', '-5', '-4', '-3', '-2', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250']\n",
      "训练集大小: 20040\n",
      "测试集大小: 5010\n"
     ]
    }
   ],
   "source": [
    "# 设置数据集路径\n",
    "total_dir = \"/Users/fcccasa/Downloads/R/img\"\n",
    "\n",
    "# 定义数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "total_data = datasets.ImageFolder(total_dir, transform)\n",
    "\n",
    "# 获取类别并按数字顺序排序\n",
    "def sort_key(class_name):\n",
    "    # 尝试将类别名称转换为整数，若失败则返回原字符串\n",
    "    try:\n",
    "        return int(class_name)  # 如果类别是数字，则按数字排序\n",
    "    except ValueError:\n",
    "        return class_name  # 如果类别是非数字，则按字母排序\n",
    "\n",
    "sorted_classes = sorted(total_data.classes, key=sort_key)\n",
    "\n",
    "# 更新 ImageFolder 的 class_to_idx 字典\n",
    "total_data.class_to_idx = {cls: idx for idx, cls in enumerate(sorted_classes)}\n",
    "\n",
    "print(\"Sorted Classes:\", sorted_classes)\n",
    "\n",
    "# 使用排序后的类名创建训练集和测试集索引\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# 遍历数据集中的每个类别（文件夹）\n",
    "for class_idx in range(len(sorted_classes)):\n",
    "    # 获取当前类别所有图像的索引\n",
    "    class_indices = [i for i, label in enumerate(total_data.targets) if label == class_idx]\n",
    "    \n",
    "    # 分割：前40个图像为训练集，剩下的为测试集\n",
    "    train_indices.extend(class_indices[:40])\n",
    "    test_indices.extend(class_indices[40:])\n",
    "\n",
    "# 使用索引创建训练集和测试集\n",
    "train_dataset = torch.utils.data.Subset(total_data, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(total_data, test_indices)\n",
    "\n",
    "# 打印训练集和测试集的大小\n",
    "print(f'训练集大小: {len(train_dataset)}')\n",
    "print(f'测试集大小: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63c1fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=40, shuffle=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da3672d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "num_classes = 1\n",
    "\n",
    "MAX_EPOCH = 20     \n",
    "LR = 0.001         \n",
    "log_interval = 10    # 每隔 1 个 训练批次（或 epoch）记录一次训练日志。 表示训练过程中，控制打印训练信息的频率。\n",
    "val_interval = 1    # 每隔 1 个 epoch 进行一次验证集的评估。验证集的评估是为了监控模型在验证集上的表现，从而避免过拟合。\n",
    "classes = 1         # 分类任务的类别数为 2。\n",
    "start_epoch = -1     \n",
    "lr_decay_step = 5   # 表示学习率的衰减步长。 每隔一定的 epoch（如 1）对学习率进行衰减，通常是为了使模型在后期更稳定地收敛。\n",
    "                    # 可结合优化器的学习率调度策略（如 StepLR）使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fb33a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = device = torch.device(\"mps\")\n",
    "res_model = models.resnet18(pretrained=True)  # 使用预训练权重\n",
    "# 获取 ResNet 的最后一层输入特征数\n",
    "num_ftrs = res_model.fc.in_features  # 获取fc层的输入特征数\n",
    "res_model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),           # 将输入特征维度映射到4096\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)                    # 输出为1，进行回归预测任务\n",
    ")\n",
    "print(num_ftrs)\n",
    "res_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6727b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fdf1d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(res_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51d8e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[000/020] Iteration[010/501] Loss: 237.1185\n",
      "Training: Epoch[000/020] Iteration[020/501] Loss: 226.0619\n",
      "Training: Epoch[000/020] Iteration[030/501] Loss: 182.5959\n",
      "Training: Epoch[000/020] Iteration[040/501] Loss: 137.0634\n",
      "Training: Epoch[000/020] Iteration[050/501] Loss: 81.6169\n",
      "Training: Epoch[000/020] Iteration[060/501] Loss: 47.2030\n",
      "Training: Epoch[000/020] Iteration[070/501] Loss: 46.1357\n",
      "Training: Epoch[000/020] Iteration[080/501] Loss: 33.7395\n",
      "Training: Epoch[000/020] Iteration[090/501] Loss: 30.8008\n",
      "Training: Epoch[000/020] Iteration[100/501] Loss: 37.8186\n",
      "Training: Epoch[000/020] Iteration[110/501] Loss: 37.1115\n",
      "Training: Epoch[000/020] Iteration[120/501] Loss: 33.1079\n",
      "Training: Epoch[000/020] Iteration[130/501] Loss: 30.4821\n",
      "Training: Epoch[000/020] Iteration[140/501] Loss: 22.7706\n",
      "Training: Epoch[000/020] Iteration[150/501] Loss: 28.3780\n",
      "Training: Epoch[000/020] Iteration[160/501] Loss: 28.9952\n",
      "Training: Epoch[000/020] Iteration[170/501] Loss: 26.1499\n",
      "Training: Epoch[000/020] Iteration[180/501] Loss: 27.1973\n",
      "Training: Epoch[000/020] Iteration[190/501] Loss: 26.5307\n",
      "Training: Epoch[000/020] Iteration[200/501] Loss: 28.4524\n",
      "Training: Epoch[000/020] Iteration[210/501] Loss: 24.4028\n",
      "Training: Epoch[000/020] Iteration[220/501] Loss: 34.7930\n",
      "Training: Epoch[000/020] Iteration[230/501] Loss: 34.5741\n",
      "Training: Epoch[000/020] Iteration[240/501] Loss: 26.3217\n",
      "Training: Epoch[000/020] Iteration[250/501] Loss: 27.0227\n",
      "Training: Epoch[000/020] Iteration[260/501] Loss: 24.1127\n",
      "Training: Epoch[000/020] Iteration[270/501] Loss: 26.3017\n",
      "Training: Epoch[000/020] Iteration[280/501] Loss: 22.8125\n",
      "Training: Epoch[000/020] Iteration[290/501] Loss: 21.7354\n",
      "Training: Epoch[000/020] Iteration[300/501] Loss: 20.5434\n",
      "Training: Epoch[000/020] Iteration[310/501] Loss: 20.7065\n",
      "Training: Epoch[000/020] Iteration[320/501] Loss: 24.1655\n",
      "Training: Epoch[000/020] Iteration[330/501] Loss: 20.9976\n",
      "Training: Epoch[000/020] Iteration[340/501] Loss: 22.0088\n",
      "Training: Epoch[000/020] Iteration[350/501] Loss: 23.4956\n",
      "Training: Epoch[000/020] Iteration[360/501] Loss: 20.7457\n",
      "Training: Epoch[000/020] Iteration[370/501] Loss: 24.6828\n",
      "Training: Epoch[000/020] Iteration[380/501] Loss: 22.2539\n",
      "Training: Epoch[000/020] Iteration[390/501] Loss: 21.3834\n",
      "Training: Epoch[000/020] Iteration[400/501] Loss: 26.7662\n",
      "Training: Epoch[000/020] Iteration[410/501] Loss: 24.4233\n",
      "Training: Epoch[000/020] Iteration[420/501] Loss: 28.7863\n",
      "Training: Epoch[000/020] Iteration[430/501] Loss: 19.6323\n",
      "Training: Epoch[000/020] Iteration[440/501] Loss: 22.6951\n",
      "Training: Epoch[000/020] Iteration[450/501] Loss: 20.7120\n",
      "Training: Epoch[000/020] Iteration[460/501] Loss: 16.9583\n",
      "Training: Epoch[000/020] Iteration[470/501] Loss: 19.9896\n",
      "Training: Epoch[000/020] Iteration[480/501] Loss: 20.6504\n",
      "Training: Epoch[000/020] Iteration[490/501] Loss: 18.4052\n",
      "Training: Epoch[000/020] Iteration[500/501] Loss: 21.5441\n",
      "Valid: Epoch[000/020] Iteration[501/501] Loss: 23.4877\n",
      "Training: Epoch[001/020] Iteration[010/501] Loss: 21.9616\n",
      "Training: Epoch[001/020] Iteration[020/501] Loss: 17.9220\n",
      "Training: Epoch[001/020] Iteration[030/501] Loss: 21.6596\n",
      "Training: Epoch[001/020] Iteration[040/501] Loss: 21.0195\n",
      "Training: Epoch[001/020] Iteration[050/501] Loss: 20.4964\n",
      "Training: Epoch[001/020] Iteration[060/501] Loss: 15.0814\n",
      "Training: Epoch[001/020] Iteration[070/501] Loss: 13.9102\n",
      "Training: Epoch[001/020] Iteration[080/501] Loss: 18.2318\n",
      "Training: Epoch[001/020] Iteration[090/501] Loss: 17.9966\n",
      "Training: Epoch[001/020] Iteration[100/501] Loss: 22.4747\n",
      "Training: Epoch[001/020] Iteration[110/501] Loss: 18.9371\n",
      "Training: Epoch[001/020] Iteration[120/501] Loss: 18.3239\n",
      "Training: Epoch[001/020] Iteration[130/501] Loss: 18.0560\n",
      "Training: Epoch[001/020] Iteration[140/501] Loss: 21.9421\n",
      "Training: Epoch[001/020] Iteration[150/501] Loss: 24.8710\n",
      "Training: Epoch[001/020] Iteration[160/501] Loss: 20.4994\n",
      "Training: Epoch[001/020] Iteration[170/501] Loss: 18.6064\n",
      "Training: Epoch[001/020] Iteration[180/501] Loss: 16.9626\n",
      "Training: Epoch[001/020] Iteration[190/501] Loss: 19.6133\n",
      "Training: Epoch[001/020] Iteration[200/501] Loss: 19.3972\n",
      "Training: Epoch[001/020] Iteration[210/501] Loss: 16.9874\n",
      "Training: Epoch[001/020] Iteration[220/501] Loss: 16.0522\n",
      "Training: Epoch[001/020] Iteration[230/501] Loss: 16.4020\n",
      "Training: Epoch[001/020] Iteration[240/501] Loss: 14.9876\n",
      "Training: Epoch[001/020] Iteration[250/501] Loss: 18.6005\n",
      "Training: Epoch[001/020] Iteration[260/501] Loss: 19.1821\n",
      "Training: Epoch[001/020] Iteration[270/501] Loss: 19.6588\n",
      "Training: Epoch[001/020] Iteration[280/501] Loss: 20.7496\n",
      "Training: Epoch[001/020] Iteration[290/501] Loss: 16.0883\n",
      "Training: Epoch[001/020] Iteration[300/501] Loss: 18.6463\n",
      "Training: Epoch[001/020] Iteration[310/501] Loss: 17.4164\n",
      "Training: Epoch[001/020] Iteration[320/501] Loss: 17.3586\n",
      "Training: Epoch[001/020] Iteration[330/501] Loss: 16.5001\n",
      "Training: Epoch[001/020] Iteration[340/501] Loss: 16.9641\n",
      "Training: Epoch[001/020] Iteration[350/501] Loss: 16.3821\n",
      "Training: Epoch[001/020] Iteration[360/501] Loss: 16.3406\n",
      "Training: Epoch[001/020] Iteration[370/501] Loss: 17.4578\n",
      "Training: Epoch[001/020] Iteration[380/501] Loss: 15.4093\n",
      "Training: Epoch[001/020] Iteration[390/501] Loss: 17.7163\n",
      "Training: Epoch[001/020] Iteration[400/501] Loss: 17.9892\n",
      "Training: Epoch[001/020] Iteration[410/501] Loss: 18.0039\n",
      "Training: Epoch[001/020] Iteration[420/501] Loss: 14.2798\n",
      "Training: Epoch[001/020] Iteration[430/501] Loss: 17.9556\n",
      "Training: Epoch[001/020] Iteration[440/501] Loss: 16.3683\n",
      "Training: Epoch[001/020] Iteration[450/501] Loss: 15.6459\n",
      "Training: Epoch[001/020] Iteration[460/501] Loss: 16.7102\n",
      "Training: Epoch[001/020] Iteration[470/501] Loss: 18.5641\n",
      "Training: Epoch[001/020] Iteration[480/501] Loss: 18.9270\n",
      "Training: Epoch[001/020] Iteration[490/501] Loss: 16.7406\n",
      "Training: Epoch[001/020] Iteration[500/501] Loss: 14.6095\n",
      "Valid: Epoch[001/020] Iteration[501/501] Loss: 14.7361\n",
      "Training: Epoch[002/020] Iteration[010/501] Loss: 17.1015\n",
      "Training: Epoch[002/020] Iteration[020/501] Loss: 16.8729\n",
      "Training: Epoch[002/020] Iteration[030/501] Loss: 17.0395\n",
      "Training: Epoch[002/020] Iteration[040/501] Loss: 14.4516\n",
      "Training: Epoch[002/020] Iteration[050/501] Loss: 16.6576\n",
      "Training: Epoch[002/020] Iteration[060/501] Loss: 17.5418\n",
      "Training: Epoch[002/020] Iteration[070/501] Loss: 17.1209\n",
      "Training: Epoch[002/020] Iteration[080/501] Loss: 16.4594\n",
      "Training: Epoch[002/020] Iteration[090/501] Loss: 15.8062\n",
      "Training: Epoch[002/020] Iteration[100/501] Loss: 19.0566\n",
      "Training: Epoch[002/020] Iteration[110/501] Loss: 20.0275\n",
      "Training: Epoch[002/020] Iteration[120/501] Loss: 14.2074\n",
      "Training: Epoch[002/020] Iteration[130/501] Loss: 12.0435\n",
      "Training: Epoch[002/020] Iteration[140/501] Loss: 14.9460\n",
      "Training: Epoch[002/020] Iteration[150/501] Loss: 13.7357\n",
      "Training: Epoch[002/020] Iteration[160/501] Loss: 15.7044\n",
      "Training: Epoch[002/020] Iteration[170/501] Loss: 17.1271\n",
      "Training: Epoch[002/020] Iteration[180/501] Loss: 15.0019\n",
      "Training: Epoch[002/020] Iteration[190/501] Loss: 16.2459\n",
      "Training: Epoch[002/020] Iteration[200/501] Loss: 16.7297\n",
      "Training: Epoch[002/020] Iteration[210/501] Loss: 15.0265\n",
      "Training: Epoch[002/020] Iteration[220/501] Loss: 12.3074\n",
      "Training: Epoch[002/020] Iteration[230/501] Loss: 15.2762\n",
      "Training: Epoch[002/020] Iteration[240/501] Loss: 14.9206\n",
      "Training: Epoch[002/020] Iteration[250/501] Loss: 14.0859\n",
      "Training: Epoch[002/020] Iteration[260/501] Loss: 14.9764\n",
      "Training: Epoch[002/020] Iteration[270/501] Loss: 11.1811\n",
      "Training: Epoch[002/020] Iteration[280/501] Loss: 14.0384\n",
      "Training: Epoch[002/020] Iteration[290/501] Loss: 12.9813\n",
      "Training: Epoch[002/020] Iteration[300/501] Loss: 15.9934\n",
      "Training: Epoch[002/020] Iteration[310/501] Loss: 15.6459\n",
      "Training: Epoch[002/020] Iteration[320/501] Loss: 18.7192\n",
      "Training: Epoch[002/020] Iteration[330/501] Loss: 15.5261\n",
      "Training: Epoch[002/020] Iteration[340/501] Loss: 11.8854\n",
      "Training: Epoch[002/020] Iteration[350/501] Loss: 15.5723\n",
      "Training: Epoch[002/020] Iteration[360/501] Loss: 9.6942\n",
      "Training: Epoch[002/020] Iteration[370/501] Loss: 15.2318\n",
      "Training: Epoch[002/020] Iteration[380/501] Loss: 13.5694\n",
      "Training: Epoch[002/020] Iteration[390/501] Loss: 14.6324\n",
      "Training: Epoch[002/020] Iteration[400/501] Loss: 11.6398\n",
      "Training: Epoch[002/020] Iteration[410/501] Loss: 13.3240\n",
      "Training: Epoch[002/020] Iteration[420/501] Loss: 12.5566\n",
      "Training: Epoch[002/020] Iteration[430/501] Loss: 15.2156\n",
      "Training: Epoch[002/020] Iteration[440/501] Loss: 16.1000\n",
      "Training: Epoch[002/020] Iteration[450/501] Loss: 13.9181\n",
      "Training: Epoch[002/020] Iteration[460/501] Loss: 13.9747\n",
      "Training: Epoch[002/020] Iteration[470/501] Loss: 14.2254\n",
      "Training: Epoch[002/020] Iteration[480/501] Loss: 14.0085\n",
      "Training: Epoch[002/020] Iteration[490/501] Loss: 14.2921\n",
      "Training: Epoch[002/020] Iteration[500/501] Loss: 14.5404\n",
      "Valid: Epoch[002/020] Iteration[501/501] Loss: 12.9308\n",
      "Training: Epoch[003/020] Iteration[010/501] Loss: 12.9610\n",
      "Training: Epoch[003/020] Iteration[020/501] Loss: 14.2513\n",
      "Training: Epoch[003/020] Iteration[030/501] Loss: 13.7585\n",
      "Training: Epoch[003/020] Iteration[040/501] Loss: 10.2365\n",
      "Training: Epoch[003/020] Iteration[050/501] Loss: 12.2973\n",
      "Training: Epoch[003/020] Iteration[060/501] Loss: 14.2348\n",
      "Training: Epoch[003/020] Iteration[070/501] Loss: 14.2872\n",
      "Training: Epoch[003/020] Iteration[080/501] Loss: 12.6264\n",
      "Training: Epoch[003/020] Iteration[090/501] Loss: 11.9084\n",
      "Training: Epoch[003/020] Iteration[100/501] Loss: 14.3996\n",
      "Training: Epoch[003/020] Iteration[110/501] Loss: 13.4813\n",
      "Training: Epoch[003/020] Iteration[120/501] Loss: 14.7837\n",
      "Training: Epoch[003/020] Iteration[130/501] Loss: 11.5779\n",
      "Training: Epoch[003/020] Iteration[140/501] Loss: 15.2137\n",
      "Training: Epoch[003/020] Iteration[150/501] Loss: 15.3138\n",
      "Training: Epoch[003/020] Iteration[160/501] Loss: 16.1020\n",
      "Training: Epoch[003/020] Iteration[170/501] Loss: 12.8375\n",
      "Training: Epoch[003/020] Iteration[180/501] Loss: 10.5861\n",
      "Training: Epoch[003/020] Iteration[190/501] Loss: 13.3371\n",
      "Training: Epoch[003/020] Iteration[200/501] Loss: 13.2318\n",
      "Training: Epoch[003/020] Iteration[210/501] Loss: 10.2638\n",
      "Training: Epoch[003/020] Iteration[220/501] Loss: 10.4712\n",
      "Training: Epoch[003/020] Iteration[230/501] Loss: 14.4303\n",
      "Training: Epoch[003/020] Iteration[240/501] Loss: 14.9827\n",
      "Training: Epoch[003/020] Iteration[250/501] Loss: 15.5919\n",
      "Training: Epoch[003/020] Iteration[260/501] Loss: 12.5529\n",
      "Training: Epoch[003/020] Iteration[270/501] Loss: 13.2011\n",
      "Training: Epoch[003/020] Iteration[280/501] Loss: 13.0079\n",
      "Training: Epoch[003/020] Iteration[290/501] Loss: 12.0484\n",
      "Training: Epoch[003/020] Iteration[300/501] Loss: 11.0460\n",
      "Training: Epoch[003/020] Iteration[310/501] Loss: 13.5659\n",
      "Training: Epoch[003/020] Iteration[320/501] Loss: 12.0966\n",
      "Training: Epoch[003/020] Iteration[330/501] Loss: 13.2213\n",
      "Training: Epoch[003/020] Iteration[340/501] Loss: 11.8422\n",
      "Training: Epoch[003/020] Iteration[350/501] Loss: 13.5737\n",
      "Training: Epoch[003/020] Iteration[360/501] Loss: 12.4239\n",
      "Training: Epoch[003/020] Iteration[370/501] Loss: 14.0192\n",
      "Training: Epoch[003/020] Iteration[380/501] Loss: 13.8430\n",
      "Training: Epoch[003/020] Iteration[390/501] Loss: 15.2729\n",
      "Training: Epoch[003/020] Iteration[400/501] Loss: 14.6229\n",
      "Training: Epoch[003/020] Iteration[410/501] Loss: 11.8388\n",
      "Training: Epoch[003/020] Iteration[420/501] Loss: 10.6214\n",
      "Training: Epoch[003/020] Iteration[430/501] Loss: 15.3751\n",
      "Training: Epoch[003/020] Iteration[440/501] Loss: 11.5153\n",
      "Training: Epoch[003/020] Iteration[450/501] Loss: 13.8845\n",
      "Training: Epoch[003/020] Iteration[460/501] Loss: 12.3778\n",
      "Training: Epoch[003/020] Iteration[470/501] Loss: 9.7874\n",
      "Training: Epoch[003/020] Iteration[480/501] Loss: 14.5016\n",
      "Training: Epoch[003/020] Iteration[490/501] Loss: 14.8975\n",
      "Training: Epoch[003/020] Iteration[500/501] Loss: 13.3315\n",
      "Valid: Epoch[003/020] Iteration[501/501] Loss: 15.1262\n",
      "Training: Epoch[004/020] Iteration[010/501] Loss: 13.0798\n",
      "Training: Epoch[004/020] Iteration[020/501] Loss: 10.9784\n",
      "Training: Epoch[004/020] Iteration[030/501] Loss: 11.7859\n",
      "Training: Epoch[004/020] Iteration[040/501] Loss: 11.3281\n",
      "Training: Epoch[004/020] Iteration[050/501] Loss: 13.9980\n",
      "Training: Epoch[004/020] Iteration[060/501] Loss: 12.4840\n",
      "Training: Epoch[004/020] Iteration[070/501] Loss: 13.4065\n",
      "Training: Epoch[004/020] Iteration[080/501] Loss: 12.2704\n",
      "Training: Epoch[004/020] Iteration[090/501] Loss: 10.6921\n",
      "Training: Epoch[004/020] Iteration[100/501] Loss: 11.6060\n",
      "Training: Epoch[004/020] Iteration[110/501] Loss: 11.6478\n",
      "Training: Epoch[004/020] Iteration[120/501] Loss: 14.9116\n",
      "Training: Epoch[004/020] Iteration[130/501] Loss: 16.3923\n",
      "Training: Epoch[004/020] Iteration[140/501] Loss: 14.3108\n",
      "Training: Epoch[004/020] Iteration[150/501] Loss: 13.5801\n",
      "Training: Epoch[004/020] Iteration[160/501] Loss: 12.3843\n",
      "Training: Epoch[004/020] Iteration[170/501] Loss: 12.0734\n",
      "Training: Epoch[004/020] Iteration[180/501] Loss: 9.9322\n",
      "Training: Epoch[004/020] Iteration[190/501] Loss: 11.2064\n",
      "Training: Epoch[004/020] Iteration[200/501] Loss: 13.0888\n",
      "Training: Epoch[004/020] Iteration[210/501] Loss: 10.7374\n",
      "Training: Epoch[004/020] Iteration[220/501] Loss: 11.0750\n",
      "Training: Epoch[004/020] Iteration[230/501] Loss: 12.4776\n",
      "Training: Epoch[004/020] Iteration[240/501] Loss: 11.3488\n",
      "Training: Epoch[004/020] Iteration[250/501] Loss: 15.8819\n",
      "Training: Epoch[004/020] Iteration[260/501] Loss: 11.8823\n",
      "Training: Epoch[004/020] Iteration[270/501] Loss: 12.9395\n",
      "Training: Epoch[004/020] Iteration[280/501] Loss: 12.5337\n",
      "Training: Epoch[004/020] Iteration[290/501] Loss: 11.9746\n",
      "Training: Epoch[004/020] Iteration[300/501] Loss: 11.7099\n",
      "Training: Epoch[004/020] Iteration[310/501] Loss: 13.7803\n",
      "Training: Epoch[004/020] Iteration[320/501] Loss: 14.7240\n",
      "Training: Epoch[004/020] Iteration[330/501] Loss: 11.8578\n",
      "Training: Epoch[004/020] Iteration[340/501] Loss: 13.5315\n",
      "Training: Epoch[004/020] Iteration[350/501] Loss: 14.0193\n",
      "Training: Epoch[004/020] Iteration[360/501] Loss: 11.5500\n",
      "Training: Epoch[004/020] Iteration[370/501] Loss: 13.2272\n",
      "Training: Epoch[004/020] Iteration[380/501] Loss: 12.7899\n",
      "Training: Epoch[004/020] Iteration[390/501] Loss: 10.3868\n",
      "Training: Epoch[004/020] Iteration[400/501] Loss: 11.1794\n",
      "Training: Epoch[004/020] Iteration[410/501] Loss: 8.0132\n",
      "Training: Epoch[004/020] Iteration[420/501] Loss: 11.1162\n",
      "Training: Epoch[004/020] Iteration[430/501] Loss: 14.7749\n",
      "Training: Epoch[004/020] Iteration[440/501] Loss: 11.3599\n",
      "Training: Epoch[004/020] Iteration[450/501] Loss: 10.9069\n",
      "Training: Epoch[004/020] Iteration[460/501] Loss: 10.2211\n",
      "Training: Epoch[004/020] Iteration[470/501] Loss: 10.7275\n",
      "Training: Epoch[004/020] Iteration[480/501] Loss: 12.1110\n",
      "Training: Epoch[004/020] Iteration[490/501] Loss: 10.2690\n",
      "Training: Epoch[004/020] Iteration[500/501] Loss: 11.2041\n",
      "Valid: Epoch[004/020] Iteration[501/501] Loss: 14.9799\n",
      "Training: Epoch[005/020] Iteration[010/501] Loss: 9.9495\n",
      "Training: Epoch[005/020] Iteration[020/501] Loss: 8.1908\n",
      "Training: Epoch[005/020] Iteration[030/501] Loss: 8.7736\n",
      "Training: Epoch[005/020] Iteration[040/501] Loss: 7.4107\n",
      "Training: Epoch[005/020] Iteration[050/501] Loss: 7.7929\n",
      "Training: Epoch[005/020] Iteration[060/501] Loss: 11.6179\n",
      "Training: Epoch[005/020] Iteration[070/501] Loss: 8.7379\n",
      "Training: Epoch[005/020] Iteration[080/501] Loss: 8.7012\n",
      "Training: Epoch[005/020] Iteration[090/501] Loss: 7.5637\n",
      "Training: Epoch[005/020] Iteration[100/501] Loss: 7.3621\n",
      "Training: Epoch[005/020] Iteration[110/501] Loss: 10.2695\n",
      "Training: Epoch[005/020] Iteration[120/501] Loss: 9.0486\n",
      "Training: Epoch[005/020] Iteration[130/501] Loss: 7.9075\n",
      "Training: Epoch[005/020] Iteration[140/501] Loss: 7.3563\n",
      "Training: Epoch[005/020] Iteration[150/501] Loss: 9.7730\n",
      "Training: Epoch[005/020] Iteration[160/501] Loss: 7.9681\n",
      "Training: Epoch[005/020] Iteration[170/501] Loss: 8.5774\n",
      "Training: Epoch[005/020] Iteration[180/501] Loss: 7.5586\n",
      "Training: Epoch[005/020] Iteration[190/501] Loss: 7.2713\n",
      "Training: Epoch[005/020] Iteration[200/501] Loss: 8.6452\n",
      "Training: Epoch[005/020] Iteration[210/501] Loss: 7.0020\n",
      "Training: Epoch[005/020] Iteration[220/501] Loss: 6.5858\n",
      "Training: Epoch[005/020] Iteration[230/501] Loss: 7.6554\n",
      "Training: Epoch[005/020] Iteration[240/501] Loss: 5.6202\n",
      "Training: Epoch[005/020] Iteration[250/501] Loss: 7.8290\n",
      "Training: Epoch[005/020] Iteration[260/501] Loss: 7.9571\n",
      "Training: Epoch[005/020] Iteration[270/501] Loss: 8.5126\n",
      "Training: Epoch[005/020] Iteration[280/501] Loss: 10.2805\n",
      "Training: Epoch[005/020] Iteration[290/501] Loss: 6.6323\n",
      "Training: Epoch[005/020] Iteration[300/501] Loss: 8.2963\n",
      "Training: Epoch[005/020] Iteration[310/501] Loss: 7.4127\n",
      "Training: Epoch[005/020] Iteration[320/501] Loss: 6.9837\n",
      "Training: Epoch[005/020] Iteration[330/501] Loss: 8.3732\n",
      "Training: Epoch[005/020] Iteration[340/501] Loss: 7.0063\n",
      "Training: Epoch[005/020] Iteration[350/501] Loss: 6.2057\n",
      "Training: Epoch[005/020] Iteration[360/501] Loss: 7.9186\n",
      "Training: Epoch[005/020] Iteration[370/501] Loss: 9.4088\n",
      "Training: Epoch[005/020] Iteration[380/501] Loss: 6.7924\n",
      "Training: Epoch[005/020] Iteration[390/501] Loss: 8.3408\n",
      "Training: Epoch[005/020] Iteration[400/501] Loss: 8.0537\n",
      "Training: Epoch[005/020] Iteration[410/501] Loss: 6.5150\n",
      "Training: Epoch[005/020] Iteration[420/501] Loss: 7.2299\n",
      "Training: Epoch[005/020] Iteration[430/501] Loss: 4.9679\n",
      "Training: Epoch[005/020] Iteration[440/501] Loss: 6.4461\n",
      "Training: Epoch[005/020] Iteration[450/501] Loss: 6.4661\n",
      "Training: Epoch[005/020] Iteration[460/501] Loss: 7.5159\n",
      "Training: Epoch[005/020] Iteration[470/501] Loss: 6.4541\n",
      "Training: Epoch[005/020] Iteration[480/501] Loss: 6.5987\n",
      "Training: Epoch[005/020] Iteration[490/501] Loss: 4.9465\n",
      "Training: Epoch[005/020] Iteration[500/501] Loss: 6.9800\n",
      "Valid: Epoch[005/020] Iteration[501/501] Loss: 8.2860\n",
      "Training: Epoch[006/020] Iteration[010/501] Loss: 6.2740\n",
      "Training: Epoch[006/020] Iteration[020/501] Loss: 8.0325\n",
      "Training: Epoch[006/020] Iteration[030/501] Loss: 10.2952\n",
      "Training: Epoch[006/020] Iteration[040/501] Loss: 6.6140\n",
      "Training: Epoch[006/020] Iteration[050/501] Loss: 6.7217\n",
      "Training: Epoch[006/020] Iteration[060/501] Loss: 8.1598\n",
      "Training: Epoch[006/020] Iteration[070/501] Loss: 6.8647\n",
      "Training: Epoch[006/020] Iteration[080/501] Loss: 6.5839\n",
      "Training: Epoch[006/020] Iteration[090/501] Loss: 8.1028\n",
      "Training: Epoch[006/020] Iteration[100/501] Loss: 6.1759\n",
      "Training: Epoch[006/020] Iteration[110/501] Loss: 7.7615\n",
      "Training: Epoch[006/020] Iteration[120/501] Loss: 5.0435\n",
      "Training: Epoch[006/020] Iteration[130/501] Loss: 5.7690\n",
      "Training: Epoch[006/020] Iteration[140/501] Loss: 5.7359\n",
      "Training: Epoch[006/020] Iteration[150/501] Loss: 7.4938\n",
      "Training: Epoch[006/020] Iteration[160/501] Loss: 6.8556\n",
      "Training: Epoch[006/020] Iteration[170/501] Loss: 5.7374\n",
      "Training: Epoch[006/020] Iteration[180/501] Loss: 7.6473\n",
      "Training: Epoch[006/020] Iteration[190/501] Loss: 6.9583\n",
      "Training: Epoch[006/020] Iteration[200/501] Loss: 5.5182\n",
      "Training: Epoch[006/020] Iteration[210/501] Loss: 5.4858\n",
      "Training: Epoch[006/020] Iteration[220/501] Loss: 6.0459\n",
      "Training: Epoch[006/020] Iteration[230/501] Loss: 6.6133\n",
      "Training: Epoch[006/020] Iteration[240/501] Loss: 6.7677\n",
      "Training: Epoch[006/020] Iteration[250/501] Loss: 7.1224\n",
      "Training: Epoch[006/020] Iteration[260/501] Loss: 5.8361\n",
      "Training: Epoch[006/020] Iteration[270/501] Loss: 4.4742\n",
      "Training: Epoch[006/020] Iteration[280/501] Loss: 7.6705\n",
      "Training: Epoch[006/020] Iteration[290/501] Loss: 7.4496\n",
      "Training: Epoch[006/020] Iteration[300/501] Loss: 6.7991\n",
      "Training: Epoch[006/020] Iteration[310/501] Loss: 6.8822\n",
      "Training: Epoch[006/020] Iteration[320/501] Loss: 8.2138\n",
      "Training: Epoch[006/020] Iteration[330/501] Loss: 7.0147\n",
      "Training: Epoch[006/020] Iteration[340/501] Loss: 8.9929\n",
      "Training: Epoch[006/020] Iteration[350/501] Loss: 8.0374\n",
      "Training: Epoch[006/020] Iteration[360/501] Loss: 5.4734\n",
      "Training: Epoch[006/020] Iteration[370/501] Loss: 6.1318\n",
      "Training: Epoch[006/020] Iteration[380/501] Loss: 6.2310\n",
      "Training: Epoch[006/020] Iteration[390/501] Loss: 5.9863\n",
      "Training: Epoch[006/020] Iteration[400/501] Loss: 6.2532\n",
      "Training: Epoch[006/020] Iteration[410/501] Loss: 7.2573\n",
      "Training: Epoch[006/020] Iteration[420/501] Loss: 7.3966\n",
      "Training: Epoch[006/020] Iteration[430/501] Loss: 7.4743\n",
      "Training: Epoch[006/020] Iteration[440/501] Loss: 6.5088\n",
      "Training: Epoch[006/020] Iteration[450/501] Loss: 6.4936\n",
      "Training: Epoch[006/020] Iteration[460/501] Loss: 5.9980\n",
      "Training: Epoch[006/020] Iteration[470/501] Loss: 7.6139\n",
      "Training: Epoch[006/020] Iteration[480/501] Loss: 6.8204\n",
      "Training: Epoch[006/020] Iteration[490/501] Loss: 6.5927\n",
      "Training: Epoch[006/020] Iteration[500/501] Loss: 6.5466\n",
      "Valid: Epoch[006/020] Iteration[501/501] Loss: 8.1432\n",
      "Training: Epoch[007/020] Iteration[010/501] Loss: 7.1006\n",
      "Training: Epoch[007/020] Iteration[020/501] Loss: 5.0442\n",
      "Training: Epoch[007/020] Iteration[030/501] Loss: 5.1656\n",
      "Training: Epoch[007/020] Iteration[040/501] Loss: 6.8583\n",
      "Training: Epoch[007/020] Iteration[050/501] Loss: 5.7589\n",
      "Training: Epoch[007/020] Iteration[060/501] Loss: 6.5831\n",
      "Training: Epoch[007/020] Iteration[070/501] Loss: 5.0977\n",
      "Training: Epoch[007/020] Iteration[080/501] Loss: 5.2677\n",
      "Training: Epoch[007/020] Iteration[090/501] Loss: 5.0049\n",
      "Training: Epoch[007/020] Iteration[100/501] Loss: 6.6723\n",
      "Training: Epoch[007/020] Iteration[110/501] Loss: 7.0462\n",
      "Training: Epoch[007/020] Iteration[120/501] Loss: 7.7005\n",
      "Training: Epoch[007/020] Iteration[130/501] Loss: 5.8202\n",
      "Training: Epoch[007/020] Iteration[140/501] Loss: 6.4999\n",
      "Training: Epoch[007/020] Iteration[150/501] Loss: 5.8178\n",
      "Training: Epoch[007/020] Iteration[160/501] Loss: 6.1548\n",
      "Training: Epoch[007/020] Iteration[170/501] Loss: 6.8519\n",
      "Training: Epoch[007/020] Iteration[180/501] Loss: 5.6560\n",
      "Training: Epoch[007/020] Iteration[190/501] Loss: 4.7183\n",
      "Training: Epoch[007/020] Iteration[200/501] Loss: 5.0955\n",
      "Training: Epoch[007/020] Iteration[210/501] Loss: 7.5500\n",
      "Training: Epoch[007/020] Iteration[220/501] Loss: 7.5336\n",
      "Training: Epoch[007/020] Iteration[230/501] Loss: 6.4644\n",
      "Training: Epoch[007/020] Iteration[240/501] Loss: 6.5598\n",
      "Training: Epoch[007/020] Iteration[250/501] Loss: 4.6142\n",
      "Training: Epoch[007/020] Iteration[260/501] Loss: 5.3489\n",
      "Training: Epoch[007/020] Iteration[270/501] Loss: 6.6394\n",
      "Training: Epoch[007/020] Iteration[280/501] Loss: 5.4699\n",
      "Training: Epoch[007/020] Iteration[290/501] Loss: 6.3251\n",
      "Training: Epoch[007/020] Iteration[300/501] Loss: 5.7155\n",
      "Training: Epoch[007/020] Iteration[310/501] Loss: 4.6859\n",
      "Training: Epoch[007/020] Iteration[320/501] Loss: 6.7175\n",
      "Training: Epoch[007/020] Iteration[330/501] Loss: 6.5564\n",
      "Training: Epoch[007/020] Iteration[340/501] Loss: 5.6242\n",
      "Training: Epoch[007/020] Iteration[350/501] Loss: 7.2721\n",
      "Training: Epoch[007/020] Iteration[360/501] Loss: 6.5090\n",
      "Training: Epoch[007/020] Iteration[370/501] Loss: 7.9309\n",
      "Training: Epoch[007/020] Iteration[380/501] Loss: 4.9243\n",
      "Training: Epoch[007/020] Iteration[390/501] Loss: 5.5637\n",
      "Training: Epoch[007/020] Iteration[400/501] Loss: 7.2469\n",
      "Training: Epoch[007/020] Iteration[410/501] Loss: 5.8761\n",
      "Training: Epoch[007/020] Iteration[420/501] Loss: 6.9472\n",
      "Training: Epoch[007/020] Iteration[430/501] Loss: 7.5448\n",
      "Training: Epoch[007/020] Iteration[440/501] Loss: 6.3789\n",
      "Training: Epoch[007/020] Iteration[450/501] Loss: 5.2425\n",
      "Training: Epoch[007/020] Iteration[460/501] Loss: 7.0689\n",
      "Training: Epoch[007/020] Iteration[470/501] Loss: 7.1352\n",
      "Training: Epoch[007/020] Iteration[480/501] Loss: 5.2868\n",
      "Training: Epoch[007/020] Iteration[490/501] Loss: 5.0428\n",
      "Training: Epoch[007/020] Iteration[500/501] Loss: 7.6999\n",
      "Valid: Epoch[007/020] Iteration[501/501] Loss: 7.2236\n",
      "Training: Epoch[008/020] Iteration[010/501] Loss: 6.8559\n",
      "Training: Epoch[008/020] Iteration[020/501] Loss: 5.5596\n",
      "Training: Epoch[008/020] Iteration[030/501] Loss: 6.6428\n",
      "Training: Epoch[008/020] Iteration[040/501] Loss: 3.7837\n",
      "Training: Epoch[008/020] Iteration[050/501] Loss: 5.6617\n",
      "Training: Epoch[008/020] Iteration[060/501] Loss: 6.2859\n",
      "Training: Epoch[008/020] Iteration[070/501] Loss: 4.4421\n",
      "Training: Epoch[008/020] Iteration[080/501] Loss: 5.8685\n",
      "Training: Epoch[008/020] Iteration[090/501] Loss: 4.8840\n",
      "Training: Epoch[008/020] Iteration[100/501] Loss: 6.4411\n",
      "Training: Epoch[008/020] Iteration[110/501] Loss: 5.2694\n",
      "Training: Epoch[008/020] Iteration[120/501] Loss: 6.7483\n",
      "Training: Epoch[008/020] Iteration[130/501] Loss: 5.5730\n",
      "Training: Epoch[008/020] Iteration[140/501] Loss: 6.0911\n",
      "Training: Epoch[008/020] Iteration[150/501] Loss: 4.8102\n",
      "Training: Epoch[008/020] Iteration[160/501] Loss: 6.5241\n",
      "Training: Epoch[008/020] Iteration[170/501] Loss: 4.2393\n",
      "Training: Epoch[008/020] Iteration[180/501] Loss: 6.3393\n",
      "Training: Epoch[008/020] Iteration[190/501] Loss: 6.1358\n",
      "Training: Epoch[008/020] Iteration[200/501] Loss: 6.0512\n",
      "Training: Epoch[008/020] Iteration[210/501] Loss: 5.5820\n",
      "Training: Epoch[008/020] Iteration[220/501] Loss: 5.7592\n",
      "Training: Epoch[008/020] Iteration[230/501] Loss: 7.7348\n",
      "Training: Epoch[008/020] Iteration[240/501] Loss: 4.9601\n",
      "Training: Epoch[008/020] Iteration[250/501] Loss: 5.8367\n",
      "Training: Epoch[008/020] Iteration[260/501] Loss: 5.2747\n",
      "Training: Epoch[008/020] Iteration[270/501] Loss: 5.7370\n",
      "Training: Epoch[008/020] Iteration[280/501] Loss: 5.2990\n",
      "Training: Epoch[008/020] Iteration[290/501] Loss: 4.6338\n",
      "Training: Epoch[008/020] Iteration[300/501] Loss: 5.4675\n",
      "Training: Epoch[008/020] Iteration[310/501] Loss: 5.1370\n",
      "Training: Epoch[008/020] Iteration[320/501] Loss: 6.3403\n",
      "Training: Epoch[008/020] Iteration[330/501] Loss: 6.6236\n",
      "Training: Epoch[008/020] Iteration[340/501] Loss: 5.6262\n",
      "Training: Epoch[008/020] Iteration[350/501] Loss: 5.3288\n",
      "Training: Epoch[008/020] Iteration[360/501] Loss: 6.8892\n",
      "Training: Epoch[008/020] Iteration[370/501] Loss: 4.2638\n",
      "Training: Epoch[008/020] Iteration[380/501] Loss: 4.6962\n",
      "Training: Epoch[008/020] Iteration[390/501] Loss: 6.1828\n",
      "Training: Epoch[008/020] Iteration[400/501] Loss: 6.4781\n",
      "Training: Epoch[008/020] Iteration[410/501] Loss: 6.3208\n",
      "Training: Epoch[008/020] Iteration[420/501] Loss: 5.6645\n",
      "Training: Epoch[008/020] Iteration[430/501] Loss: 4.1022\n",
      "Training: Epoch[008/020] Iteration[440/501] Loss: 5.8475\n",
      "Training: Epoch[008/020] Iteration[450/501] Loss: 5.1730\n",
      "Training: Epoch[008/020] Iteration[460/501] Loss: 6.8589\n",
      "Training: Epoch[008/020] Iteration[470/501] Loss: 4.5757\n",
      "Training: Epoch[008/020] Iteration[480/501] Loss: 8.4779\n",
      "Training: Epoch[008/020] Iteration[490/501] Loss: 5.4684\n",
      "Training: Epoch[008/020] Iteration[500/501] Loss: 8.2084\n",
      "Valid: Epoch[008/020] Iteration[501/501] Loss: 7.2743\n",
      "Training: Epoch[009/020] Iteration[010/501] Loss: 4.5098\n",
      "Training: Epoch[009/020] Iteration[020/501] Loss: 6.4875\n",
      "Training: Epoch[009/020] Iteration[030/501] Loss: 9.0990\n",
      "Training: Epoch[009/020] Iteration[040/501] Loss: 7.6261\n",
      "Training: Epoch[009/020] Iteration[050/501] Loss: 4.3583\n",
      "Training: Epoch[009/020] Iteration[060/501] Loss: 6.7297\n",
      "Training: Epoch[009/020] Iteration[070/501] Loss: 8.3644\n",
      "Training: Epoch[009/020] Iteration[080/501] Loss: 6.2030\n",
      "Training: Epoch[009/020] Iteration[090/501] Loss: 3.9871\n",
      "Training: Epoch[009/020] Iteration[100/501] Loss: 6.0704\n",
      "Training: Epoch[009/020] Iteration[110/501] Loss: 5.0757\n",
      "Training: Epoch[009/020] Iteration[120/501] Loss: 6.4979\n",
      "Training: Epoch[009/020] Iteration[130/501] Loss: 4.5967\n",
      "Training: Epoch[009/020] Iteration[140/501] Loss: 5.6690\n",
      "Training: Epoch[009/020] Iteration[150/501] Loss: 4.9921\n",
      "Training: Epoch[009/020] Iteration[160/501] Loss: 5.1637\n",
      "Training: Epoch[009/020] Iteration[170/501] Loss: 5.4661\n",
      "Training: Epoch[009/020] Iteration[180/501] Loss: 4.7943\n",
      "Training: Epoch[009/020] Iteration[190/501] Loss: 6.3205\n",
      "Training: Epoch[009/020] Iteration[200/501] Loss: 4.6210\n",
      "Training: Epoch[009/020] Iteration[210/501] Loss: 5.0852\n",
      "Training: Epoch[009/020] Iteration[220/501] Loss: 4.8402\n",
      "Training: Epoch[009/020] Iteration[230/501] Loss: 4.9562\n",
      "Training: Epoch[009/020] Iteration[240/501] Loss: 5.9823\n",
      "Training: Epoch[009/020] Iteration[250/501] Loss: 4.8712\n",
      "Training: Epoch[009/020] Iteration[260/501] Loss: 5.3720\n",
      "Training: Epoch[009/020] Iteration[270/501] Loss: 4.6832\n",
      "Training: Epoch[009/020] Iteration[280/501] Loss: 5.0872\n",
      "Training: Epoch[009/020] Iteration[290/501] Loss: 3.9422\n",
      "Training: Epoch[009/020] Iteration[300/501] Loss: 4.3022\n",
      "Training: Epoch[009/020] Iteration[310/501] Loss: 5.0985\n",
      "Training: Epoch[009/020] Iteration[320/501] Loss: 6.6983\n",
      "Training: Epoch[009/020] Iteration[330/501] Loss: 5.0311\n",
      "Training: Epoch[009/020] Iteration[340/501] Loss: 4.6543\n",
      "Training: Epoch[009/020] Iteration[350/501] Loss: 5.2403\n",
      "Training: Epoch[009/020] Iteration[360/501] Loss: 5.5429\n",
      "Training: Epoch[009/020] Iteration[370/501] Loss: 4.6199\n",
      "Training: Epoch[009/020] Iteration[380/501] Loss: 4.7414\n",
      "Training: Epoch[009/020] Iteration[390/501] Loss: 4.1545\n",
      "Training: Epoch[009/020] Iteration[400/501] Loss: 5.5411\n",
      "Training: Epoch[009/020] Iteration[410/501] Loss: 6.7297\n",
      "Training: Epoch[009/020] Iteration[420/501] Loss: 4.8517\n",
      "Training: Epoch[009/020] Iteration[430/501] Loss: 5.5304\n",
      "Training: Epoch[009/020] Iteration[440/501] Loss: 4.8536\n",
      "Training: Epoch[009/020] Iteration[450/501] Loss: 4.2831\n",
      "Training: Epoch[009/020] Iteration[460/501] Loss: 7.7861\n",
      "Training: Epoch[009/020] Iteration[470/501] Loss: 5.6266\n",
      "Training: Epoch[009/020] Iteration[480/501] Loss: 6.2651\n",
      "Training: Epoch[009/020] Iteration[490/501] Loss: 5.8929\n",
      "Training: Epoch[009/020] Iteration[500/501] Loss: 6.2882\n",
      "Valid: Epoch[009/020] Iteration[501/501] Loss: 7.6151\n",
      "Training: Epoch[010/020] Iteration[010/501] Loss: 5.0911\n",
      "Training: Epoch[010/020] Iteration[020/501] Loss: 5.1298\n",
      "Training: Epoch[010/020] Iteration[030/501] Loss: 4.5087\n",
      "Training: Epoch[010/020] Iteration[040/501] Loss: 4.8746\n",
      "Training: Epoch[010/020] Iteration[050/501] Loss: 5.3534\n",
      "Training: Epoch[010/020] Iteration[060/501] Loss: 6.0908\n",
      "Training: Epoch[010/020] Iteration[070/501] Loss: 4.5454\n",
      "Training: Epoch[010/020] Iteration[080/501] Loss: 6.2236\n",
      "Training: Epoch[010/020] Iteration[090/501] Loss: 6.7477\n",
      "Training: Epoch[010/020] Iteration[100/501] Loss: 4.0052\n",
      "Training: Epoch[010/020] Iteration[110/501] Loss: 3.6495\n",
      "Training: Epoch[010/020] Iteration[120/501] Loss: 4.4665\n",
      "Training: Epoch[010/020] Iteration[130/501] Loss: 4.9767\n",
      "Training: Epoch[010/020] Iteration[140/501] Loss: 4.6012\n",
      "Training: Epoch[010/020] Iteration[150/501] Loss: 4.3254\n",
      "Training: Epoch[010/020] Iteration[160/501] Loss: 4.8156\n",
      "Training: Epoch[010/020] Iteration[170/501] Loss: 4.0602\n",
      "Training: Epoch[010/020] Iteration[180/501] Loss: 4.0391\n",
      "Training: Epoch[010/020] Iteration[190/501] Loss: 5.7896\n",
      "Training: Epoch[010/020] Iteration[200/501] Loss: 5.0379\n",
      "Training: Epoch[010/020] Iteration[210/501] Loss: 3.6909\n",
      "Training: Epoch[010/020] Iteration[220/501] Loss: 3.5229\n",
      "Training: Epoch[010/020] Iteration[230/501] Loss: 5.1857\n",
      "Training: Epoch[010/020] Iteration[240/501] Loss: 4.0628\n",
      "Training: Epoch[010/020] Iteration[250/501] Loss: 4.9965\n",
      "Training: Epoch[010/020] Iteration[260/501] Loss: 5.5185\n",
      "Training: Epoch[010/020] Iteration[270/501] Loss: 4.0441\n",
      "Training: Epoch[010/020] Iteration[280/501] Loss: 5.0254\n",
      "Training: Epoch[010/020] Iteration[290/501] Loss: 4.3062\n",
      "Training: Epoch[010/020] Iteration[300/501] Loss: 4.6412\n",
      "Training: Epoch[010/020] Iteration[310/501] Loss: 4.1608\n",
      "Training: Epoch[010/020] Iteration[320/501] Loss: 5.0375\n",
      "Training: Epoch[010/020] Iteration[330/501] Loss: 6.3589\n",
      "Training: Epoch[010/020] Iteration[340/501] Loss: 5.0952\n",
      "Training: Epoch[010/020] Iteration[350/501] Loss: 4.7338\n",
      "Training: Epoch[010/020] Iteration[360/501] Loss: 3.8318\n",
      "Training: Epoch[010/020] Iteration[370/501] Loss: 4.1976\n",
      "Training: Epoch[010/020] Iteration[380/501] Loss: 4.3981\n",
      "Training: Epoch[010/020] Iteration[390/501] Loss: 4.9473\n",
      "Training: Epoch[010/020] Iteration[400/501] Loss: 4.9063\n",
      "Training: Epoch[010/020] Iteration[410/501] Loss: 4.4551\n",
      "Training: Epoch[010/020] Iteration[420/501] Loss: 4.7777\n",
      "Training: Epoch[010/020] Iteration[430/501] Loss: 3.8416\n",
      "Training: Epoch[010/020] Iteration[440/501] Loss: 4.4933\n",
      "Training: Epoch[010/020] Iteration[450/501] Loss: 4.8217\n",
      "Training: Epoch[010/020] Iteration[460/501] Loss: 5.0199\n",
      "Training: Epoch[010/020] Iteration[470/501] Loss: 5.0763\n",
      "Training: Epoch[010/020] Iteration[480/501] Loss: 5.3024\n",
      "Training: Epoch[010/020] Iteration[490/501] Loss: 5.1365\n",
      "Training: Epoch[010/020] Iteration[500/501] Loss: 5.7666\n",
      "Valid: Epoch[010/020] Iteration[501/501] Loss: 7.0624\n",
      "Training: Epoch[011/020] Iteration[010/501] Loss: 4.2311\n",
      "Training: Epoch[011/020] Iteration[020/501] Loss: 3.2929\n",
      "Training: Epoch[011/020] Iteration[030/501] Loss: 4.1296\n",
      "Training: Epoch[011/020] Iteration[040/501] Loss: 5.8973\n",
      "Training: Epoch[011/020] Iteration[050/501] Loss: 5.3150\n",
      "Training: Epoch[011/020] Iteration[060/501] Loss: 4.4304\n",
      "Training: Epoch[011/020] Iteration[070/501] Loss: 5.0945\n",
      "Training: Epoch[011/020] Iteration[080/501] Loss: 4.9768\n",
      "Training: Epoch[011/020] Iteration[090/501] Loss: 5.0726\n",
      "Training: Epoch[011/020] Iteration[100/501] Loss: 6.4219\n",
      "Training: Epoch[011/020] Iteration[110/501] Loss: 4.6718\n",
      "Training: Epoch[011/020] Iteration[120/501] Loss: 5.0768\n",
      "Training: Epoch[011/020] Iteration[130/501] Loss: 4.6920\n",
      "Training: Epoch[011/020] Iteration[140/501] Loss: 3.9835\n",
      "Training: Epoch[011/020] Iteration[150/501] Loss: 3.9006\n",
      "Training: Epoch[011/020] Iteration[160/501] Loss: 4.5214\n",
      "Training: Epoch[011/020] Iteration[170/501] Loss: 4.7561\n",
      "Training: Epoch[011/020] Iteration[180/501] Loss: 4.4468\n",
      "Training: Epoch[011/020] Iteration[190/501] Loss: 4.7712\n",
      "Training: Epoch[011/020] Iteration[200/501] Loss: 4.8350\n",
      "Training: Epoch[011/020] Iteration[210/501] Loss: 5.1841\n",
      "Training: Epoch[011/020] Iteration[220/501] Loss: 5.7978\n",
      "Training: Epoch[011/020] Iteration[230/501] Loss: 3.8880\n",
      "Training: Epoch[011/020] Iteration[240/501] Loss: 6.3281\n",
      "Training: Epoch[011/020] Iteration[250/501] Loss: 3.7853\n",
      "Training: Epoch[011/020] Iteration[260/501] Loss: 4.9507\n",
      "Training: Epoch[011/020] Iteration[270/501] Loss: 5.1350\n",
      "Training: Epoch[011/020] Iteration[280/501] Loss: 5.0487\n",
      "Training: Epoch[011/020] Iteration[290/501] Loss: 4.1026\n",
      "Training: Epoch[011/020] Iteration[300/501] Loss: 3.8604\n",
      "Training: Epoch[011/020] Iteration[310/501] Loss: 3.1240\n",
      "Training: Epoch[011/020] Iteration[320/501] Loss: 3.7762\n",
      "Training: Epoch[011/020] Iteration[330/501] Loss: 5.9869\n",
      "Training: Epoch[011/020] Iteration[340/501] Loss: 6.1069\n",
      "Training: Epoch[011/020] Iteration[350/501] Loss: 5.2341\n",
      "Training: Epoch[011/020] Iteration[360/501] Loss: 5.7283\n",
      "Training: Epoch[011/020] Iteration[370/501] Loss: 4.1064\n",
      "Training: Epoch[011/020] Iteration[380/501] Loss: 3.4213\n",
      "Training: Epoch[011/020] Iteration[390/501] Loss: 3.4580\n",
      "Training: Epoch[011/020] Iteration[400/501] Loss: 4.0978\n",
      "Training: Epoch[011/020] Iteration[410/501] Loss: 3.2442\n",
      "Training: Epoch[011/020] Iteration[420/501] Loss: 4.4040\n",
      "Training: Epoch[011/020] Iteration[430/501] Loss: 5.2202\n",
      "Training: Epoch[011/020] Iteration[440/501] Loss: 5.3502\n",
      "Training: Epoch[011/020] Iteration[450/501] Loss: 7.6260\n",
      "Training: Epoch[011/020] Iteration[460/501] Loss: 6.4344\n",
      "Training: Epoch[011/020] Iteration[470/501] Loss: 4.0340\n",
      "Training: Epoch[011/020] Iteration[480/501] Loss: 4.5678\n",
      "Training: Epoch[011/020] Iteration[490/501] Loss: 4.8981\n",
      "Training: Epoch[011/020] Iteration[500/501] Loss: 4.3053\n",
      "Valid: Epoch[011/020] Iteration[501/501] Loss: 6.7640\n",
      "Training: Epoch[012/020] Iteration[010/501] Loss: 4.8421\n",
      "Training: Epoch[012/020] Iteration[020/501] Loss: 3.7130\n",
      "Training: Epoch[012/020] Iteration[030/501] Loss: 3.7472\n",
      "Training: Epoch[012/020] Iteration[040/501] Loss: 4.7134\n",
      "Training: Epoch[012/020] Iteration[050/501] Loss: 4.0547\n",
      "Training: Epoch[012/020] Iteration[060/501] Loss: 4.3320\n",
      "Training: Epoch[012/020] Iteration[070/501] Loss: 4.2210\n",
      "Training: Epoch[012/020] Iteration[080/501] Loss: 4.5949\n",
      "Training: Epoch[012/020] Iteration[090/501] Loss: 5.0979\n",
      "Training: Epoch[012/020] Iteration[100/501] Loss: 5.3957\n",
      "Training: Epoch[012/020] Iteration[110/501] Loss: 4.5817\n",
      "Training: Epoch[012/020] Iteration[120/501] Loss: 4.6145\n",
      "Training: Epoch[012/020] Iteration[130/501] Loss: 5.6674\n",
      "Training: Epoch[012/020] Iteration[140/501] Loss: 3.7751\n",
      "Training: Epoch[012/020] Iteration[150/501] Loss: 3.1601\n",
      "Training: Epoch[012/020] Iteration[160/501] Loss: 3.9810\n",
      "Training: Epoch[012/020] Iteration[170/501] Loss: 3.3212\n",
      "Training: Epoch[012/020] Iteration[180/501] Loss: 4.3653\n",
      "Training: Epoch[012/020] Iteration[190/501] Loss: 4.6785\n",
      "Training: Epoch[012/020] Iteration[200/501] Loss: 5.4371\n",
      "Training: Epoch[012/020] Iteration[210/501] Loss: 3.9632\n",
      "Training: Epoch[012/020] Iteration[220/501] Loss: 4.4354\n",
      "Training: Epoch[012/020] Iteration[230/501] Loss: 3.4153\n",
      "Training: Epoch[012/020] Iteration[240/501] Loss: 3.9563\n",
      "Training: Epoch[012/020] Iteration[250/501] Loss: 3.9140\n",
      "Training: Epoch[012/020] Iteration[260/501] Loss: 5.3026\n",
      "Training: Epoch[012/020] Iteration[270/501] Loss: 5.6201\n",
      "Training: Epoch[012/020] Iteration[280/501] Loss: 5.0047\n",
      "Training: Epoch[012/020] Iteration[290/501] Loss: 4.8060\n",
      "Training: Epoch[012/020] Iteration[300/501] Loss: 5.5901\n",
      "Training: Epoch[012/020] Iteration[310/501] Loss: 4.0167\n",
      "Training: Epoch[012/020] Iteration[320/501] Loss: 3.7041\n",
      "Training: Epoch[012/020] Iteration[330/501] Loss: 5.4570\n",
      "Training: Epoch[012/020] Iteration[340/501] Loss: 5.7837\n",
      "Training: Epoch[012/020] Iteration[350/501] Loss: 4.2904\n",
      "Training: Epoch[012/020] Iteration[360/501] Loss: 4.1651\n",
      "Training: Epoch[012/020] Iteration[370/501] Loss: 5.8668\n",
      "Training: Epoch[012/020] Iteration[380/501] Loss: 4.5168\n",
      "Training: Epoch[012/020] Iteration[390/501] Loss: 4.9796\n",
      "Training: Epoch[012/020] Iteration[400/501] Loss: 5.0035\n",
      "Training: Epoch[012/020] Iteration[410/501] Loss: 4.6172\n",
      "Training: Epoch[012/020] Iteration[420/501] Loss: 5.4661\n",
      "Training: Epoch[012/020] Iteration[430/501] Loss: 4.7178\n",
      "Training: Epoch[012/020] Iteration[440/501] Loss: 4.3777\n",
      "Training: Epoch[012/020] Iteration[450/501] Loss: 5.1526\n",
      "Training: Epoch[012/020] Iteration[460/501] Loss: 4.0922\n",
      "Training: Epoch[012/020] Iteration[470/501] Loss: 7.1013\n",
      "Training: Epoch[012/020] Iteration[480/501] Loss: 5.0876\n",
      "Training: Epoch[012/020] Iteration[490/501] Loss: 3.3257\n",
      "Training: Epoch[012/020] Iteration[500/501] Loss: 6.4195\n",
      "Valid: Epoch[012/020] Iteration[501/501] Loss: 6.8082\n",
      "Training: Epoch[013/020] Iteration[010/501] Loss: 4.8181\n",
      "Training: Epoch[013/020] Iteration[020/501] Loss: 6.2887\n",
      "Training: Epoch[013/020] Iteration[030/501] Loss: 4.4567\n",
      "Training: Epoch[013/020] Iteration[040/501] Loss: 4.8545\n",
      "Training: Epoch[013/020] Iteration[050/501] Loss: 5.0855\n",
      "Training: Epoch[013/020] Iteration[060/501] Loss: 4.6645\n",
      "Training: Epoch[013/020] Iteration[070/501] Loss: 4.0766\n",
      "Training: Epoch[013/020] Iteration[080/501] Loss: 5.3548\n",
      "Training: Epoch[013/020] Iteration[090/501] Loss: 3.6425\n",
      "Training: Epoch[013/020] Iteration[100/501] Loss: 4.6170\n",
      "Training: Epoch[013/020] Iteration[110/501] Loss: 4.5635\n",
      "Training: Epoch[013/020] Iteration[120/501] Loss: 4.4555\n",
      "Training: Epoch[013/020] Iteration[130/501] Loss: 5.1431\n",
      "Training: Epoch[013/020] Iteration[140/501] Loss: 5.2038\n",
      "Training: Epoch[013/020] Iteration[150/501] Loss: 5.8133\n",
      "Training: Epoch[013/020] Iteration[160/501] Loss: 5.1072\n",
      "Training: Epoch[013/020] Iteration[170/501] Loss: 3.8400\n",
      "Training: Epoch[013/020] Iteration[180/501] Loss: 4.8390\n",
      "Training: Epoch[013/020] Iteration[190/501] Loss: 4.6921\n",
      "Training: Epoch[013/020] Iteration[200/501] Loss: 4.1302\n",
      "Training: Epoch[013/020] Iteration[210/501] Loss: 4.4134\n",
      "Training: Epoch[013/020] Iteration[220/501] Loss: 4.8795\n",
      "Training: Epoch[013/020] Iteration[230/501] Loss: 3.2831\n",
      "Training: Epoch[013/020] Iteration[240/501] Loss: 4.5701\n",
      "Training: Epoch[013/020] Iteration[250/501] Loss: 4.7491\n",
      "Training: Epoch[013/020] Iteration[260/501] Loss: 4.6445\n",
      "Training: Epoch[013/020] Iteration[270/501] Loss: 4.4312\n",
      "Training: Epoch[013/020] Iteration[280/501] Loss: 5.8243\n",
      "Training: Epoch[013/020] Iteration[290/501] Loss: 3.4274\n",
      "Training: Epoch[013/020] Iteration[300/501] Loss: 4.9526\n",
      "Training: Epoch[013/020] Iteration[310/501] Loss: 5.2119\n",
      "Training: Epoch[013/020] Iteration[320/501] Loss: 4.3231\n",
      "Training: Epoch[013/020] Iteration[330/501] Loss: 4.5329\n",
      "Training: Epoch[013/020] Iteration[340/501] Loss: 4.4617\n",
      "Training: Epoch[013/020] Iteration[350/501] Loss: 4.4862\n",
      "Training: Epoch[013/020] Iteration[360/501] Loss: 3.0910\n",
      "Training: Epoch[013/020] Iteration[370/501] Loss: 3.4584\n",
      "Training: Epoch[013/020] Iteration[380/501] Loss: 3.3923\n",
      "Training: Epoch[013/020] Iteration[390/501] Loss: 6.2698\n",
      "Training: Epoch[013/020] Iteration[400/501] Loss: 4.7919\n",
      "Training: Epoch[013/020] Iteration[410/501] Loss: 3.6716\n",
      "Training: Epoch[013/020] Iteration[420/501] Loss: 5.1626\n",
      "Training: Epoch[013/020] Iteration[430/501] Loss: 3.9471\n",
      "Training: Epoch[013/020] Iteration[440/501] Loss: 4.4322\n",
      "Training: Epoch[013/020] Iteration[450/501] Loss: 3.5827\n",
      "Training: Epoch[013/020] Iteration[460/501] Loss: 5.4027\n",
      "Training: Epoch[013/020] Iteration[470/501] Loss: 3.9882\n",
      "Training: Epoch[013/020] Iteration[480/501] Loss: 6.0628\n",
      "Training: Epoch[013/020] Iteration[490/501] Loss: 5.3256\n",
      "Training: Epoch[013/020] Iteration[500/501] Loss: 4.0716\n",
      "Valid: Epoch[013/020] Iteration[501/501] Loss: 6.6894\n",
      "Training: Epoch[014/020] Iteration[010/501] Loss: 3.2942\n",
      "Training: Epoch[014/020] Iteration[020/501] Loss: 4.1463\n",
      "Training: Epoch[014/020] Iteration[030/501] Loss: 3.9247\n",
      "Training: Epoch[014/020] Iteration[040/501] Loss: 4.0177\n",
      "Training: Epoch[014/020] Iteration[050/501] Loss: 5.7332\n",
      "Training: Epoch[014/020] Iteration[060/501] Loss: 4.9522\n",
      "Training: Epoch[014/020] Iteration[070/501] Loss: 3.7596\n",
      "Training: Epoch[014/020] Iteration[080/501] Loss: 5.0542\n",
      "Training: Epoch[014/020] Iteration[090/501] Loss: 4.2644\n",
      "Training: Epoch[014/020] Iteration[100/501] Loss: 4.6948\n",
      "Training: Epoch[014/020] Iteration[110/501] Loss: 4.2093\n",
      "Training: Epoch[014/020] Iteration[120/501] Loss: 4.4681\n",
      "Training: Epoch[014/020] Iteration[130/501] Loss: 6.5245\n",
      "Training: Epoch[014/020] Iteration[140/501] Loss: 4.2163\n",
      "Training: Epoch[014/020] Iteration[150/501] Loss: 4.6849\n",
      "Training: Epoch[014/020] Iteration[160/501] Loss: 3.9408\n",
      "Training: Epoch[014/020] Iteration[170/501] Loss: 4.4790\n",
      "Training: Epoch[014/020] Iteration[180/501] Loss: 4.3173\n",
      "Training: Epoch[014/020] Iteration[190/501] Loss: 5.2522\n",
      "Training: Epoch[014/020] Iteration[200/501] Loss: 4.3081\n",
      "Training: Epoch[014/020] Iteration[210/501] Loss: 5.5206\n",
      "Training: Epoch[014/020] Iteration[220/501] Loss: 4.7455\n",
      "Training: Epoch[014/020] Iteration[230/501] Loss: 4.7612\n",
      "Training: Epoch[014/020] Iteration[240/501] Loss: 4.0573\n",
      "Training: Epoch[014/020] Iteration[250/501] Loss: 5.2448\n",
      "Training: Epoch[014/020] Iteration[260/501] Loss: 6.2058\n",
      "Training: Epoch[014/020] Iteration[270/501] Loss: 5.7070\n",
      "Training: Epoch[014/020] Iteration[280/501] Loss: 4.7107\n",
      "Training: Epoch[014/020] Iteration[290/501] Loss: 3.5887\n",
      "Training: Epoch[014/020] Iteration[300/501] Loss: 5.5060\n",
      "Training: Epoch[014/020] Iteration[310/501] Loss: 4.4914\n",
      "Training: Epoch[014/020] Iteration[320/501] Loss: 3.9478\n",
      "Training: Epoch[014/020] Iteration[330/501] Loss: 4.9051\n",
      "Training: Epoch[014/020] Iteration[340/501] Loss: 4.0464\n",
      "Training: Epoch[014/020] Iteration[350/501] Loss: 3.1605\n",
      "Training: Epoch[014/020] Iteration[360/501] Loss: 3.9144\n",
      "Training: Epoch[014/020] Iteration[370/501] Loss: 4.1915\n",
      "Training: Epoch[014/020] Iteration[380/501] Loss: 5.1495\n",
      "Training: Epoch[014/020] Iteration[390/501] Loss: 3.6801\n",
      "Training: Epoch[014/020] Iteration[400/501] Loss: 4.1598\n",
      "Training: Epoch[014/020] Iteration[410/501] Loss: 3.9515\n",
      "Training: Epoch[014/020] Iteration[420/501] Loss: 4.2129\n",
      "Training: Epoch[014/020] Iteration[430/501] Loss: 4.1700\n",
      "Training: Epoch[014/020] Iteration[440/501] Loss: 4.7959\n",
      "Training: Epoch[014/020] Iteration[450/501] Loss: 5.8973\n",
      "Training: Epoch[014/020] Iteration[460/501] Loss: 4.3361\n",
      "Training: Epoch[014/020] Iteration[470/501] Loss: 4.1618\n",
      "Training: Epoch[014/020] Iteration[480/501] Loss: 5.1603\n",
      "Training: Epoch[014/020] Iteration[490/501] Loss: 4.4026\n",
      "Training: Epoch[014/020] Iteration[500/501] Loss: 4.0596\n",
      "Valid: Epoch[014/020] Iteration[501/501] Loss: 6.8180\n",
      "Training: Epoch[015/020] Iteration[010/501] Loss: 3.7934\n",
      "Training: Epoch[015/020] Iteration[020/501] Loss: 4.4836\n",
      "Training: Epoch[015/020] Iteration[030/501] Loss: 3.2337\n",
      "Training: Epoch[015/020] Iteration[040/501] Loss: 3.9169\n",
      "Training: Epoch[015/020] Iteration[050/501] Loss: 3.9571\n",
      "Training: Epoch[015/020] Iteration[060/501] Loss: 2.6056\n",
      "Training: Epoch[015/020] Iteration[070/501] Loss: 4.5396\n",
      "Training: Epoch[015/020] Iteration[080/501] Loss: 4.8736\n",
      "Training: Epoch[015/020] Iteration[090/501] Loss: 4.0960\n",
      "Training: Epoch[015/020] Iteration[100/501] Loss: 3.4799\n",
      "Training: Epoch[015/020] Iteration[110/501] Loss: 3.6178\n",
      "Training: Epoch[015/020] Iteration[120/501] Loss: 4.0093\n",
      "Training: Epoch[015/020] Iteration[130/501] Loss: 4.1043\n",
      "Training: Epoch[015/020] Iteration[140/501] Loss: 5.0110\n",
      "Training: Epoch[015/020] Iteration[150/501] Loss: 5.3765\n",
      "Training: Epoch[015/020] Iteration[160/501] Loss: 5.5909\n",
      "Training: Epoch[015/020] Iteration[170/501] Loss: 5.4357\n",
      "Training: Epoch[015/020] Iteration[180/501] Loss: 3.6523\n",
      "Training: Epoch[015/020] Iteration[190/501] Loss: 5.5479\n",
      "Training: Epoch[015/020] Iteration[200/501] Loss: 4.5428\n",
      "Training: Epoch[015/020] Iteration[210/501] Loss: 2.9685\n",
      "Training: Epoch[015/020] Iteration[220/501] Loss: 3.9031\n",
      "Training: Epoch[015/020] Iteration[230/501] Loss: 3.7277\n",
      "Training: Epoch[015/020] Iteration[240/501] Loss: 3.3414\n",
      "Training: Epoch[015/020] Iteration[250/501] Loss: 5.2171\n",
      "Training: Epoch[015/020] Iteration[260/501] Loss: 3.4609\n",
      "Training: Epoch[015/020] Iteration[270/501] Loss: 4.4349\n",
      "Training: Epoch[015/020] Iteration[280/501] Loss: 3.7095\n",
      "Training: Epoch[015/020] Iteration[290/501] Loss: 4.0497\n",
      "Training: Epoch[015/020] Iteration[300/501] Loss: 6.0548\n",
      "Training: Epoch[015/020] Iteration[310/501] Loss: 4.6117\n",
      "Training: Epoch[015/020] Iteration[320/501] Loss: 2.9387\n",
      "Training: Epoch[015/020] Iteration[330/501] Loss: 4.6970\n",
      "Training: Epoch[015/020] Iteration[340/501] Loss: 4.1647\n",
      "Training: Epoch[015/020] Iteration[350/501] Loss: 4.9244\n",
      "Training: Epoch[015/020] Iteration[360/501] Loss: 6.6532\n",
      "Training: Epoch[015/020] Iteration[370/501] Loss: 4.7480\n",
      "Training: Epoch[015/020] Iteration[380/501] Loss: 4.3764\n",
      "Training: Epoch[015/020] Iteration[390/501] Loss: 5.5858\n",
      "Training: Epoch[015/020] Iteration[400/501] Loss: 4.1959\n",
      "Training: Epoch[015/020] Iteration[410/501] Loss: 4.8356\n",
      "Training: Epoch[015/020] Iteration[420/501] Loss: 5.0822\n",
      "Training: Epoch[015/020] Iteration[430/501] Loss: 5.0434\n",
      "Training: Epoch[015/020] Iteration[440/501] Loss: 4.3325\n",
      "Training: Epoch[015/020] Iteration[450/501] Loss: 6.7530\n",
      "Training: Epoch[015/020] Iteration[460/501] Loss: 5.1507\n",
      "Training: Epoch[015/020] Iteration[470/501] Loss: 4.1427\n",
      "Training: Epoch[015/020] Iteration[480/501] Loss: 5.2357\n",
      "Training: Epoch[015/020] Iteration[490/501] Loss: 4.1671\n",
      "Training: Epoch[015/020] Iteration[500/501] Loss: 4.7004\n",
      "Valid: Epoch[015/020] Iteration[501/501] Loss: 6.6569\n",
      "Training: Epoch[016/020] Iteration[010/501] Loss: 4.6045\n",
      "Training: Epoch[016/020] Iteration[020/501] Loss: 4.1390\n",
      "Training: Epoch[016/020] Iteration[030/501] Loss: 3.8020\n",
      "Training: Epoch[016/020] Iteration[040/501] Loss: 6.2895\n",
      "Training: Epoch[016/020] Iteration[050/501] Loss: 5.7020\n",
      "Training: Epoch[016/020] Iteration[060/501] Loss: 5.5200\n",
      "Training: Epoch[016/020] Iteration[070/501] Loss: 4.8904\n",
      "Training: Epoch[016/020] Iteration[080/501] Loss: 4.2433\n",
      "Training: Epoch[016/020] Iteration[090/501] Loss: 4.9908\n",
      "Training: Epoch[016/020] Iteration[100/501] Loss: 4.4097\n",
      "Training: Epoch[016/020] Iteration[110/501] Loss: 5.4191\n",
      "Training: Epoch[016/020] Iteration[120/501] Loss: 3.6608\n",
      "Training: Epoch[016/020] Iteration[130/501] Loss: 5.6372\n",
      "Training: Epoch[016/020] Iteration[140/501] Loss: 3.2080\n",
      "Training: Epoch[016/020] Iteration[150/501] Loss: 3.1417\n",
      "Training: Epoch[016/020] Iteration[160/501] Loss: 3.4137\n",
      "Training: Epoch[016/020] Iteration[170/501] Loss: 3.7014\n",
      "Training: Epoch[016/020] Iteration[180/501] Loss: 4.1051\n",
      "Training: Epoch[016/020] Iteration[190/501] Loss: 3.8262\n",
      "Training: Epoch[016/020] Iteration[200/501] Loss: 4.3186\n",
      "Training: Epoch[016/020] Iteration[210/501] Loss: 5.3158\n",
      "Training: Epoch[016/020] Iteration[220/501] Loss: 3.1149\n",
      "Training: Epoch[016/020] Iteration[230/501] Loss: 4.0808\n",
      "Training: Epoch[016/020] Iteration[240/501] Loss: 5.1918\n",
      "Training: Epoch[016/020] Iteration[250/501] Loss: 3.6179\n",
      "Training: Epoch[016/020] Iteration[260/501] Loss: 4.2031\n",
      "Training: Epoch[016/020] Iteration[270/501] Loss: 4.0656\n",
      "Training: Epoch[016/020] Iteration[280/501] Loss: 4.0922\n",
      "Training: Epoch[016/020] Iteration[290/501] Loss: 4.5838\n",
      "Training: Epoch[016/020] Iteration[300/501] Loss: 5.0596\n",
      "Training: Epoch[016/020] Iteration[310/501] Loss: 3.4319\n",
      "Training: Epoch[016/020] Iteration[320/501] Loss: 6.4971\n",
      "Training: Epoch[016/020] Iteration[330/501] Loss: 4.6200\n",
      "Training: Epoch[016/020] Iteration[340/501] Loss: 3.7276\n",
      "Training: Epoch[016/020] Iteration[350/501] Loss: 5.7290\n",
      "Training: Epoch[016/020] Iteration[360/501] Loss: 4.9185\n",
      "Training: Epoch[016/020] Iteration[370/501] Loss: 5.1361\n",
      "Training: Epoch[016/020] Iteration[380/501] Loss: 2.8899\n",
      "Training: Epoch[016/020] Iteration[390/501] Loss: 3.5599\n",
      "Training: Epoch[016/020] Iteration[400/501] Loss: 5.5730\n",
      "Training: Epoch[016/020] Iteration[410/501] Loss: 4.6324\n",
      "Training: Epoch[016/020] Iteration[420/501] Loss: 4.9226\n",
      "Training: Epoch[016/020] Iteration[430/501] Loss: 5.5084\n",
      "Training: Epoch[016/020] Iteration[440/501] Loss: 4.2658\n",
      "Training: Epoch[016/020] Iteration[450/501] Loss: 4.1260\n",
      "Training: Epoch[016/020] Iteration[460/501] Loss: 4.6318\n",
      "Training: Epoch[016/020] Iteration[470/501] Loss: 3.4909\n",
      "Training: Epoch[016/020] Iteration[480/501] Loss: 4.9033\n",
      "Training: Epoch[016/020] Iteration[490/501] Loss: 4.3190\n",
      "Training: Epoch[016/020] Iteration[500/501] Loss: 4.5177\n",
      "Valid: Epoch[016/020] Iteration[501/501] Loss: 7.2558\n",
      "Training: Epoch[017/020] Iteration[010/501] Loss: 4.3538\n",
      "Training: Epoch[017/020] Iteration[020/501] Loss: 4.6549\n",
      "Training: Epoch[017/020] Iteration[030/501] Loss: 4.5584\n",
      "Training: Epoch[017/020] Iteration[040/501] Loss: 4.5894\n",
      "Training: Epoch[017/020] Iteration[050/501] Loss: 5.4302\n",
      "Training: Epoch[017/020] Iteration[060/501] Loss: 3.7528\n",
      "Training: Epoch[017/020] Iteration[070/501] Loss: 5.1285\n",
      "Training: Epoch[017/020] Iteration[080/501] Loss: 3.6796\n",
      "Training: Epoch[017/020] Iteration[090/501] Loss: 5.8401\n",
      "Training: Epoch[017/020] Iteration[100/501] Loss: 5.7376\n",
      "Training: Epoch[017/020] Iteration[110/501] Loss: 3.3304\n",
      "Training: Epoch[017/020] Iteration[120/501] Loss: 4.8276\n",
      "Training: Epoch[017/020] Iteration[130/501] Loss: 3.1891\n",
      "Training: Epoch[017/020] Iteration[140/501] Loss: 3.1507\n",
      "Training: Epoch[017/020] Iteration[150/501] Loss: 3.4974\n",
      "Training: Epoch[017/020] Iteration[160/501] Loss: 4.3590\n",
      "Training: Epoch[017/020] Iteration[170/501] Loss: 5.5645\n",
      "Training: Epoch[017/020] Iteration[180/501] Loss: 4.6565\n",
      "Training: Epoch[017/020] Iteration[190/501] Loss: 4.1583\n",
      "Training: Epoch[017/020] Iteration[200/501] Loss: 4.5395\n",
      "Training: Epoch[017/020] Iteration[210/501] Loss: 4.1181\n",
      "Training: Epoch[017/020] Iteration[220/501] Loss: 5.4374\n",
      "Training: Epoch[017/020] Iteration[230/501] Loss: 3.7101\n",
      "Training: Epoch[017/020] Iteration[240/501] Loss: 3.8599\n",
      "Training: Epoch[017/020] Iteration[250/501] Loss: 4.1112\n",
      "Training: Epoch[017/020] Iteration[260/501] Loss: 3.6771\n",
      "Training: Epoch[017/020] Iteration[270/501] Loss: 3.9067\n",
      "Training: Epoch[017/020] Iteration[280/501] Loss: 4.2773\n",
      "Training: Epoch[017/020] Iteration[290/501] Loss: 5.6405\n",
      "Training: Epoch[017/020] Iteration[300/501] Loss: 3.8776\n",
      "Training: Epoch[017/020] Iteration[310/501] Loss: 2.9256\n",
      "Training: Epoch[017/020] Iteration[320/501] Loss: 3.7738\n",
      "Training: Epoch[017/020] Iteration[330/501] Loss: 4.7277\n",
      "Training: Epoch[017/020] Iteration[340/501] Loss: 4.2880\n",
      "Training: Epoch[017/020] Iteration[350/501] Loss: 4.8372\n",
      "Training: Epoch[017/020] Iteration[360/501] Loss: 3.5241\n",
      "Training: Epoch[017/020] Iteration[370/501] Loss: 5.0105\n",
      "Training: Epoch[017/020] Iteration[380/501] Loss: 4.2965\n",
      "Training: Epoch[017/020] Iteration[390/501] Loss: 4.7166\n",
      "Training: Epoch[017/020] Iteration[400/501] Loss: 4.0337\n",
      "Training: Epoch[017/020] Iteration[410/501] Loss: 4.9828\n",
      "Training: Epoch[017/020] Iteration[420/501] Loss: 5.9009\n",
      "Training: Epoch[017/020] Iteration[430/501] Loss: 5.6221\n",
      "Training: Epoch[017/020] Iteration[440/501] Loss: 3.7972\n",
      "Training: Epoch[017/020] Iteration[450/501] Loss: 4.6109\n",
      "Training: Epoch[017/020] Iteration[460/501] Loss: 4.8285\n",
      "Training: Epoch[017/020] Iteration[470/501] Loss: 5.2958\n",
      "Training: Epoch[017/020] Iteration[480/501] Loss: 5.1252\n",
      "Training: Epoch[017/020] Iteration[490/501] Loss: 3.8864\n",
      "Training: Epoch[017/020] Iteration[500/501] Loss: 4.6037\n",
      "Valid: Epoch[017/020] Iteration[501/501] Loss: 6.7621\n",
      "Training: Epoch[018/020] Iteration[010/501] Loss: 4.0974\n",
      "Training: Epoch[018/020] Iteration[020/501] Loss: 4.2879\n",
      "Training: Epoch[018/020] Iteration[030/501] Loss: 4.3889\n",
      "Training: Epoch[018/020] Iteration[040/501] Loss: 3.8349\n",
      "Training: Epoch[018/020] Iteration[050/501] Loss: 5.0234\n",
      "Training: Epoch[018/020] Iteration[060/501] Loss: 4.1925\n",
      "Training: Epoch[018/020] Iteration[070/501] Loss: 4.2510\n",
      "Training: Epoch[018/020] Iteration[080/501] Loss: 4.9032\n",
      "Training: Epoch[018/020] Iteration[090/501] Loss: 5.4729\n",
      "Training: Epoch[018/020] Iteration[100/501] Loss: 3.9331\n",
      "Training: Epoch[018/020] Iteration[110/501] Loss: 4.3371\n",
      "Training: Epoch[018/020] Iteration[120/501] Loss: 4.5729\n",
      "Training: Epoch[018/020] Iteration[130/501] Loss: 5.2359\n",
      "Training: Epoch[018/020] Iteration[140/501] Loss: 3.9206\n",
      "Training: Epoch[018/020] Iteration[150/501] Loss: 4.6733\n",
      "Training: Epoch[018/020] Iteration[160/501] Loss: 4.0838\n",
      "Training: Epoch[018/020] Iteration[170/501] Loss: 4.2523\n",
      "Training: Epoch[018/020] Iteration[180/501] Loss: 6.0486\n",
      "Training: Epoch[018/020] Iteration[190/501] Loss: 4.5471\n",
      "Training: Epoch[018/020] Iteration[200/501] Loss: 3.9173\n",
      "Training: Epoch[018/020] Iteration[210/501] Loss: 5.7326\n",
      "Training: Epoch[018/020] Iteration[220/501] Loss: 3.6374\n",
      "Training: Epoch[018/020] Iteration[230/501] Loss: 4.8533\n",
      "Training: Epoch[018/020] Iteration[240/501] Loss: 4.2934\n",
      "Training: Epoch[018/020] Iteration[250/501] Loss: 5.0577\n",
      "Training: Epoch[018/020] Iteration[260/501] Loss: 4.4060\n",
      "Training: Epoch[018/020] Iteration[270/501] Loss: 3.2958\n",
      "Training: Epoch[018/020] Iteration[280/501] Loss: 3.9759\n",
      "Training: Epoch[018/020] Iteration[290/501] Loss: 4.4368\n",
      "Training: Epoch[018/020] Iteration[300/501] Loss: 4.7661\n",
      "Training: Epoch[018/020] Iteration[310/501] Loss: 4.6363\n",
      "Training: Epoch[018/020] Iteration[320/501] Loss: 3.8673\n",
      "Training: Epoch[018/020] Iteration[330/501] Loss: 3.9238\n",
      "Training: Epoch[018/020] Iteration[340/501] Loss: 4.4107\n",
      "Training: Epoch[018/020] Iteration[350/501] Loss: 4.1150\n",
      "Training: Epoch[018/020] Iteration[360/501] Loss: 4.1247\n",
      "Training: Epoch[018/020] Iteration[370/501] Loss: 4.0068\n",
      "Training: Epoch[018/020] Iteration[380/501] Loss: 4.1283\n",
      "Training: Epoch[018/020] Iteration[390/501] Loss: 3.4851\n",
      "Training: Epoch[018/020] Iteration[400/501] Loss: 3.0057\n",
      "Training: Epoch[018/020] Iteration[410/501] Loss: 4.1248\n",
      "Training: Epoch[018/020] Iteration[420/501] Loss: 3.4593\n",
      "Training: Epoch[018/020] Iteration[430/501] Loss: 4.8984\n",
      "Training: Epoch[018/020] Iteration[440/501] Loss: 4.2592\n",
      "Training: Epoch[018/020] Iteration[450/501] Loss: 3.8614\n",
      "Training: Epoch[018/020] Iteration[460/501] Loss: 5.0796\n",
      "Training: Epoch[018/020] Iteration[470/501] Loss: 6.8555\n",
      "Training: Epoch[018/020] Iteration[480/501] Loss: 6.0361\n",
      "Training: Epoch[018/020] Iteration[490/501] Loss: 3.2620\n",
      "Training: Epoch[018/020] Iteration[500/501] Loss: 4.2007\n",
      "Valid: Epoch[018/020] Iteration[501/501] Loss: 6.9676\n",
      "Training: Epoch[019/020] Iteration[010/501] Loss: 4.2407\n",
      "Training: Epoch[019/020] Iteration[020/501] Loss: 3.7200\n",
      "Training: Epoch[019/020] Iteration[030/501] Loss: 4.3662\n",
      "Training: Epoch[019/020] Iteration[040/501] Loss: 4.3777\n",
      "Training: Epoch[019/020] Iteration[050/501] Loss: 4.9294\n",
      "Training: Epoch[019/020] Iteration[060/501] Loss: 3.6013\n",
      "Training: Epoch[019/020] Iteration[070/501] Loss: 5.0810\n",
      "Training: Epoch[019/020] Iteration[080/501] Loss: 3.4598\n",
      "Training: Epoch[019/020] Iteration[090/501] Loss: 6.4533\n",
      "Training: Epoch[019/020] Iteration[100/501] Loss: 5.3896\n",
      "Training: Epoch[019/020] Iteration[110/501] Loss: 4.1757\n",
      "Training: Epoch[019/020] Iteration[120/501] Loss: 4.7140\n",
      "Training: Epoch[019/020] Iteration[130/501] Loss: 6.2055\n",
      "Training: Epoch[019/020] Iteration[140/501] Loss: 3.1579\n",
      "Training: Epoch[019/020] Iteration[150/501] Loss: 3.9453\n",
      "Training: Epoch[019/020] Iteration[160/501] Loss: 4.8957\n",
      "Training: Epoch[019/020] Iteration[170/501] Loss: 5.2687\n",
      "Training: Epoch[019/020] Iteration[180/501] Loss: 5.3300\n",
      "Training: Epoch[019/020] Iteration[190/501] Loss: 3.7029\n",
      "Training: Epoch[019/020] Iteration[200/501] Loss: 4.1208\n",
      "Training: Epoch[019/020] Iteration[210/501] Loss: 5.8218\n",
      "Training: Epoch[019/020] Iteration[220/501] Loss: 4.8309\n",
      "Training: Epoch[019/020] Iteration[230/501] Loss: 4.8876\n",
      "Training: Epoch[019/020] Iteration[240/501] Loss: 4.5188\n",
      "Training: Epoch[019/020] Iteration[250/501] Loss: 4.1579\n",
      "Training: Epoch[019/020] Iteration[260/501] Loss: 5.4795\n",
      "Training: Epoch[019/020] Iteration[270/501] Loss: 4.6689\n",
      "Training: Epoch[019/020] Iteration[280/501] Loss: 4.3768\n",
      "Training: Epoch[019/020] Iteration[290/501] Loss: 4.9284\n",
      "Training: Epoch[019/020] Iteration[300/501] Loss: 5.1154\n",
      "Training: Epoch[019/020] Iteration[310/501] Loss: 3.4739\n",
      "Training: Epoch[019/020] Iteration[320/501] Loss: 4.8784\n",
      "Training: Epoch[019/020] Iteration[330/501] Loss: 4.6231\n",
      "Training: Epoch[019/020] Iteration[340/501] Loss: 3.3090\n",
      "Training: Epoch[019/020] Iteration[350/501] Loss: 4.1529\n",
      "Training: Epoch[019/020] Iteration[360/501] Loss: 4.9430\n",
      "Training: Epoch[019/020] Iteration[370/501] Loss: 3.3792\n",
      "Training: Epoch[019/020] Iteration[380/501] Loss: 3.5290\n",
      "Training: Epoch[019/020] Iteration[390/501] Loss: 3.4126\n",
      "Training: Epoch[019/020] Iteration[400/501] Loss: 5.1454\n",
      "Training: Epoch[019/020] Iteration[410/501] Loss: 4.5786\n",
      "Training: Epoch[019/020] Iteration[420/501] Loss: 4.3345\n",
      "Training: Epoch[019/020] Iteration[430/501] Loss: 4.7540\n",
      "Training: Epoch[019/020] Iteration[440/501] Loss: 3.3195\n",
      "Training: Epoch[019/020] Iteration[450/501] Loss: 4.3045\n",
      "Training: Epoch[019/020] Iteration[460/501] Loss: 4.4278\n",
      "Training: Epoch[019/020] Iteration[470/501] Loss: 3.0547\n",
      "Training: Epoch[019/020] Iteration[480/501] Loss: 3.5796\n",
      "Training: Epoch[019/020] Iteration[490/501] Loss: 4.4259\n",
      "Training: Epoch[019/020] Iteration[500/501] Loss: 5.2577\n",
      "Valid: Epoch[019/020] Iteration[501/501] Loss: 8.5639\n"
     ]
    }
   ],
   "source": [
    "train_curve = list() \n",
    "valid_curve = list()\n",
    "\n",
    "for epoch in range(start_epoch + 1, MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    res_model.train() # 将模型切换到训练模式，启用 dropout 等操作。\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # 遍历训练数据加载器，data 包含一个批次的 inputs（输入图像）和 labels（对应的标签）。\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = res_model(inputs)\n",
    "\n",
    "        # 调整标签的形状\n",
    "        labels = labels.view(-1, 1)  # 确保标签的形状与输出一致\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        # 清空上一批次的梯度，防止累积。\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # 通过自动求导计算梯度。\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # 使用优化器根据计算出的梯度更新模型参数。\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()        # 获取当前批次的损失值。\n",
    "        train_curve.append(loss.item()) # 将当前批次的损失值记录到 train_curve。\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_dataloader), loss_mean))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率 调用学习率调度器，根据设置调整当前学习率（如按一定步长下降）。\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        res_model.eval() \n",
    "        \n",
    "        # 禁用自动求导，减少内存占用，加速计算。\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(test_dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = res_model(inputs)\n",
    "               \n",
    "                # 调整标签的形状\n",
    "                labels = labels.view(-1, 1)  # 确保标签的形状与输出一致\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            loss_val_mean = loss_val / len(test_dataloader)\n",
    "            valid_curve.append(loss_val_mean)\n",
    "            print(\"Valid: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(test_dataloader), loss_val_mean))\n",
    "\n",
    "        res_model.train()  # 将模型切换回训练模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd444bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_dataloader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations\n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c33ee0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
