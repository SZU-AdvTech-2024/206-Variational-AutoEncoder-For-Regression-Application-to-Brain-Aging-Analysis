{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0a2caf6-81ca-4b0f-917a-29c7a4af5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.models as models\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "023fea34-20de-43dc-8cb7-969a88e9c113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted Classes: ['-250', '-249', '-248', '-247', '-246', '-245', '-244', '-243', '-242', '-241', '-240', '-239', '-238', '-237', '-236', '-235', '-234', '-233', '-232', '-231', '-230', '-229', '-228', '-227', '-226', '-225', '-224', '-223', '-222', '-221', '-220', '-219', '-218', '-217', '-216', '-215', '-214', '-213', '-212', '-211', '-210', '-209', '-208', '-207', '-206', '-205', '-204', '-203', '-202', '-201', '-200', '-199', '-198', '-197', '-196', '-195', '-194', '-193', '-192', '-191', '-190', '-189', '-188', '-187', '-186', '-185', '-184', '-183', '-182', '-181', '-180', '-179', '-178', '-177', '-176', '-175', '-174', '-173', '-172', '-171', '-170', '-169', '-168', '-167', '-166', '-165', '-164', '-163', '-162', '-161', '-160', '-159', '-158', '-157', '-156', '-155', '-154', '-153', '-152', '-151', '-150', '-149', '-148', '-147', '-146', '-145', '-144', '-143', '-142', '-141', '-140', '-139', '-138', '-137', '-136', '-135', '-134', '-133', '-132', '-131', '-130', '-129', '-128', '-127', '-126', '-125', '-124', '-123', '-122', '-121', '-120', '-119', '-118', '-117', '-116', '-115', '-114', '-113', '-112', '-111', '-110', '-109', '-108', '-107', '-106', '-105', '-104', '-103', '-102', '-101', '-100', '-99', '-98', '-97', '-96', '-95', '-94', '-93', '-92', '-91', '-90', '-89', '-88', '-87', '-86', '-85', '-84', '-83', '-82', '-81', '-80', '-79', '-78', '-77', '-76', '-75', '-74', '-73', '-72', '-71', '-70', '-69', '-68', '-67', '-66', '-65', '-64', '-63', '-62', '-61', '-60', '-59', '-58', '-57', '-56', '-55', '-54', '-53', '-52', '-51', '-50', '-49', '-48', '-47', '-46', '-45', '-44', '-43', '-42', '-41', '-40', '-39', '-38', '-37', '-36', '-35', '-34', '-33', '-32', '-31', '-30', '-29', '-28', '-27', '-26', '-25', '-24', '-23', '-22', '-21', '-20', '-19', '-18', '-17', '-16', '-15', '-14', '-13', '-12', '-11', '-10', '-9', '-8', '-7', '-6', '-5', '-4', '-3', '-2', '-1', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250']\n",
      "训练集大小: 20040\n",
      "测试集大小: 5010\n"
     ]
    }
   ],
   "source": [
    "# 设置数据集路径\n",
    "total_dir = \"/Users/fcccasa/Downloads/R/img\"\n",
    "\n",
    "# 定义数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# 加载数据集\n",
    "total_data = datasets.ImageFolder(total_dir, transform)\n",
    "\n",
    "# 获取类别并按数字顺序排序\n",
    "def sort_key(class_name):\n",
    "    # 尝试将类别名称转换为整数，若失败则返回原字符串\n",
    "    try:\n",
    "        return int(class_name)  # 如果类别是数字，则按数字排序\n",
    "    except ValueError:\n",
    "        return class_name  # 如果类别是非数字，则按字母排序\n",
    "\n",
    "sorted_classes = sorted(total_data.classes, key=sort_key)\n",
    "\n",
    "# 更新 ImageFolder 的 class_to_idx 字典\n",
    "total_data.class_to_idx = {cls: idx for idx, cls in enumerate(sorted_classes)}\n",
    "\n",
    "print(\"Sorted Classes:\", sorted_classes)\n",
    "\n",
    "# 使用排序后的类名创建训练集和测试集索引\n",
    "train_indices = []\n",
    "test_indices = []\n",
    "\n",
    "# 遍历数据集中的每个类别（文件夹）\n",
    "for class_idx in range(len(sorted_classes)):\n",
    "    # 获取当前类别所有图像的索引\n",
    "    class_indices = [i for i, label in enumerate(total_data.targets) if label == class_idx]\n",
    "    \n",
    "    # 分割：前40个图像为训练集，剩下的为测试集\n",
    "    train_indices.extend(class_indices[:40])\n",
    "    test_indices.extend(class_indices[40:])\n",
    "\n",
    "# 使用索引创建训练集和测试集\n",
    "train_dataset = torch.utils.data.Subset(total_data, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(total_data, test_indices)\n",
    "\n",
    "# 打印训练集和测试集的大小\n",
    "print(f'训练集大小: {len(train_dataset)}')\n",
    "print(f'测试集大小: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2721c0cc-1c48-4d99-b2b9-a864f7b418f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=40, shuffle=True, num_workers=1)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=10, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "32cc0c97-ac87-4c1d-992d-ed0d832fc6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化\n",
    "num_classes = 1\n",
    "\n",
    "MAX_EPOCH = 30     \n",
    "LR = 0.01         \n",
    "log_interval = 10    # 每隔 1 个 训练批次（或 epoch）记录一次训练日志。 表示训练过程中，控制打印训练信息的频率。\n",
    "val_interval = 1    # 每隔 1 个 epoch 进行一次验证集的评估。验证集的评估是为了监控模型在验证集上的表现，从而避免过拟合。\n",
    "classes = 1         # 分类任务的类别数为 2。\n",
    "start_epoch = -1     \n",
    "lr_decay_step = 5   # 表示学习率的衰减步长。 每隔一定的 epoch（如 1）对学习率进行衰减，通常是为了使模型在后期更稳定地收敛。\n",
    "                    # 可结合优化器的学习率调度策略（如 StepLR）使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1d8119b-7376-4bf8-a420-f4b71fdccfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = device = torch.device(\"mps\")\n",
    "res_model = models.resnet18(pretrained=True)  # 使用预训练权重\n",
    "# 获取 ResNet 的最后一层输入特征数\n",
    "num_ftrs = res_model.fc.in_features  # 获取fc层的输入特征数\n",
    "res_model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 128),           # 将输入特征维度映射到4096\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1)                    # 输出为1，进行回归预测任务\n",
    ")\n",
    "print(num_ftrs)\n",
    "res_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c34fe78f-f175-4b43-8fcd-7fac588a443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2199202f-6b7c-43e0-90fc-7f99260162a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(res_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_decay_step, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e26df5a-8c6d-4228-8a3b-faf7dd3cddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch[000/030] Iteration[010/501] Loss: 168.6538\n",
      "Training: Epoch[000/030] Iteration[020/501] Loss: 81.3012\n",
      "Training: Epoch[000/030] Iteration[030/501] Loss: 61.0187\n",
      "Training: Epoch[000/030] Iteration[040/501] Loss: 54.9609\n",
      "Training: Epoch[000/030] Iteration[050/501] Loss: 44.2858\n",
      "Training: Epoch[000/030] Iteration[060/501] Loss: 46.1196\n",
      "Training: Epoch[000/030] Iteration[070/501] Loss: 43.8652\n",
      "Training: Epoch[000/030] Iteration[080/501] Loss: 46.7168\n",
      "Training: Epoch[000/030] Iteration[090/501] Loss: 36.0146\n",
      "Training: Epoch[000/030] Iteration[100/501] Loss: 34.0960\n",
      "Training: Epoch[000/030] Iteration[110/501] Loss: 33.1099\n",
      "Training: Epoch[000/030] Iteration[120/501] Loss: 33.1899\n",
      "Training: Epoch[000/030] Iteration[130/501] Loss: 40.6714\n",
      "Training: Epoch[000/030] Iteration[140/501] Loss: 36.5801\n",
      "Training: Epoch[000/030] Iteration[150/501] Loss: 43.8157\n",
      "Training: Epoch[000/030] Iteration[160/501] Loss: 40.6776\n",
      "Training: Epoch[000/030] Iteration[170/501] Loss: 33.7665\n",
      "Training: Epoch[000/030] Iteration[180/501] Loss: 41.7248\n",
      "Training: Epoch[000/030] Iteration[190/501] Loss: 35.1750\n",
      "Training: Epoch[000/030] Iteration[200/501] Loss: 34.0453\n",
      "Training: Epoch[000/030] Iteration[210/501] Loss: 31.9592\n",
      "Training: Epoch[000/030] Iteration[220/501] Loss: 29.5474\n",
      "Training: Epoch[000/030] Iteration[230/501] Loss: 31.1903\n",
      "Training: Epoch[000/030] Iteration[240/501] Loss: 34.5541\n",
      "Training: Epoch[000/030] Iteration[250/501] Loss: 38.2568\n",
      "Training: Epoch[000/030] Iteration[260/501] Loss: 36.7519\n",
      "Training: Epoch[000/030] Iteration[270/501] Loss: 31.7817\n",
      "Training: Epoch[000/030] Iteration[280/501] Loss: 32.0743\n",
      "Training: Epoch[000/030] Iteration[290/501] Loss: 31.5844\n",
      "Training: Epoch[000/030] Iteration[300/501] Loss: 35.3015\n",
      "Training: Epoch[000/030] Iteration[310/501] Loss: 35.5198\n",
      "Training: Epoch[000/030] Iteration[320/501] Loss: 36.6395\n",
      "Training: Epoch[000/030] Iteration[330/501] Loss: 32.5573\n",
      "Training: Epoch[000/030] Iteration[340/501] Loss: 37.8681\n",
      "Training: Epoch[000/030] Iteration[350/501] Loss: 28.2693\n",
      "Training: Epoch[000/030] Iteration[360/501] Loss: 37.7861\n",
      "Training: Epoch[000/030] Iteration[370/501] Loss: 44.4238\n",
      "Training: Epoch[000/030] Iteration[380/501] Loss: 39.1767\n",
      "Training: Epoch[000/030] Iteration[390/501] Loss: 46.2947\n",
      "Training: Epoch[000/030] Iteration[400/501] Loss: 39.8773\n",
      "Training: Epoch[000/030] Iteration[410/501] Loss: 49.5625\n",
      "Training: Epoch[000/030] Iteration[420/501] Loss: 39.0263\n",
      "Training: Epoch[000/030] Iteration[430/501] Loss: 37.4514\n",
      "Training: Epoch[000/030] Iteration[440/501] Loss: 34.0112\n",
      "Training: Epoch[000/030] Iteration[450/501] Loss: 41.9809\n",
      "Training: Epoch[000/030] Iteration[460/501] Loss: 30.5224\n",
      "Training: Epoch[000/030] Iteration[470/501] Loss: 34.9366\n",
      "Training: Epoch[000/030] Iteration[480/501] Loss: 29.4546\n",
      "Training: Epoch[000/030] Iteration[490/501] Loss: 37.3494\n",
      "Training: Epoch[000/030] Iteration[500/501] Loss: 37.7544\n",
      "Valid: Epoch[000/030] Iteration[501/501] Loss: 51.2292\n",
      "Training: Epoch[001/030] Iteration[010/501] Loss: 33.2834\n",
      "Training: Epoch[001/030] Iteration[020/501] Loss: 33.2089\n",
      "Training: Epoch[001/030] Iteration[030/501] Loss: 36.7763\n",
      "Training: Epoch[001/030] Iteration[040/501] Loss: 37.8385\n",
      "Training: Epoch[001/030] Iteration[050/501] Loss: 42.2883\n",
      "Training: Epoch[001/030] Iteration[060/501] Loss: 35.0324\n",
      "Training: Epoch[001/030] Iteration[070/501] Loss: 29.1773\n",
      "Training: Epoch[001/030] Iteration[080/501] Loss: 26.5724\n",
      "Training: Epoch[001/030] Iteration[090/501] Loss: 37.2793\n",
      "Training: Epoch[001/030] Iteration[100/501] Loss: 32.0271\n",
      "Training: Epoch[001/030] Iteration[110/501] Loss: 29.6186\n",
      "Training: Epoch[001/030] Iteration[120/501] Loss: 25.8263\n",
      "Training: Epoch[001/030] Iteration[130/501] Loss: 23.5214\n",
      "Training: Epoch[001/030] Iteration[140/501] Loss: 23.6598\n",
      "Training: Epoch[001/030] Iteration[150/501] Loss: 27.9704\n",
      "Training: Epoch[001/030] Iteration[160/501] Loss: 32.7553\n",
      "Training: Epoch[001/030] Iteration[170/501] Loss: 27.0209\n",
      "Training: Epoch[001/030] Iteration[180/501] Loss: 26.7965\n",
      "Training: Epoch[001/030] Iteration[190/501] Loss: 27.3163\n",
      "Training: Epoch[001/030] Iteration[200/501] Loss: 29.3732\n",
      "Training: Epoch[001/030] Iteration[210/501] Loss: 28.7212\n",
      "Training: Epoch[001/030] Iteration[220/501] Loss: 25.2033\n",
      "Training: Epoch[001/030] Iteration[230/501] Loss: 33.4622\n",
      "Training: Epoch[001/030] Iteration[240/501] Loss: 28.8349\n",
      "Training: Epoch[001/030] Iteration[250/501] Loss: 24.6443\n",
      "Training: Epoch[001/030] Iteration[260/501] Loss: 24.0953\n",
      "Training: Epoch[001/030] Iteration[270/501] Loss: 27.1367\n",
      "Training: Epoch[001/030] Iteration[280/501] Loss: 22.0341\n",
      "Training: Epoch[001/030] Iteration[290/501] Loss: 23.4139\n",
      "Training: Epoch[001/030] Iteration[300/501] Loss: 25.9623\n",
      "Training: Epoch[001/030] Iteration[310/501] Loss: 24.9412\n",
      "Training: Epoch[001/030] Iteration[320/501] Loss: 30.0302\n",
      "Training: Epoch[001/030] Iteration[330/501] Loss: 28.9132\n",
      "Training: Epoch[001/030] Iteration[340/501] Loss: 28.8803\n",
      "Training: Epoch[001/030] Iteration[350/501] Loss: 28.6815\n",
      "Training: Epoch[001/030] Iteration[360/501] Loss: 34.7683\n",
      "Training: Epoch[001/030] Iteration[370/501] Loss: 24.0014\n",
      "Training: Epoch[001/030] Iteration[380/501] Loss: 21.1958\n",
      "Training: Epoch[001/030] Iteration[390/501] Loss: 25.3207\n",
      "Training: Epoch[001/030] Iteration[400/501] Loss: 27.4359\n",
      "Training: Epoch[001/030] Iteration[410/501] Loss: 23.6207\n",
      "Training: Epoch[001/030] Iteration[420/501] Loss: 23.9747\n",
      "Training: Epoch[001/030] Iteration[430/501] Loss: 23.6428\n",
      "Training: Epoch[001/030] Iteration[440/501] Loss: 34.5487\n",
      "Training: Epoch[001/030] Iteration[450/501] Loss: 30.3135\n",
      "Training: Epoch[001/030] Iteration[460/501] Loss: 23.6720\n",
      "Training: Epoch[001/030] Iteration[470/501] Loss: 22.8595\n",
      "Training: Epoch[001/030] Iteration[480/501] Loss: 20.9367\n",
      "Training: Epoch[001/030] Iteration[490/501] Loss: 22.8273\n",
      "Training: Epoch[001/030] Iteration[500/501] Loss: 21.8942\n",
      "Valid: Epoch[001/030] Iteration[501/501] Loss: 21.3301\n",
      "Training: Epoch[002/030] Iteration[010/501] Loss: 23.6519\n",
      "Training: Epoch[002/030] Iteration[020/501] Loss: 20.5652\n",
      "Training: Epoch[002/030] Iteration[030/501] Loss: 23.7409\n",
      "Training: Epoch[002/030] Iteration[040/501] Loss: 24.2729\n",
      "Training: Epoch[002/030] Iteration[050/501] Loss: 19.3870\n",
      "Training: Epoch[002/030] Iteration[060/501] Loss: 22.3949\n",
      "Training: Epoch[002/030] Iteration[070/501] Loss: 17.5771\n",
      "Training: Epoch[002/030] Iteration[080/501] Loss: 22.3742\n",
      "Training: Epoch[002/030] Iteration[090/501] Loss: 23.3328\n",
      "Training: Epoch[002/030] Iteration[100/501] Loss: 27.3356\n",
      "Training: Epoch[002/030] Iteration[110/501] Loss: 20.2526\n",
      "Training: Epoch[002/030] Iteration[120/501] Loss: 19.3554\n",
      "Training: Epoch[002/030] Iteration[130/501] Loss: 20.3733\n",
      "Training: Epoch[002/030] Iteration[140/501] Loss: 22.1485\n",
      "Training: Epoch[002/030] Iteration[150/501] Loss: 23.1195\n",
      "Training: Epoch[002/030] Iteration[160/501] Loss: 23.9377\n",
      "Training: Epoch[002/030] Iteration[170/501] Loss: 17.5248\n",
      "Training: Epoch[002/030] Iteration[180/501] Loss: 22.0127\n",
      "Training: Epoch[002/030] Iteration[190/501] Loss: 26.9039\n",
      "Training: Epoch[002/030] Iteration[200/501] Loss: 25.2606\n",
      "Training: Epoch[002/030] Iteration[210/501] Loss: 23.3502\n",
      "Training: Epoch[002/030] Iteration[220/501] Loss: 20.5228\n",
      "Training: Epoch[002/030] Iteration[230/501] Loss: 20.5308\n",
      "Training: Epoch[002/030] Iteration[240/501] Loss: 20.0558\n",
      "Training: Epoch[002/030] Iteration[250/501] Loss: 26.1136\n",
      "Training: Epoch[002/030] Iteration[260/501] Loss: 19.6587\n",
      "Training: Epoch[002/030] Iteration[270/501] Loss: 20.3930\n",
      "Training: Epoch[002/030] Iteration[280/501] Loss: 23.7432\n",
      "Training: Epoch[002/030] Iteration[290/501] Loss: 26.4687\n",
      "Training: Epoch[002/030] Iteration[300/501] Loss: 22.0464\n",
      "Training: Epoch[002/030] Iteration[310/501] Loss: 19.3676\n",
      "Training: Epoch[002/030] Iteration[320/501] Loss: 18.8956\n",
      "Training: Epoch[002/030] Iteration[330/501] Loss: 21.4477\n",
      "Training: Epoch[002/030] Iteration[340/501] Loss: 20.0155\n",
      "Training: Epoch[002/030] Iteration[350/501] Loss: 24.0513\n",
      "Training: Epoch[002/030] Iteration[360/501] Loss: 18.8654\n",
      "Training: Epoch[002/030] Iteration[370/501] Loss: 21.1392\n",
      "Training: Epoch[002/030] Iteration[380/501] Loss: 23.2227\n",
      "Training: Epoch[002/030] Iteration[390/501] Loss: 21.2764\n",
      "Training: Epoch[002/030] Iteration[400/501] Loss: 20.3410\n",
      "Training: Epoch[002/030] Iteration[410/501] Loss: 21.1978\n",
      "Training: Epoch[002/030] Iteration[420/501] Loss: 23.7232\n",
      "Training: Epoch[002/030] Iteration[430/501] Loss: 24.4463\n",
      "Training: Epoch[002/030] Iteration[440/501] Loss: 26.6741\n",
      "Training: Epoch[002/030] Iteration[450/501] Loss: 20.3028\n",
      "Training: Epoch[002/030] Iteration[460/501] Loss: 18.8685\n",
      "Training: Epoch[002/030] Iteration[470/501] Loss: 20.6656\n",
      "Training: Epoch[002/030] Iteration[480/501] Loss: 18.2972\n",
      "Training: Epoch[002/030] Iteration[490/501] Loss: 22.0836\n",
      "Training: Epoch[002/030] Iteration[500/501] Loss: 25.0150\n",
      "Valid: Epoch[002/030] Iteration[501/501] Loss: 39.1004\n",
      "Training: Epoch[003/030] Iteration[010/501] Loss: 18.3656\n",
      "Training: Epoch[003/030] Iteration[020/501] Loss: 22.2463\n",
      "Training: Epoch[003/030] Iteration[030/501] Loss: 20.1812\n",
      "Training: Epoch[003/030] Iteration[040/501] Loss: 17.9766\n",
      "Training: Epoch[003/030] Iteration[050/501] Loss: 22.0726\n",
      "Training: Epoch[003/030] Iteration[060/501] Loss: 16.5052\n",
      "Training: Epoch[003/030] Iteration[070/501] Loss: 20.2974\n",
      "Training: Epoch[003/030] Iteration[080/501] Loss: 16.9067\n",
      "Training: Epoch[003/030] Iteration[090/501] Loss: 17.0378\n",
      "Training: Epoch[003/030] Iteration[100/501] Loss: 15.7696\n",
      "Training: Epoch[003/030] Iteration[110/501] Loss: 19.8626\n",
      "Training: Epoch[003/030] Iteration[120/501] Loss: 18.0921\n",
      "Training: Epoch[003/030] Iteration[130/501] Loss: 15.5397\n",
      "Training: Epoch[003/030] Iteration[140/501] Loss: 23.8982\n",
      "Training: Epoch[003/030] Iteration[150/501] Loss: 21.5436\n",
      "Training: Epoch[003/030] Iteration[160/501] Loss: 18.5915\n",
      "Training: Epoch[003/030] Iteration[170/501] Loss: 22.1732\n",
      "Training: Epoch[003/030] Iteration[180/501] Loss: 17.7303\n",
      "Training: Epoch[003/030] Iteration[190/501] Loss: 14.6326\n",
      "Training: Epoch[003/030] Iteration[200/501] Loss: 17.3277\n",
      "Training: Epoch[003/030] Iteration[210/501] Loss: 16.4936\n",
      "Training: Epoch[003/030] Iteration[220/501] Loss: 15.9728\n",
      "Training: Epoch[003/030] Iteration[230/501] Loss: 20.9497\n",
      "Training: Epoch[003/030] Iteration[240/501] Loss: 17.0147\n",
      "Training: Epoch[003/030] Iteration[250/501] Loss: 16.7352\n",
      "Training: Epoch[003/030] Iteration[260/501] Loss: 21.2401\n",
      "Training: Epoch[003/030] Iteration[270/501] Loss: 18.4994\n",
      "Training: Epoch[003/030] Iteration[280/501] Loss: 22.6184\n",
      "Training: Epoch[003/030] Iteration[290/501] Loss: 19.0342\n",
      "Training: Epoch[003/030] Iteration[300/501] Loss: 17.2636\n",
      "Training: Epoch[003/030] Iteration[310/501] Loss: 16.3783\n",
      "Training: Epoch[003/030] Iteration[320/501] Loss: 23.1753\n",
      "Training: Epoch[003/030] Iteration[330/501] Loss: 21.6052\n",
      "Training: Epoch[003/030] Iteration[340/501] Loss: 23.2239\n",
      "Training: Epoch[003/030] Iteration[350/501] Loss: 28.4601\n",
      "Training: Epoch[003/030] Iteration[360/501] Loss: 22.0428\n",
      "Training: Epoch[003/030] Iteration[370/501] Loss: 18.5942\n",
      "Training: Epoch[003/030] Iteration[380/501] Loss: 19.2399\n",
      "Training: Epoch[003/030] Iteration[390/501] Loss: 21.5753\n",
      "Training: Epoch[003/030] Iteration[400/501] Loss: 18.9917\n",
      "Training: Epoch[003/030] Iteration[410/501] Loss: 17.2851\n",
      "Training: Epoch[003/030] Iteration[420/501] Loss: 19.9567\n",
      "Training: Epoch[003/030] Iteration[430/501] Loss: 19.4643\n",
      "Training: Epoch[003/030] Iteration[440/501] Loss: 18.6306\n",
      "Training: Epoch[003/030] Iteration[450/501] Loss: 19.6984\n",
      "Training: Epoch[003/030] Iteration[460/501] Loss: 19.6849\n",
      "Training: Epoch[003/030] Iteration[470/501] Loss: 22.2233\n",
      "Training: Epoch[003/030] Iteration[480/501] Loss: 26.3442\n",
      "Training: Epoch[003/030] Iteration[490/501] Loss: 22.4270\n",
      "Training: Epoch[003/030] Iteration[500/501] Loss: 18.8452\n",
      "Valid: Epoch[003/030] Iteration[501/501] Loss: 20.5745\n",
      "Training: Epoch[004/030] Iteration[010/501] Loss: 14.1211\n",
      "Training: Epoch[004/030] Iteration[020/501] Loss: 17.8320\n",
      "Training: Epoch[004/030] Iteration[030/501] Loss: 18.3295\n",
      "Training: Epoch[004/030] Iteration[040/501] Loss: 20.6225\n",
      "Training: Epoch[004/030] Iteration[050/501] Loss: 28.7742\n",
      "Training: Epoch[004/030] Iteration[060/501] Loss: 23.8688\n",
      "Training: Epoch[004/030] Iteration[070/501] Loss: 18.9852\n",
      "Training: Epoch[004/030] Iteration[080/501] Loss: 14.4922\n",
      "Training: Epoch[004/030] Iteration[090/501] Loss: 20.0042\n",
      "Training: Epoch[004/030] Iteration[100/501] Loss: 21.8495\n",
      "Training: Epoch[004/030] Iteration[110/501] Loss: 18.6286\n",
      "Training: Epoch[004/030] Iteration[120/501] Loss: 19.8525\n",
      "Training: Epoch[004/030] Iteration[130/501] Loss: 18.3936\n",
      "Training: Epoch[004/030] Iteration[140/501] Loss: 17.3400\n",
      "Training: Epoch[004/030] Iteration[150/501] Loss: 17.7026\n",
      "Training: Epoch[004/030] Iteration[160/501] Loss: 16.2921\n",
      "Training: Epoch[004/030] Iteration[170/501] Loss: 19.6152\n",
      "Training: Epoch[004/030] Iteration[180/501] Loss: 19.2691\n",
      "Training: Epoch[004/030] Iteration[190/501] Loss: 17.8181\n",
      "Training: Epoch[004/030] Iteration[200/501] Loss: 19.4743\n",
      "Training: Epoch[004/030] Iteration[210/501] Loss: 13.5686\n",
      "Training: Epoch[004/030] Iteration[220/501] Loss: 17.9505\n",
      "Training: Epoch[004/030] Iteration[230/501] Loss: 16.0753\n",
      "Training: Epoch[004/030] Iteration[240/501] Loss: 16.6308\n",
      "Training: Epoch[004/030] Iteration[250/501] Loss: 18.4463\n",
      "Training: Epoch[004/030] Iteration[260/501] Loss: 14.7681\n",
      "Training: Epoch[004/030] Iteration[270/501] Loss: 14.3279\n",
      "Training: Epoch[004/030] Iteration[280/501] Loss: 17.3888\n",
      "Training: Epoch[004/030] Iteration[290/501] Loss: 13.7856\n",
      "Training: Epoch[004/030] Iteration[300/501] Loss: 16.9312\n",
      "Training: Epoch[004/030] Iteration[310/501] Loss: 16.2120\n",
      "Training: Epoch[004/030] Iteration[320/501] Loss: 18.9082\n",
      "Training: Epoch[004/030] Iteration[330/501] Loss: 22.6206\n",
      "Training: Epoch[004/030] Iteration[340/501] Loss: 18.0549\n",
      "Training: Epoch[004/030] Iteration[350/501] Loss: 18.9290\n",
      "Training: Epoch[004/030] Iteration[360/501] Loss: 21.2943\n",
      "Training: Epoch[004/030] Iteration[370/501] Loss: 23.0713\n",
      "Training: Epoch[004/030] Iteration[380/501] Loss: 19.9058\n",
      "Training: Epoch[004/030] Iteration[390/501] Loss: 21.0913\n",
      "Training: Epoch[004/030] Iteration[400/501] Loss: 17.6715\n",
      "Training: Epoch[004/030] Iteration[410/501] Loss: 16.4300\n",
      "Training: Epoch[004/030] Iteration[420/501] Loss: 17.7012\n",
      "Training: Epoch[004/030] Iteration[430/501] Loss: 10.8083\n",
      "Training: Epoch[004/030] Iteration[440/501] Loss: 13.0164\n",
      "Training: Epoch[004/030] Iteration[450/501] Loss: 16.4188\n",
      "Training: Epoch[004/030] Iteration[460/501] Loss: 16.6382\n",
      "Training: Epoch[004/030] Iteration[470/501] Loss: 13.4467\n",
      "Training: Epoch[004/030] Iteration[480/501] Loss: 14.0578\n",
      "Training: Epoch[004/030] Iteration[490/501] Loss: 13.5615\n",
      "Training: Epoch[004/030] Iteration[500/501] Loss: 12.9250\n",
      "Valid: Epoch[004/030] Iteration[501/501] Loss: 18.7390\n",
      "Training: Epoch[005/030] Iteration[010/501] Loss: 15.7313\n",
      "Training: Epoch[005/030] Iteration[020/501] Loss: 12.6834\n",
      "Training: Epoch[005/030] Iteration[030/501] Loss: 13.7577\n",
      "Training: Epoch[005/030] Iteration[040/501] Loss: 12.7103\n",
      "Training: Epoch[005/030] Iteration[050/501] Loss: 16.4926\n",
      "Training: Epoch[005/030] Iteration[060/501] Loss: 12.6788\n",
      "Training: Epoch[005/030] Iteration[070/501] Loss: 12.6698\n",
      "Training: Epoch[005/030] Iteration[080/501] Loss: 13.4134\n",
      "Training: Epoch[005/030] Iteration[090/501] Loss: 13.0129\n",
      "Training: Epoch[005/030] Iteration[100/501] Loss: 14.4502\n",
      "Training: Epoch[005/030] Iteration[110/501] Loss: 11.8912\n",
      "Training: Epoch[005/030] Iteration[120/501] Loss: 11.4898\n",
      "Training: Epoch[005/030] Iteration[130/501] Loss: 14.8711\n",
      "Training: Epoch[005/030] Iteration[140/501] Loss: 10.0147\n",
      "Training: Epoch[005/030] Iteration[150/501] Loss: 11.5542\n",
      "Training: Epoch[005/030] Iteration[160/501] Loss: 10.8215\n",
      "Training: Epoch[005/030] Iteration[170/501] Loss: 11.8968\n",
      "Training: Epoch[005/030] Iteration[180/501] Loss: 10.3661\n",
      "Training: Epoch[005/030] Iteration[190/501] Loss: 9.6257\n",
      "Training: Epoch[005/030] Iteration[200/501] Loss: 12.5660\n",
      "Training: Epoch[005/030] Iteration[210/501] Loss: 12.1173\n",
      "Training: Epoch[005/030] Iteration[220/501] Loss: 12.9152\n",
      "Training: Epoch[005/030] Iteration[230/501] Loss: 8.8780\n",
      "Training: Epoch[005/030] Iteration[240/501] Loss: 11.4396\n",
      "Training: Epoch[005/030] Iteration[250/501] Loss: 11.0391\n",
      "Training: Epoch[005/030] Iteration[260/501] Loss: 10.6963\n",
      "Training: Epoch[005/030] Iteration[270/501] Loss: 9.9130\n",
      "Training: Epoch[005/030] Iteration[280/501] Loss: 13.1444\n",
      "Training: Epoch[005/030] Iteration[290/501] Loss: 11.4536\n",
      "Training: Epoch[005/030] Iteration[300/501] Loss: 9.9006\n",
      "Training: Epoch[005/030] Iteration[310/501] Loss: 12.2182\n",
      "Training: Epoch[005/030] Iteration[320/501] Loss: 11.2171\n",
      "Training: Epoch[005/030] Iteration[330/501] Loss: 7.6675\n",
      "Training: Epoch[005/030] Iteration[340/501] Loss: 12.5787\n",
      "Training: Epoch[005/030] Iteration[350/501] Loss: 12.9913\n",
      "Training: Epoch[005/030] Iteration[360/501] Loss: 12.3034\n",
      "Training: Epoch[005/030] Iteration[370/501] Loss: 12.5719\n",
      "Training: Epoch[005/030] Iteration[380/501] Loss: 13.5307\n",
      "Training: Epoch[005/030] Iteration[390/501] Loss: 10.1450\n",
      "Training: Epoch[005/030] Iteration[400/501] Loss: 11.4342\n",
      "Training: Epoch[005/030] Iteration[410/501] Loss: 9.9847\n",
      "Training: Epoch[005/030] Iteration[420/501] Loss: 8.9458\n",
      "Training: Epoch[005/030] Iteration[430/501] Loss: 11.9707\n",
      "Training: Epoch[005/030] Iteration[440/501] Loss: 12.7512\n",
      "Training: Epoch[005/030] Iteration[450/501] Loss: 11.7857\n",
      "Training: Epoch[005/030] Iteration[460/501] Loss: 11.5816\n",
      "Training: Epoch[005/030] Iteration[470/501] Loss: 9.8369\n",
      "Training: Epoch[005/030] Iteration[480/501] Loss: 11.7040\n",
      "Training: Epoch[005/030] Iteration[490/501] Loss: 11.2903\n",
      "Training: Epoch[005/030] Iteration[500/501] Loss: 10.8245\n",
      "Valid: Epoch[005/030] Iteration[501/501] Loss: 12.2973\n",
      "Training: Epoch[006/030] Iteration[010/501] Loss: 12.2202\n",
      "Training: Epoch[006/030] Iteration[020/501] Loss: 11.8305\n",
      "Training: Epoch[006/030] Iteration[030/501] Loss: 10.2335\n",
      "Training: Epoch[006/030] Iteration[040/501] Loss: 9.8647\n",
      "Training: Epoch[006/030] Iteration[050/501] Loss: 12.1175\n",
      "Training: Epoch[006/030] Iteration[060/501] Loss: 13.6092\n",
      "Training: Epoch[006/030] Iteration[070/501] Loss: 9.8150\n",
      "Training: Epoch[006/030] Iteration[080/501] Loss: 11.1283\n",
      "Training: Epoch[006/030] Iteration[090/501] Loss: 9.3308\n",
      "Training: Epoch[006/030] Iteration[100/501] Loss: 11.2294\n",
      "Training: Epoch[006/030] Iteration[110/501] Loss: 9.8053\n",
      "Training: Epoch[006/030] Iteration[120/501] Loss: 13.4299\n",
      "Training: Epoch[006/030] Iteration[130/501] Loss: 10.9372\n",
      "Training: Epoch[006/030] Iteration[140/501] Loss: 11.4979\n",
      "Training: Epoch[006/030] Iteration[150/501] Loss: 13.6329\n",
      "Training: Epoch[006/030] Iteration[160/501] Loss: 10.3360\n",
      "Training: Epoch[006/030] Iteration[170/501] Loss: 9.9957\n",
      "Training: Epoch[006/030] Iteration[180/501] Loss: 12.9357\n",
      "Training: Epoch[006/030] Iteration[190/501] Loss: 9.3376\n",
      "Training: Epoch[006/030] Iteration[200/501] Loss: 9.1141\n",
      "Training: Epoch[006/030] Iteration[210/501] Loss: 8.9425\n",
      "Training: Epoch[006/030] Iteration[220/501] Loss: 13.1420\n",
      "Training: Epoch[006/030] Iteration[230/501] Loss: 11.8985\n",
      "Training: Epoch[006/030] Iteration[240/501] Loss: 11.8969\n",
      "Training: Epoch[006/030] Iteration[250/501] Loss: 11.3168\n",
      "Training: Epoch[006/030] Iteration[260/501] Loss: 9.9116\n",
      "Training: Epoch[006/030] Iteration[270/501] Loss: 9.3129\n",
      "Training: Epoch[006/030] Iteration[280/501] Loss: 11.9750\n",
      "Training: Epoch[006/030] Iteration[290/501] Loss: 10.1634\n",
      "Training: Epoch[006/030] Iteration[300/501] Loss: 11.0370\n",
      "Training: Epoch[006/030] Iteration[310/501] Loss: 9.5549\n",
      "Training: Epoch[006/030] Iteration[320/501] Loss: 10.3002\n",
      "Training: Epoch[006/030] Iteration[330/501] Loss: 11.7902\n",
      "Training: Epoch[006/030] Iteration[340/501] Loss: 9.5774\n",
      "Training: Epoch[006/030] Iteration[350/501] Loss: 12.6562\n",
      "Training: Epoch[006/030] Iteration[360/501] Loss: 11.8090\n",
      "Training: Epoch[006/030] Iteration[370/501] Loss: 9.8455\n",
      "Training: Epoch[006/030] Iteration[380/501] Loss: 8.1714\n",
      "Training: Epoch[006/030] Iteration[390/501] Loss: 9.4372\n",
      "Training: Epoch[006/030] Iteration[400/501] Loss: 11.0689\n",
      "Training: Epoch[006/030] Iteration[410/501] Loss: 10.7083\n",
      "Training: Epoch[006/030] Iteration[420/501] Loss: 11.5684\n",
      "Training: Epoch[006/030] Iteration[430/501] Loss: 8.5149\n",
      "Training: Epoch[006/030] Iteration[440/501] Loss: 10.1209\n",
      "Training: Epoch[006/030] Iteration[450/501] Loss: 10.6821\n",
      "Training: Epoch[006/030] Iteration[460/501] Loss: 10.8698\n",
      "Training: Epoch[006/030] Iteration[470/501] Loss: 11.4628\n",
      "Training: Epoch[006/030] Iteration[480/501] Loss: 9.0565\n",
      "Training: Epoch[006/030] Iteration[490/501] Loss: 11.7307\n",
      "Training: Epoch[006/030] Iteration[500/501] Loss: 9.7730\n",
      "Valid: Epoch[006/030] Iteration[501/501] Loss: 11.8927\n",
      "Training: Epoch[007/030] Iteration[010/501] Loss: 10.0427\n",
      "Training: Epoch[007/030] Iteration[020/501] Loss: 8.1926\n",
      "Training: Epoch[007/030] Iteration[030/501] Loss: 10.6719\n",
      "Training: Epoch[007/030] Iteration[040/501] Loss: 9.5155\n",
      "Training: Epoch[007/030] Iteration[050/501] Loss: 7.5600\n",
      "Training: Epoch[007/030] Iteration[060/501] Loss: 10.6802\n",
      "Training: Epoch[007/030] Iteration[070/501] Loss: 11.5465\n",
      "Training: Epoch[007/030] Iteration[080/501] Loss: 11.5531\n",
      "Training: Epoch[007/030] Iteration[090/501] Loss: 8.9034\n",
      "Training: Epoch[007/030] Iteration[100/501] Loss: 11.0023\n",
      "Training: Epoch[007/030] Iteration[110/501] Loss: 10.5189\n",
      "Training: Epoch[007/030] Iteration[120/501] Loss: 8.6014\n",
      "Training: Epoch[007/030] Iteration[130/501] Loss: 11.8448\n",
      "Training: Epoch[007/030] Iteration[140/501] Loss: 13.1645\n",
      "Training: Epoch[007/030] Iteration[150/501] Loss: 8.2807\n",
      "Training: Epoch[007/030] Iteration[160/501] Loss: 11.2560\n",
      "Training: Epoch[007/030] Iteration[170/501] Loss: 10.6012\n",
      "Training: Epoch[007/030] Iteration[180/501] Loss: 9.1533\n",
      "Training: Epoch[007/030] Iteration[190/501] Loss: 11.1557\n",
      "Training: Epoch[007/030] Iteration[200/501] Loss: 9.9476\n",
      "Training: Epoch[007/030] Iteration[210/501] Loss: 9.3156\n",
      "Training: Epoch[007/030] Iteration[220/501] Loss: 9.7928\n",
      "Training: Epoch[007/030] Iteration[230/501] Loss: 11.8938\n",
      "Training: Epoch[007/030] Iteration[240/501] Loss: 9.9001\n",
      "Training: Epoch[007/030] Iteration[250/501] Loss: 12.1129\n",
      "Training: Epoch[007/030] Iteration[260/501] Loss: 10.9642\n",
      "Training: Epoch[007/030] Iteration[270/501] Loss: 10.0617\n",
      "Training: Epoch[007/030] Iteration[280/501] Loss: 10.4415\n",
      "Training: Epoch[007/030] Iteration[290/501] Loss: 9.0694\n",
      "Training: Epoch[007/030] Iteration[300/501] Loss: 11.4049\n",
      "Training: Epoch[007/030] Iteration[310/501] Loss: 13.1119\n",
      "Training: Epoch[007/030] Iteration[320/501] Loss: 13.1398\n",
      "Training: Epoch[007/030] Iteration[330/501] Loss: 9.6556\n",
      "Training: Epoch[007/030] Iteration[340/501] Loss: 12.1881\n",
      "Training: Epoch[007/030] Iteration[350/501] Loss: 10.3570\n",
      "Training: Epoch[007/030] Iteration[360/501] Loss: 8.3182\n",
      "Training: Epoch[007/030] Iteration[370/501] Loss: 10.4023\n",
      "Training: Epoch[007/030] Iteration[380/501] Loss: 10.3523\n",
      "Training: Epoch[007/030] Iteration[390/501] Loss: 11.5778\n",
      "Training: Epoch[007/030] Iteration[400/501] Loss: 7.2808\n",
      "Training: Epoch[007/030] Iteration[410/501] Loss: 8.4677\n",
      "Training: Epoch[007/030] Iteration[420/501] Loss: 9.5659\n",
      "Training: Epoch[007/030] Iteration[430/501] Loss: 10.6085\n",
      "Training: Epoch[007/030] Iteration[440/501] Loss: 9.5460\n",
      "Training: Epoch[007/030] Iteration[450/501] Loss: 8.7998\n",
      "Training: Epoch[007/030] Iteration[460/501] Loss: 11.3705\n",
      "Training: Epoch[007/030] Iteration[470/501] Loss: 12.5320\n",
      "Training: Epoch[007/030] Iteration[480/501] Loss: 7.9325\n",
      "Training: Epoch[007/030] Iteration[490/501] Loss: 9.6022\n",
      "Training: Epoch[007/030] Iteration[500/501] Loss: 8.8393\n",
      "Valid: Epoch[007/030] Iteration[501/501] Loss: 10.8026\n",
      "Training: Epoch[008/030] Iteration[010/501] Loss: 8.5104\n",
      "Training: Epoch[008/030] Iteration[020/501] Loss: 8.4030\n",
      "Training: Epoch[008/030] Iteration[030/501] Loss: 11.8631\n",
      "Training: Epoch[008/030] Iteration[040/501] Loss: 7.1223\n",
      "Training: Epoch[008/030] Iteration[050/501] Loss: 7.5211\n",
      "Training: Epoch[008/030] Iteration[060/501] Loss: 10.9041\n",
      "Training: Epoch[008/030] Iteration[070/501] Loss: 9.1802\n",
      "Training: Epoch[008/030] Iteration[080/501] Loss: 9.5379\n",
      "Training: Epoch[008/030] Iteration[090/501] Loss: 9.5337\n",
      "Training: Epoch[008/030] Iteration[100/501] Loss: 8.6582\n",
      "Training: Epoch[008/030] Iteration[110/501] Loss: 12.0068\n",
      "Training: Epoch[008/030] Iteration[120/501] Loss: 11.7144\n",
      "Training: Epoch[008/030] Iteration[130/501] Loss: 10.7721\n",
      "Training: Epoch[008/030] Iteration[140/501] Loss: 11.4109\n",
      "Training: Epoch[008/030] Iteration[150/501] Loss: 8.6225\n",
      "Training: Epoch[008/030] Iteration[160/501] Loss: 9.3947\n",
      "Training: Epoch[008/030] Iteration[170/501] Loss: 7.2525\n",
      "Training: Epoch[008/030] Iteration[180/501] Loss: 10.2006\n",
      "Training: Epoch[008/030] Iteration[190/501] Loss: 9.8582\n",
      "Training: Epoch[008/030] Iteration[200/501] Loss: 10.7905\n",
      "Training: Epoch[008/030] Iteration[210/501] Loss: 5.5030\n",
      "Training: Epoch[008/030] Iteration[220/501] Loss: 9.5191\n",
      "Training: Epoch[008/030] Iteration[230/501] Loss: 8.2761\n",
      "Training: Epoch[008/030] Iteration[240/501] Loss: 13.1084\n",
      "Training: Epoch[008/030] Iteration[250/501] Loss: 6.1815\n",
      "Training: Epoch[008/030] Iteration[260/501] Loss: 9.3298\n",
      "Training: Epoch[008/030] Iteration[270/501] Loss: 8.9152\n",
      "Training: Epoch[008/030] Iteration[280/501] Loss: 9.5228\n",
      "Training: Epoch[008/030] Iteration[290/501] Loss: 10.6972\n",
      "Training: Epoch[008/030] Iteration[300/501] Loss: 10.9318\n",
      "Training: Epoch[008/030] Iteration[310/501] Loss: 9.4632\n",
      "Training: Epoch[008/030] Iteration[320/501] Loss: 8.2275\n",
      "Training: Epoch[008/030] Iteration[330/501] Loss: 7.7677\n",
      "Training: Epoch[008/030] Iteration[340/501] Loss: 9.0169\n",
      "Training: Epoch[008/030] Iteration[350/501] Loss: 9.5753\n",
      "Training: Epoch[008/030] Iteration[360/501] Loss: 10.9597\n",
      "Training: Epoch[008/030] Iteration[370/501] Loss: 9.1396\n",
      "Training: Epoch[008/030] Iteration[380/501] Loss: 8.4612\n",
      "Training: Epoch[008/030] Iteration[390/501] Loss: 7.2929\n",
      "Training: Epoch[008/030] Iteration[400/501] Loss: 8.4105\n",
      "Training: Epoch[008/030] Iteration[410/501] Loss: 10.7929\n",
      "Training: Epoch[008/030] Iteration[420/501] Loss: 12.8731\n",
      "Training: Epoch[008/030] Iteration[430/501] Loss: 10.7633\n",
      "Training: Epoch[008/030] Iteration[440/501] Loss: 10.7216\n",
      "Training: Epoch[008/030] Iteration[450/501] Loss: 8.2918\n",
      "Training: Epoch[008/030] Iteration[460/501] Loss: 10.9389\n",
      "Training: Epoch[008/030] Iteration[470/501] Loss: 9.8115\n",
      "Training: Epoch[008/030] Iteration[480/501] Loss: 9.5176\n",
      "Training: Epoch[008/030] Iteration[490/501] Loss: 7.4857\n",
      "Training: Epoch[008/030] Iteration[500/501] Loss: 9.7075\n",
      "Valid: Epoch[008/030] Iteration[501/501] Loss: 10.2389\n",
      "Training: Epoch[009/030] Iteration[010/501] Loss: 9.7137\n",
      "Training: Epoch[009/030] Iteration[020/501] Loss: 8.7295\n",
      "Training: Epoch[009/030] Iteration[030/501] Loss: 9.7560\n",
      "Training: Epoch[009/030] Iteration[040/501] Loss: 12.7381\n",
      "Training: Epoch[009/030] Iteration[050/501] Loss: 9.0022\n",
      "Training: Epoch[009/030] Iteration[060/501] Loss: 8.8452\n",
      "Training: Epoch[009/030] Iteration[070/501] Loss: 9.7361\n",
      "Training: Epoch[009/030] Iteration[080/501] Loss: 9.6937\n",
      "Training: Epoch[009/030] Iteration[090/501] Loss: 10.6562\n",
      "Training: Epoch[009/030] Iteration[100/501] Loss: 9.4931\n",
      "Training: Epoch[009/030] Iteration[110/501] Loss: 9.9939\n",
      "Training: Epoch[009/030] Iteration[120/501] Loss: 9.2143\n",
      "Training: Epoch[009/030] Iteration[130/501] Loss: 6.1452\n",
      "Training: Epoch[009/030] Iteration[140/501] Loss: 9.0995\n",
      "Training: Epoch[009/030] Iteration[150/501] Loss: 10.6896\n",
      "Training: Epoch[009/030] Iteration[160/501] Loss: 9.3450\n",
      "Training: Epoch[009/030] Iteration[170/501] Loss: 10.7987\n",
      "Training: Epoch[009/030] Iteration[180/501] Loss: 8.1954\n",
      "Training: Epoch[009/030] Iteration[190/501] Loss: 8.2049\n",
      "Training: Epoch[009/030] Iteration[200/501] Loss: 8.5183\n",
      "Training: Epoch[009/030] Iteration[210/501] Loss: 9.7597\n",
      "Training: Epoch[009/030] Iteration[220/501] Loss: 8.7112\n",
      "Training: Epoch[009/030] Iteration[230/501] Loss: 9.3927\n",
      "Training: Epoch[009/030] Iteration[240/501] Loss: 9.6776\n",
      "Training: Epoch[009/030] Iteration[250/501] Loss: 7.2619\n",
      "Training: Epoch[009/030] Iteration[260/501] Loss: 6.9774\n",
      "Training: Epoch[009/030] Iteration[270/501] Loss: 9.6036\n",
      "Training: Epoch[009/030] Iteration[280/501] Loss: 9.6934\n",
      "Training: Epoch[009/030] Iteration[290/501] Loss: 8.8871\n",
      "Training: Epoch[009/030] Iteration[300/501] Loss: 6.6708\n",
      "Training: Epoch[009/030] Iteration[310/501] Loss: 9.5866\n",
      "Training: Epoch[009/030] Iteration[320/501] Loss: 8.8985\n",
      "Training: Epoch[009/030] Iteration[330/501] Loss: 9.3137\n",
      "Training: Epoch[009/030] Iteration[340/501] Loss: 8.8323\n",
      "Training: Epoch[009/030] Iteration[350/501] Loss: 7.9642\n",
      "Training: Epoch[009/030] Iteration[360/501] Loss: 7.6603\n",
      "Training: Epoch[009/030] Iteration[370/501] Loss: 11.5204\n",
      "Training: Epoch[009/030] Iteration[380/501] Loss: 8.4766\n",
      "Training: Epoch[009/030] Iteration[390/501] Loss: 9.2288\n",
      "Training: Epoch[009/030] Iteration[400/501] Loss: 8.9021\n",
      "Training: Epoch[009/030] Iteration[410/501] Loss: 8.4312\n",
      "Training: Epoch[009/030] Iteration[420/501] Loss: 8.0029\n",
      "Training: Epoch[009/030] Iteration[430/501] Loss: 7.4077\n",
      "Training: Epoch[009/030] Iteration[440/501] Loss: 11.1575\n",
      "Training: Epoch[009/030] Iteration[450/501] Loss: 8.9526\n",
      "Training: Epoch[009/030] Iteration[460/501] Loss: 10.1354\n",
      "Training: Epoch[009/030] Iteration[470/501] Loss: 8.8612\n",
      "Training: Epoch[009/030] Iteration[480/501] Loss: 12.7911\n",
      "Training: Epoch[009/030] Iteration[490/501] Loss: 9.0893\n",
      "Training: Epoch[009/030] Iteration[500/501] Loss: 9.1940\n",
      "Valid: Epoch[009/030] Iteration[501/501] Loss: 10.2831\n",
      "Training: Epoch[010/030] Iteration[010/501] Loss: 7.5018\n",
      "Training: Epoch[010/030] Iteration[020/501] Loss: 7.9680\n",
      "Training: Epoch[010/030] Iteration[030/501] Loss: 6.9936\n",
      "Training: Epoch[010/030] Iteration[040/501] Loss: 8.2994\n",
      "Training: Epoch[010/030] Iteration[050/501] Loss: 9.6254\n",
      "Training: Epoch[010/030] Iteration[060/501] Loss: 8.9916\n",
      "Training: Epoch[010/030] Iteration[070/501] Loss: 9.1333\n",
      "Training: Epoch[010/030] Iteration[080/501] Loss: 12.3807\n",
      "Training: Epoch[010/030] Iteration[090/501] Loss: 7.3909\n",
      "Training: Epoch[010/030] Iteration[100/501] Loss: 11.1257\n",
      "Training: Epoch[010/030] Iteration[110/501] Loss: 6.5201\n",
      "Training: Epoch[010/030] Iteration[120/501] Loss: 7.4045\n",
      "Training: Epoch[010/030] Iteration[130/501] Loss: 8.1936\n",
      "Training: Epoch[010/030] Iteration[140/501] Loss: 9.3584\n",
      "Training: Epoch[010/030] Iteration[150/501] Loss: 7.9829\n",
      "Training: Epoch[010/030] Iteration[160/501] Loss: 5.6230\n",
      "Training: Epoch[010/030] Iteration[170/501] Loss: 8.0955\n",
      "Training: Epoch[010/030] Iteration[180/501] Loss: 10.6665\n",
      "Training: Epoch[010/030] Iteration[190/501] Loss: 8.5728\n",
      "Training: Epoch[010/030] Iteration[200/501] Loss: 9.2028\n",
      "Training: Epoch[010/030] Iteration[210/501] Loss: 7.2812\n",
      "Training: Epoch[010/030] Iteration[220/501] Loss: 7.0747\n",
      "Training: Epoch[010/030] Iteration[230/501] Loss: 10.4061\n",
      "Training: Epoch[010/030] Iteration[240/501] Loss: 8.4488\n",
      "Training: Epoch[010/030] Iteration[250/501] Loss: 7.3299\n",
      "Training: Epoch[010/030] Iteration[260/501] Loss: 9.2181\n",
      "Training: Epoch[010/030] Iteration[270/501] Loss: 6.9909\n",
      "Training: Epoch[010/030] Iteration[280/501] Loss: 8.2915\n",
      "Training: Epoch[010/030] Iteration[290/501] Loss: 8.7076\n",
      "Training: Epoch[010/030] Iteration[300/501] Loss: 9.2029\n",
      "Training: Epoch[010/030] Iteration[310/501] Loss: 7.9025\n",
      "Training: Epoch[010/030] Iteration[320/501] Loss: 6.6715\n",
      "Training: Epoch[010/030] Iteration[330/501] Loss: 8.3036\n",
      "Training: Epoch[010/030] Iteration[340/501] Loss: 7.3680\n",
      "Training: Epoch[010/030] Iteration[350/501] Loss: 7.5436\n",
      "Training: Epoch[010/030] Iteration[360/501] Loss: 7.9964\n",
      "Training: Epoch[010/030] Iteration[370/501] Loss: 7.4694\n",
      "Training: Epoch[010/030] Iteration[380/501] Loss: 8.4050\n",
      "Training: Epoch[010/030] Iteration[390/501] Loss: 6.7736\n",
      "Training: Epoch[010/030] Iteration[400/501] Loss: 7.3448\n",
      "Training: Epoch[010/030] Iteration[410/501] Loss: 7.6711\n",
      "Training: Epoch[010/030] Iteration[420/501] Loss: 5.9512\n",
      "Training: Epoch[010/030] Iteration[430/501] Loss: 7.4121\n",
      "Training: Epoch[010/030] Iteration[440/501] Loss: 5.8382\n",
      "Training: Epoch[010/030] Iteration[450/501] Loss: 7.9010\n",
      "Training: Epoch[010/030] Iteration[460/501] Loss: 8.3212\n",
      "Training: Epoch[010/030] Iteration[470/501] Loss: 8.7280\n",
      "Training: Epoch[010/030] Iteration[480/501] Loss: 10.3101\n",
      "Training: Epoch[010/030] Iteration[490/501] Loss: 6.5807\n",
      "Training: Epoch[010/030] Iteration[500/501] Loss: 11.6905\n",
      "Valid: Epoch[010/030] Iteration[501/501] Loss: 10.2719\n",
      "Training: Epoch[011/030] Iteration[010/501] Loss: 8.5111\n",
      "Training: Epoch[011/030] Iteration[020/501] Loss: 8.0900\n",
      "Training: Epoch[011/030] Iteration[030/501] Loss: 6.8059\n",
      "Training: Epoch[011/030] Iteration[040/501] Loss: 6.9119\n",
      "Training: Epoch[011/030] Iteration[050/501] Loss: 7.4757\n",
      "Training: Epoch[011/030] Iteration[060/501] Loss: 7.3728\n",
      "Training: Epoch[011/030] Iteration[070/501] Loss: 5.1992\n",
      "Training: Epoch[011/030] Iteration[080/501] Loss: 7.3063\n",
      "Training: Epoch[011/030] Iteration[090/501] Loss: 10.5806\n",
      "Training: Epoch[011/030] Iteration[100/501] Loss: 8.6312\n",
      "Training: Epoch[011/030] Iteration[110/501] Loss: 7.6013\n",
      "Training: Epoch[011/030] Iteration[120/501] Loss: 8.4534\n",
      "Training: Epoch[011/030] Iteration[130/501] Loss: 8.5032\n",
      "Training: Epoch[011/030] Iteration[140/501] Loss: 7.7772\n",
      "Training: Epoch[011/030] Iteration[150/501] Loss: 9.3519\n",
      "Training: Epoch[011/030] Iteration[160/501] Loss: 7.7778\n",
      "Training: Epoch[011/030] Iteration[170/501] Loss: 5.3674\n",
      "Training: Epoch[011/030] Iteration[180/501] Loss: 8.7510\n",
      "Training: Epoch[011/030] Iteration[190/501] Loss: 7.8001\n",
      "Training: Epoch[011/030] Iteration[200/501] Loss: 13.7073\n",
      "Training: Epoch[011/030] Iteration[210/501] Loss: 6.5522\n",
      "Training: Epoch[011/030] Iteration[220/501] Loss: 8.0126\n",
      "Training: Epoch[011/030] Iteration[230/501] Loss: 6.7520\n",
      "Training: Epoch[011/030] Iteration[240/501] Loss: 7.9234\n",
      "Training: Epoch[011/030] Iteration[250/501] Loss: 10.3199\n",
      "Training: Epoch[011/030] Iteration[260/501] Loss: 6.7515\n",
      "Training: Epoch[011/030] Iteration[270/501] Loss: 7.7990\n",
      "Training: Epoch[011/030] Iteration[280/501] Loss: 8.3435\n",
      "Training: Epoch[011/030] Iteration[290/501] Loss: 7.2184\n",
      "Training: Epoch[011/030] Iteration[300/501] Loss: 8.2773\n",
      "Training: Epoch[011/030] Iteration[310/501] Loss: 6.5481\n",
      "Training: Epoch[011/030] Iteration[320/501] Loss: 9.3087\n",
      "Training: Epoch[011/030] Iteration[330/501] Loss: 6.8917\n",
      "Training: Epoch[011/030] Iteration[340/501] Loss: 7.2874\n",
      "Training: Epoch[011/030] Iteration[350/501] Loss: 7.5401\n",
      "Training: Epoch[011/030] Iteration[360/501] Loss: 8.2996\n",
      "Training: Epoch[011/030] Iteration[370/501] Loss: 8.9482\n",
      "Training: Epoch[011/030] Iteration[380/501] Loss: 8.6358\n",
      "Training: Epoch[011/030] Iteration[390/501] Loss: 6.3760\n",
      "Training: Epoch[011/030] Iteration[400/501] Loss: 7.0214\n",
      "Training: Epoch[011/030] Iteration[410/501] Loss: 7.7727\n",
      "Training: Epoch[011/030] Iteration[420/501] Loss: 9.6603\n",
      "Training: Epoch[011/030] Iteration[430/501] Loss: 7.6617\n",
      "Training: Epoch[011/030] Iteration[440/501] Loss: 7.8780\n",
      "Training: Epoch[011/030] Iteration[450/501] Loss: 7.7949\n",
      "Training: Epoch[011/030] Iteration[460/501] Loss: 6.5948\n",
      "Training: Epoch[011/030] Iteration[470/501] Loss: 8.1664\n",
      "Training: Epoch[011/030] Iteration[480/501] Loss: 9.9154\n",
      "Training: Epoch[011/030] Iteration[490/501] Loss: 9.7542\n",
      "Training: Epoch[011/030] Iteration[500/501] Loss: 9.9892\n",
      "Valid: Epoch[011/030] Iteration[501/501] Loss: 9.4394\n",
      "Training: Epoch[012/030] Iteration[010/501] Loss: 6.5475\n",
      "Training: Epoch[012/030] Iteration[020/501] Loss: 7.5572\n",
      "Training: Epoch[012/030] Iteration[030/501] Loss: 7.6777\n",
      "Training: Epoch[012/030] Iteration[040/501] Loss: 8.7862\n",
      "Training: Epoch[012/030] Iteration[050/501] Loss: 5.4506\n",
      "Training: Epoch[012/030] Iteration[060/501] Loss: 8.6106\n",
      "Training: Epoch[012/030] Iteration[070/501] Loss: 7.4041\n",
      "Training: Epoch[012/030] Iteration[080/501] Loss: 9.1045\n",
      "Training: Epoch[012/030] Iteration[090/501] Loss: 7.2260\n",
      "Training: Epoch[012/030] Iteration[100/501] Loss: 11.0852\n",
      "Training: Epoch[012/030] Iteration[110/501] Loss: 7.7261\n",
      "Training: Epoch[012/030] Iteration[120/501] Loss: 6.6859\n",
      "Training: Epoch[012/030] Iteration[130/501] Loss: 8.2593\n",
      "Training: Epoch[012/030] Iteration[140/501] Loss: 8.6517\n",
      "Training: Epoch[012/030] Iteration[150/501] Loss: 4.3139\n",
      "Training: Epoch[012/030] Iteration[160/501] Loss: 9.7365\n",
      "Training: Epoch[012/030] Iteration[170/501] Loss: 5.7373\n",
      "Training: Epoch[012/030] Iteration[180/501] Loss: 7.4587\n",
      "Training: Epoch[012/030] Iteration[190/501] Loss: 10.4124\n",
      "Training: Epoch[012/030] Iteration[200/501] Loss: 8.3450\n",
      "Training: Epoch[012/030] Iteration[210/501] Loss: 7.9175\n",
      "Training: Epoch[012/030] Iteration[220/501] Loss: 8.2130\n",
      "Training: Epoch[012/030] Iteration[230/501] Loss: 7.1906\n",
      "Training: Epoch[012/030] Iteration[240/501] Loss: 9.3477\n",
      "Training: Epoch[012/030] Iteration[250/501] Loss: 8.5084\n",
      "Training: Epoch[012/030] Iteration[260/501] Loss: 8.8410\n",
      "Training: Epoch[012/030] Iteration[270/501] Loss: 6.2798\n",
      "Training: Epoch[012/030] Iteration[280/501] Loss: 9.0852\n",
      "Training: Epoch[012/030] Iteration[290/501] Loss: 6.7886\n",
      "Training: Epoch[012/030] Iteration[300/501] Loss: 7.1569\n",
      "Training: Epoch[012/030] Iteration[310/501] Loss: 8.3656\n",
      "Training: Epoch[012/030] Iteration[320/501] Loss: 8.0710\n",
      "Training: Epoch[012/030] Iteration[330/501] Loss: 5.2106\n",
      "Training: Epoch[012/030] Iteration[340/501] Loss: 9.5958\n",
      "Training: Epoch[012/030] Iteration[350/501] Loss: 7.8925\n",
      "Training: Epoch[012/030] Iteration[360/501] Loss: 9.6757\n",
      "Training: Epoch[012/030] Iteration[370/501] Loss: 8.5328\n",
      "Training: Epoch[012/030] Iteration[380/501] Loss: 8.8134\n",
      "Training: Epoch[012/030] Iteration[390/501] Loss: 8.2426\n",
      "Training: Epoch[012/030] Iteration[400/501] Loss: 6.9305\n",
      "Training: Epoch[012/030] Iteration[410/501] Loss: 7.5792\n",
      "Training: Epoch[012/030] Iteration[420/501] Loss: 6.5768\n",
      "Training: Epoch[012/030] Iteration[430/501] Loss: 8.7649\n",
      "Training: Epoch[012/030] Iteration[440/501] Loss: 8.1007\n",
      "Training: Epoch[012/030] Iteration[450/501] Loss: 6.2541\n",
      "Training: Epoch[012/030] Iteration[460/501] Loss: 10.4103\n",
      "Training: Epoch[012/030] Iteration[470/501] Loss: 8.8243\n",
      "Training: Epoch[012/030] Iteration[480/501] Loss: 6.5137\n",
      "Training: Epoch[012/030] Iteration[490/501] Loss: 5.2903\n",
      "Training: Epoch[012/030] Iteration[500/501] Loss: 10.4965\n",
      "Valid: Epoch[012/030] Iteration[501/501] Loss: 9.7204\n",
      "Training: Epoch[013/030] Iteration[010/501] Loss: 8.2181\n",
      "Training: Epoch[013/030] Iteration[020/501] Loss: 9.2483\n",
      "Training: Epoch[013/030] Iteration[030/501] Loss: 7.3193\n",
      "Training: Epoch[013/030] Iteration[040/501] Loss: 9.4453\n",
      "Training: Epoch[013/030] Iteration[050/501] Loss: 9.5987\n",
      "Training: Epoch[013/030] Iteration[060/501] Loss: 9.0986\n",
      "Training: Epoch[013/030] Iteration[070/501] Loss: 9.6712\n",
      "Training: Epoch[013/030] Iteration[080/501] Loss: 6.9160\n",
      "Training: Epoch[013/030] Iteration[090/501] Loss: 7.0272\n",
      "Training: Epoch[013/030] Iteration[100/501] Loss: 7.3603\n",
      "Training: Epoch[013/030] Iteration[110/501] Loss: 7.2712\n",
      "Training: Epoch[013/030] Iteration[120/501] Loss: 8.1524\n",
      "Training: Epoch[013/030] Iteration[130/501] Loss: 6.0043\n",
      "Training: Epoch[013/030] Iteration[140/501] Loss: 7.5876\n",
      "Training: Epoch[013/030] Iteration[150/501] Loss: 7.6996\n",
      "Training: Epoch[013/030] Iteration[160/501] Loss: 6.4783\n",
      "Training: Epoch[013/030] Iteration[170/501] Loss: 8.0399\n",
      "Training: Epoch[013/030] Iteration[180/501] Loss: 5.9709\n",
      "Training: Epoch[013/030] Iteration[190/501] Loss: 9.5935\n",
      "Training: Epoch[013/030] Iteration[200/501] Loss: 6.7065\n",
      "Training: Epoch[013/030] Iteration[210/501] Loss: 9.7629\n",
      "Training: Epoch[013/030] Iteration[220/501] Loss: 8.1875\n",
      "Training: Epoch[013/030] Iteration[230/501] Loss: 7.9742\n",
      "Training: Epoch[013/030] Iteration[240/501] Loss: 6.7682\n",
      "Training: Epoch[013/030] Iteration[250/501] Loss: 6.4365\n",
      "Training: Epoch[013/030] Iteration[260/501] Loss: 8.1431\n",
      "Training: Epoch[013/030] Iteration[270/501] Loss: 8.0646\n",
      "Training: Epoch[013/030] Iteration[280/501] Loss: 10.0399\n",
      "Training: Epoch[013/030] Iteration[290/501] Loss: 9.4417\n",
      "Training: Epoch[013/030] Iteration[300/501] Loss: 10.7277\n",
      "Training: Epoch[013/030] Iteration[310/501] Loss: 6.9120\n",
      "Training: Epoch[013/030] Iteration[320/501] Loss: 6.7377\n",
      "Training: Epoch[013/030] Iteration[330/501] Loss: 8.7844\n",
      "Training: Epoch[013/030] Iteration[340/501] Loss: 9.4349\n",
      "Training: Epoch[013/030] Iteration[350/501] Loss: 9.0296\n",
      "Training: Epoch[013/030] Iteration[360/501] Loss: 8.3923\n",
      "Training: Epoch[013/030] Iteration[370/501] Loss: 8.9592\n",
      "Training: Epoch[013/030] Iteration[380/501] Loss: 7.6565\n",
      "Training: Epoch[013/030] Iteration[390/501] Loss: 8.9326\n",
      "Training: Epoch[013/030] Iteration[400/501] Loss: 7.5388\n",
      "Training: Epoch[013/030] Iteration[410/501] Loss: 8.5298\n",
      "Training: Epoch[013/030] Iteration[420/501] Loss: 7.4407\n",
      "Training: Epoch[013/030] Iteration[430/501] Loss: 6.0997\n",
      "Training: Epoch[013/030] Iteration[440/501] Loss: 7.1418\n",
      "Training: Epoch[013/030] Iteration[450/501] Loss: 6.6095\n",
      "Training: Epoch[013/030] Iteration[460/501] Loss: 8.6329\n",
      "Training: Epoch[013/030] Iteration[470/501] Loss: 6.6403\n",
      "Training: Epoch[013/030] Iteration[480/501] Loss: 6.1118\n",
      "Training: Epoch[013/030] Iteration[490/501] Loss: 6.1372\n",
      "Training: Epoch[013/030] Iteration[500/501] Loss: 6.6689\n",
      "Valid: Epoch[013/030] Iteration[501/501] Loss: 9.4779\n",
      "Training: Epoch[014/030] Iteration[010/501] Loss: 7.3150\n",
      "Training: Epoch[014/030] Iteration[020/501] Loss: 7.8096\n",
      "Training: Epoch[014/030] Iteration[030/501] Loss: 7.7183\n",
      "Training: Epoch[014/030] Iteration[040/501] Loss: 7.8780\n",
      "Training: Epoch[014/030] Iteration[050/501] Loss: 9.8022\n",
      "Training: Epoch[014/030] Iteration[060/501] Loss: 8.6922\n",
      "Training: Epoch[014/030] Iteration[070/501] Loss: 7.5897\n",
      "Training: Epoch[014/030] Iteration[080/501] Loss: 6.5335\n",
      "Training: Epoch[014/030] Iteration[090/501] Loss: 4.6820\n",
      "Training: Epoch[014/030] Iteration[100/501] Loss: 7.3948\n",
      "Training: Epoch[014/030] Iteration[110/501] Loss: 7.6653\n",
      "Training: Epoch[014/030] Iteration[120/501] Loss: 8.0312\n",
      "Training: Epoch[014/030] Iteration[130/501] Loss: 7.9016\n",
      "Training: Epoch[014/030] Iteration[140/501] Loss: 7.6830\n",
      "Training: Epoch[014/030] Iteration[150/501] Loss: 7.5366\n",
      "Training: Epoch[014/030] Iteration[160/501] Loss: 6.8445\n",
      "Training: Epoch[014/030] Iteration[170/501] Loss: 7.1415\n",
      "Training: Epoch[014/030] Iteration[180/501] Loss: 8.1174\n",
      "Training: Epoch[014/030] Iteration[190/501] Loss: 6.4188\n",
      "Training: Epoch[014/030] Iteration[200/501] Loss: 7.4646\n",
      "Training: Epoch[014/030] Iteration[210/501] Loss: 7.8544\n",
      "Training: Epoch[014/030] Iteration[220/501] Loss: 7.0891\n",
      "Training: Epoch[014/030] Iteration[230/501] Loss: 8.0588\n",
      "Training: Epoch[014/030] Iteration[240/501] Loss: 6.8542\n",
      "Training: Epoch[014/030] Iteration[250/501] Loss: 10.0057\n",
      "Training: Epoch[014/030] Iteration[260/501] Loss: 7.0663\n",
      "Training: Epoch[014/030] Iteration[270/501] Loss: 9.6889\n",
      "Training: Epoch[014/030] Iteration[280/501] Loss: 8.6121\n",
      "Training: Epoch[014/030] Iteration[290/501] Loss: 8.7895\n",
      "Training: Epoch[014/030] Iteration[300/501] Loss: 7.6534\n",
      "Training: Epoch[014/030] Iteration[310/501] Loss: 10.2198\n",
      "Training: Epoch[014/030] Iteration[320/501] Loss: 7.0076\n",
      "Training: Epoch[014/030] Iteration[330/501] Loss: 5.9610\n",
      "Training: Epoch[014/030] Iteration[340/501] Loss: 7.8197\n",
      "Training: Epoch[014/030] Iteration[350/501] Loss: 5.4383\n",
      "Training: Epoch[014/030] Iteration[360/501] Loss: 8.3270\n",
      "Training: Epoch[014/030] Iteration[370/501] Loss: 7.9355\n",
      "Training: Epoch[014/030] Iteration[380/501] Loss: 11.5442\n",
      "Training: Epoch[014/030] Iteration[390/501] Loss: 5.8853\n",
      "Training: Epoch[014/030] Iteration[400/501] Loss: 8.2068\n",
      "Training: Epoch[014/030] Iteration[410/501] Loss: 7.7821\n",
      "Training: Epoch[014/030] Iteration[420/501] Loss: 8.9307\n",
      "Training: Epoch[014/030] Iteration[430/501] Loss: 9.9601\n",
      "Training: Epoch[014/030] Iteration[440/501] Loss: 9.1217\n",
      "Training: Epoch[014/030] Iteration[450/501] Loss: 8.1754\n",
      "Training: Epoch[014/030] Iteration[460/501] Loss: 6.6293\n",
      "Training: Epoch[014/030] Iteration[470/501] Loss: 8.8054\n",
      "Training: Epoch[014/030] Iteration[480/501] Loss: 7.5798\n",
      "Training: Epoch[014/030] Iteration[490/501] Loss: 6.8821\n",
      "Training: Epoch[014/030] Iteration[500/501] Loss: 8.0105\n",
      "Valid: Epoch[014/030] Iteration[501/501] Loss: 10.2226\n",
      "Training: Epoch[015/030] Iteration[010/501] Loss: 6.0894\n",
      "Training: Epoch[015/030] Iteration[020/501] Loss: 7.9835\n",
      "Training: Epoch[015/030] Iteration[030/501] Loss: 8.9129\n",
      "Training: Epoch[015/030] Iteration[040/501] Loss: 8.6895\n",
      "Training: Epoch[015/030] Iteration[050/501] Loss: 6.1796\n",
      "Training: Epoch[015/030] Iteration[060/501] Loss: 9.7817\n",
      "Training: Epoch[015/030] Iteration[070/501] Loss: 8.0892\n",
      "Training: Epoch[015/030] Iteration[080/501] Loss: 5.9772\n",
      "Training: Epoch[015/030] Iteration[090/501] Loss: 7.6701\n",
      "Training: Epoch[015/030] Iteration[100/501] Loss: 5.8033\n",
      "Training: Epoch[015/030] Iteration[110/501] Loss: 5.8514\n",
      "Training: Epoch[015/030] Iteration[120/501] Loss: 10.1690\n",
      "Training: Epoch[015/030] Iteration[130/501] Loss: 6.2719\n",
      "Training: Epoch[015/030] Iteration[140/501] Loss: 8.0149\n",
      "Training: Epoch[015/030] Iteration[150/501] Loss: 7.4668\n",
      "Training: Epoch[015/030] Iteration[160/501] Loss: 9.0426\n",
      "Training: Epoch[015/030] Iteration[170/501] Loss: 8.4907\n",
      "Training: Epoch[015/030] Iteration[180/501] Loss: 6.7751\n",
      "Training: Epoch[015/030] Iteration[190/501] Loss: 6.8240\n",
      "Training: Epoch[015/030] Iteration[200/501] Loss: 7.8859\n",
      "Training: Epoch[015/030] Iteration[210/501] Loss: 7.2002\n",
      "Training: Epoch[015/030] Iteration[220/501] Loss: 8.5469\n",
      "Training: Epoch[015/030] Iteration[230/501] Loss: 6.6930\n",
      "Training: Epoch[015/030] Iteration[240/501] Loss: 7.4703\n",
      "Training: Epoch[015/030] Iteration[250/501] Loss: 7.5846\n",
      "Training: Epoch[015/030] Iteration[260/501] Loss: 8.0987\n",
      "Training: Epoch[015/030] Iteration[270/501] Loss: 7.0959\n",
      "Training: Epoch[015/030] Iteration[280/501] Loss: 7.0753\n",
      "Training: Epoch[015/030] Iteration[290/501] Loss: 8.7058\n",
      "Training: Epoch[015/030] Iteration[300/501] Loss: 5.9526\n",
      "Training: Epoch[015/030] Iteration[310/501] Loss: 7.5815\n",
      "Training: Epoch[015/030] Iteration[320/501] Loss: 9.9309\n",
      "Training: Epoch[015/030] Iteration[330/501] Loss: 7.5076\n",
      "Training: Epoch[015/030] Iteration[340/501] Loss: 7.9350\n",
      "Training: Epoch[015/030] Iteration[350/501] Loss: 8.0885\n",
      "Training: Epoch[015/030] Iteration[360/501] Loss: 7.5567\n",
      "Training: Epoch[015/030] Iteration[370/501] Loss: 8.6902\n",
      "Training: Epoch[015/030] Iteration[380/501] Loss: 7.3587\n",
      "Training: Epoch[015/030] Iteration[390/501] Loss: 7.7668\n",
      "Training: Epoch[015/030] Iteration[400/501] Loss: 8.4908\n",
      "Training: Epoch[015/030] Iteration[410/501] Loss: 7.9866\n",
      "Training: Epoch[015/030] Iteration[420/501] Loss: 7.6063\n",
      "Training: Epoch[015/030] Iteration[430/501] Loss: 7.4227\n",
      "Training: Epoch[015/030] Iteration[440/501] Loss: 7.6773\n",
      "Training: Epoch[015/030] Iteration[450/501] Loss: 6.3158\n",
      "Training: Epoch[015/030] Iteration[460/501] Loss: 9.6850\n",
      "Training: Epoch[015/030] Iteration[470/501] Loss: 9.2277\n",
      "Training: Epoch[015/030] Iteration[480/501] Loss: 8.6970\n",
      "Training: Epoch[015/030] Iteration[490/501] Loss: 8.0198\n",
      "Training: Epoch[015/030] Iteration[500/501] Loss: 7.6685\n",
      "Valid: Epoch[015/030] Iteration[501/501] Loss: 11.9168\n",
      "Training: Epoch[016/030] Iteration[010/501] Loss: 10.3175\n",
      "Training: Epoch[016/030] Iteration[020/501] Loss: 9.2637\n",
      "Training: Epoch[016/030] Iteration[030/501] Loss: 7.1965\n",
      "Training: Epoch[016/030] Iteration[040/501] Loss: 6.8702\n",
      "Training: Epoch[016/030] Iteration[050/501] Loss: 4.5303\n",
      "Training: Epoch[016/030] Iteration[060/501] Loss: 8.1527\n",
      "Training: Epoch[016/030] Iteration[070/501] Loss: 8.7175\n",
      "Training: Epoch[016/030] Iteration[080/501] Loss: 9.6958\n",
      "Training: Epoch[016/030] Iteration[090/501] Loss: 8.5255\n",
      "Training: Epoch[016/030] Iteration[100/501] Loss: 8.0541\n",
      "Training: Epoch[016/030] Iteration[110/501] Loss: 7.0433\n",
      "Training: Epoch[016/030] Iteration[120/501] Loss: 5.4415\n",
      "Training: Epoch[016/030] Iteration[130/501] Loss: 6.8090\n",
      "Training: Epoch[016/030] Iteration[140/501] Loss: 7.2759\n",
      "Training: Epoch[016/030] Iteration[150/501] Loss: 9.2457\n",
      "Training: Epoch[016/030] Iteration[160/501] Loss: 7.3609\n",
      "Training: Epoch[016/030] Iteration[170/501] Loss: 6.1129\n",
      "Training: Epoch[016/030] Iteration[180/501] Loss: 6.4512\n",
      "Training: Epoch[016/030] Iteration[190/501] Loss: 7.2628\n",
      "Training: Epoch[016/030] Iteration[200/501] Loss: 7.5793\n",
      "Training: Epoch[016/030] Iteration[210/501] Loss: 9.5698\n",
      "Training: Epoch[016/030] Iteration[220/501] Loss: 7.2929\n",
      "Training: Epoch[016/030] Iteration[230/501] Loss: 7.0266\n",
      "Training: Epoch[016/030] Iteration[240/501] Loss: 9.0072\n",
      "Training: Epoch[016/030] Iteration[250/501] Loss: 8.7981\n",
      "Training: Epoch[016/030] Iteration[260/501] Loss: 8.4701\n",
      "Training: Epoch[016/030] Iteration[270/501] Loss: 6.9721\n",
      "Training: Epoch[016/030] Iteration[280/501] Loss: 6.8251\n",
      "Training: Epoch[016/030] Iteration[290/501] Loss: 7.3563\n",
      "Training: Epoch[016/030] Iteration[300/501] Loss: 9.1248\n",
      "Training: Epoch[016/030] Iteration[310/501] Loss: 9.9304\n",
      "Training: Epoch[016/030] Iteration[320/501] Loss: 8.9782\n",
      "Training: Epoch[016/030] Iteration[330/501] Loss: 8.6475\n",
      "Training: Epoch[016/030] Iteration[340/501] Loss: 7.1919\n",
      "Training: Epoch[016/030] Iteration[350/501] Loss: 5.3311\n",
      "Training: Epoch[016/030] Iteration[360/501] Loss: 6.5748\n",
      "Training: Epoch[016/030] Iteration[370/501] Loss: 7.1914\n",
      "Training: Epoch[016/030] Iteration[380/501] Loss: 6.8842\n",
      "Training: Epoch[016/030] Iteration[390/501] Loss: 9.0449\n",
      "Training: Epoch[016/030] Iteration[400/501] Loss: 8.2189\n",
      "Training: Epoch[016/030] Iteration[410/501] Loss: 7.8150\n",
      "Training: Epoch[016/030] Iteration[420/501] Loss: 8.3100\n",
      "Training: Epoch[016/030] Iteration[430/501] Loss: 6.3441\n",
      "Training: Epoch[016/030] Iteration[440/501] Loss: 7.3829\n",
      "Training: Epoch[016/030] Iteration[450/501] Loss: 5.5616\n",
      "Training: Epoch[016/030] Iteration[460/501] Loss: 8.1412\n",
      "Training: Epoch[016/030] Iteration[470/501] Loss: 7.2628\n",
      "Training: Epoch[016/030] Iteration[480/501] Loss: 6.5992\n",
      "Training: Epoch[016/030] Iteration[490/501] Loss: 7.8772\n",
      "Training: Epoch[016/030] Iteration[500/501] Loss: 10.5478\n",
      "Valid: Epoch[016/030] Iteration[501/501] Loss: 9.4590\n",
      "Training: Epoch[017/030] Iteration[010/501] Loss: 6.5566\n",
      "Training: Epoch[017/030] Iteration[020/501] Loss: 7.6831\n",
      "Training: Epoch[017/030] Iteration[030/501] Loss: 6.1593\n",
      "Training: Epoch[017/030] Iteration[040/501] Loss: 8.2972\n",
      "Training: Epoch[017/030] Iteration[050/501] Loss: 7.9926\n",
      "Training: Epoch[017/030] Iteration[060/501] Loss: 8.8735\n",
      "Training: Epoch[017/030] Iteration[070/501] Loss: 6.5281\n",
      "Training: Epoch[017/030] Iteration[080/501] Loss: 8.0634\n",
      "Training: Epoch[017/030] Iteration[090/501] Loss: 6.0999\n",
      "Training: Epoch[017/030] Iteration[100/501] Loss: 7.3840\n",
      "Training: Epoch[017/030] Iteration[110/501] Loss: 7.8813\n",
      "Training: Epoch[017/030] Iteration[120/501] Loss: 7.6723\n",
      "Training: Epoch[017/030] Iteration[130/501] Loss: 7.8713\n",
      "Training: Epoch[017/030] Iteration[140/501] Loss: 7.4767\n",
      "Training: Epoch[017/030] Iteration[150/501] Loss: 6.8576\n",
      "Training: Epoch[017/030] Iteration[160/501] Loss: 10.3593\n",
      "Training: Epoch[017/030] Iteration[170/501] Loss: 7.5344\n",
      "Training: Epoch[017/030] Iteration[180/501] Loss: 8.6306\n",
      "Training: Epoch[017/030] Iteration[190/501] Loss: 8.8234\n",
      "Training: Epoch[017/030] Iteration[200/501] Loss: 9.3347\n",
      "Training: Epoch[017/030] Iteration[210/501] Loss: 7.2479\n",
      "Training: Epoch[017/030] Iteration[220/501] Loss: 8.7851\n",
      "Training: Epoch[017/030] Iteration[230/501] Loss: 9.4761\n",
      "Training: Epoch[017/030] Iteration[240/501] Loss: 5.4303\n",
      "Training: Epoch[017/030] Iteration[250/501] Loss: 11.0532\n",
      "Training: Epoch[017/030] Iteration[260/501] Loss: 6.9093\n",
      "Training: Epoch[017/030] Iteration[270/501] Loss: 5.6336\n",
      "Training: Epoch[017/030] Iteration[280/501] Loss: 8.1780\n",
      "Training: Epoch[017/030] Iteration[290/501] Loss: 6.6822\n",
      "Training: Epoch[017/030] Iteration[300/501] Loss: 8.5946\n",
      "Training: Epoch[017/030] Iteration[310/501] Loss: 8.2879\n",
      "Training: Epoch[017/030] Iteration[320/501] Loss: 7.2124\n",
      "Training: Epoch[017/030] Iteration[330/501] Loss: 10.7082\n",
      "Training: Epoch[017/030] Iteration[340/501] Loss: 9.1122\n",
      "Training: Epoch[017/030] Iteration[350/501] Loss: 8.4581\n",
      "Training: Epoch[017/030] Iteration[360/501] Loss: 6.6812\n",
      "Training: Epoch[017/030] Iteration[370/501] Loss: 6.5139\n",
      "Training: Epoch[017/030] Iteration[380/501] Loss: 7.0822\n",
      "Training: Epoch[017/030] Iteration[390/501] Loss: 10.3697\n",
      "Training: Epoch[017/030] Iteration[400/501] Loss: 7.6827\n",
      "Training: Epoch[017/030] Iteration[410/501] Loss: 7.2861\n",
      "Training: Epoch[017/030] Iteration[420/501] Loss: 7.6939\n",
      "Training: Epoch[017/030] Iteration[430/501] Loss: 6.1507\n",
      "Training: Epoch[017/030] Iteration[440/501] Loss: 5.8414\n",
      "Training: Epoch[017/030] Iteration[450/501] Loss: 6.7335\n",
      "Training: Epoch[017/030] Iteration[460/501] Loss: 6.8770\n",
      "Training: Epoch[017/030] Iteration[470/501] Loss: 5.7521\n",
      "Training: Epoch[017/030] Iteration[480/501] Loss: 7.0013\n",
      "Training: Epoch[017/030] Iteration[490/501] Loss: 6.1925\n",
      "Training: Epoch[017/030] Iteration[500/501] Loss: 8.8859\n",
      "Valid: Epoch[017/030] Iteration[501/501] Loss: 9.5655\n",
      "Training: Epoch[018/030] Iteration[010/501] Loss: 8.2133\n",
      "Training: Epoch[018/030] Iteration[020/501] Loss: 7.6555\n",
      "Training: Epoch[018/030] Iteration[030/501] Loss: 6.5337\n",
      "Training: Epoch[018/030] Iteration[040/501] Loss: 7.7566\n",
      "Training: Epoch[018/030] Iteration[050/501] Loss: 5.6691\n",
      "Training: Epoch[018/030] Iteration[060/501] Loss: 9.4490\n",
      "Training: Epoch[018/030] Iteration[070/501] Loss: 7.4429\n",
      "Training: Epoch[018/030] Iteration[080/501] Loss: 8.2775\n",
      "Training: Epoch[018/030] Iteration[090/501] Loss: 8.0052\n",
      "Training: Epoch[018/030] Iteration[100/501] Loss: 6.0238\n",
      "Training: Epoch[018/030] Iteration[110/501] Loss: 8.3929\n",
      "Training: Epoch[018/030] Iteration[120/501] Loss: 7.6436\n",
      "Training: Epoch[018/030] Iteration[130/501] Loss: 7.9884\n",
      "Training: Epoch[018/030] Iteration[140/501] Loss: 8.5930\n",
      "Training: Epoch[018/030] Iteration[150/501] Loss: 7.3545\n",
      "Training: Epoch[018/030] Iteration[160/501] Loss: 8.2450\n",
      "Training: Epoch[018/030] Iteration[170/501] Loss: 4.9616\n",
      "Training: Epoch[018/030] Iteration[180/501] Loss: 6.6182\n",
      "Training: Epoch[018/030] Iteration[190/501] Loss: 9.5405\n",
      "Training: Epoch[018/030] Iteration[200/501] Loss: 10.2901\n",
      "Training: Epoch[018/030] Iteration[210/501] Loss: 6.9208\n",
      "Training: Epoch[018/030] Iteration[220/501] Loss: 9.4712\n",
      "Training: Epoch[018/030] Iteration[230/501] Loss: 8.0171\n",
      "Training: Epoch[018/030] Iteration[240/501] Loss: 7.7532\n",
      "Training: Epoch[018/030] Iteration[250/501] Loss: 7.6431\n",
      "Training: Epoch[018/030] Iteration[260/501] Loss: 7.1808\n",
      "Training: Epoch[018/030] Iteration[270/501] Loss: 8.7666\n",
      "Training: Epoch[018/030] Iteration[280/501] Loss: 8.9219\n",
      "Training: Epoch[018/030] Iteration[290/501] Loss: 6.3591\n",
      "Training: Epoch[018/030] Iteration[300/501] Loss: 7.1316\n",
      "Training: Epoch[018/030] Iteration[310/501] Loss: 8.1608\n",
      "Training: Epoch[018/030] Iteration[320/501] Loss: 6.7156\n",
      "Training: Epoch[018/030] Iteration[330/501] Loss: 5.8727\n",
      "Training: Epoch[018/030] Iteration[340/501] Loss: 8.5992\n",
      "Training: Epoch[018/030] Iteration[350/501] Loss: 10.7411\n",
      "Training: Epoch[018/030] Iteration[360/501] Loss: 8.4787\n",
      "Training: Epoch[018/030] Iteration[370/501] Loss: 5.5716\n",
      "Training: Epoch[018/030] Iteration[380/501] Loss: 9.1868\n",
      "Training: Epoch[018/030] Iteration[390/501] Loss: 7.1100\n",
      "Training: Epoch[018/030] Iteration[400/501] Loss: 5.4794\n",
      "Training: Epoch[018/030] Iteration[410/501] Loss: 8.9406\n",
      "Training: Epoch[018/030] Iteration[420/501] Loss: 8.6212\n",
      "Training: Epoch[018/030] Iteration[430/501] Loss: 6.7875\n",
      "Training: Epoch[018/030] Iteration[440/501] Loss: 7.1981\n",
      "Training: Epoch[018/030] Iteration[450/501] Loss: 10.1574\n",
      "Training: Epoch[018/030] Iteration[460/501] Loss: 4.2913\n",
      "Training: Epoch[018/030] Iteration[470/501] Loss: 6.8161\n",
      "Training: Epoch[018/030] Iteration[480/501] Loss: 7.7802\n",
      "Training: Epoch[018/030] Iteration[490/501] Loss: 6.4334\n",
      "Training: Epoch[018/030] Iteration[500/501] Loss: 7.8544\n",
      "Valid: Epoch[018/030] Iteration[501/501] Loss: 10.2025\n",
      "Training: Epoch[019/030] Iteration[010/501] Loss: 7.4707\n",
      "Training: Epoch[019/030] Iteration[020/501] Loss: 8.2412\n",
      "Training: Epoch[019/030] Iteration[030/501] Loss: 6.7880\n",
      "Training: Epoch[019/030] Iteration[040/501] Loss: 6.9790\n",
      "Training: Epoch[019/030] Iteration[050/501] Loss: 9.6526\n",
      "Training: Epoch[019/030] Iteration[060/501] Loss: 6.3418\n",
      "Training: Epoch[019/030] Iteration[070/501] Loss: 5.6037\n",
      "Training: Epoch[019/030] Iteration[080/501] Loss: 5.7226\n",
      "Training: Epoch[019/030] Iteration[090/501] Loss: 7.8528\n",
      "Training: Epoch[019/030] Iteration[100/501] Loss: 7.3208\n",
      "Training: Epoch[019/030] Iteration[110/501] Loss: 6.8162\n",
      "Training: Epoch[019/030] Iteration[120/501] Loss: 9.0381\n",
      "Training: Epoch[019/030] Iteration[130/501] Loss: 5.9571\n",
      "Training: Epoch[019/030] Iteration[140/501] Loss: 7.4506\n",
      "Training: Epoch[019/030] Iteration[150/501] Loss: 6.3505\n",
      "Training: Epoch[019/030] Iteration[160/501] Loss: 5.3097\n",
      "Training: Epoch[019/030] Iteration[170/501] Loss: 8.2055\n",
      "Training: Epoch[019/030] Iteration[180/501] Loss: 11.1557\n",
      "Training: Epoch[019/030] Iteration[190/501] Loss: 7.7502\n",
      "Training: Epoch[019/030] Iteration[200/501] Loss: 7.2276\n",
      "Training: Epoch[019/030] Iteration[210/501] Loss: 6.4366\n",
      "Training: Epoch[019/030] Iteration[220/501] Loss: 8.5942\n",
      "Training: Epoch[019/030] Iteration[230/501] Loss: 6.5286\n",
      "Training: Epoch[019/030] Iteration[240/501] Loss: 7.1698\n",
      "Training: Epoch[019/030] Iteration[250/501] Loss: 7.2549\n",
      "Training: Epoch[019/030] Iteration[260/501] Loss: 8.8871\n",
      "Training: Epoch[019/030] Iteration[270/501] Loss: 5.8251\n",
      "Training: Epoch[019/030] Iteration[280/501] Loss: 8.8651\n",
      "Training: Epoch[019/030] Iteration[290/501] Loss: 7.4507\n",
      "Training: Epoch[019/030] Iteration[300/501] Loss: 7.4953\n",
      "Training: Epoch[019/030] Iteration[310/501] Loss: 6.2735\n",
      "Training: Epoch[019/030] Iteration[320/501] Loss: 9.4047\n",
      "Training: Epoch[019/030] Iteration[330/501] Loss: 7.3524\n",
      "Training: Epoch[019/030] Iteration[340/501] Loss: 7.1061\n",
      "Training: Epoch[019/030] Iteration[350/501] Loss: 7.0703\n",
      "Training: Epoch[019/030] Iteration[360/501] Loss: 8.5050\n",
      "Training: Epoch[019/030] Iteration[370/501] Loss: 8.4333\n",
      "Training: Epoch[019/030] Iteration[380/501] Loss: 10.5359\n",
      "Training: Epoch[019/030] Iteration[390/501] Loss: 7.0397\n",
      "Training: Epoch[019/030] Iteration[400/501] Loss: 6.7277\n",
      "Training: Epoch[019/030] Iteration[410/501] Loss: 8.4419\n",
      "Training: Epoch[019/030] Iteration[420/501] Loss: 7.7038\n",
      "Training: Epoch[019/030] Iteration[430/501] Loss: 7.7059\n",
      "Training: Epoch[019/030] Iteration[440/501] Loss: 7.1813\n",
      "Training: Epoch[019/030] Iteration[450/501] Loss: 7.4476\n",
      "Training: Epoch[019/030] Iteration[460/501] Loss: 8.8221\n",
      "Training: Epoch[019/030] Iteration[470/501] Loss: 6.5750\n",
      "Training: Epoch[019/030] Iteration[480/501] Loss: 8.3166\n",
      "Training: Epoch[019/030] Iteration[490/501] Loss: 10.3500\n",
      "Training: Epoch[019/030] Iteration[500/501] Loss: 12.0990\n",
      "Valid: Epoch[019/030] Iteration[501/501] Loss: 9.7649\n",
      "Training: Epoch[020/030] Iteration[010/501] Loss: 9.1655\n",
      "Training: Epoch[020/030] Iteration[020/501] Loss: 10.0665\n",
      "Training: Epoch[020/030] Iteration[030/501] Loss: 6.4750\n",
      "Training: Epoch[020/030] Iteration[040/501] Loss: 8.4277\n",
      "Training: Epoch[020/030] Iteration[050/501] Loss: 7.4796\n",
      "Training: Epoch[020/030] Iteration[060/501] Loss: 7.9326\n",
      "Training: Epoch[020/030] Iteration[070/501] Loss: 7.8989\n",
      "Training: Epoch[020/030] Iteration[080/501] Loss: 9.1062\n",
      "Training: Epoch[020/030] Iteration[090/501] Loss: 8.1031\n",
      "Training: Epoch[020/030] Iteration[100/501] Loss: 10.1547\n",
      "Training: Epoch[020/030] Iteration[110/501] Loss: 7.3494\n",
      "Training: Epoch[020/030] Iteration[120/501] Loss: 5.0562\n",
      "Training: Epoch[020/030] Iteration[130/501] Loss: 7.7125\n",
      "Training: Epoch[020/030] Iteration[140/501] Loss: 7.3096\n",
      "Training: Epoch[020/030] Iteration[150/501] Loss: 10.0556\n",
      "Training: Epoch[020/030] Iteration[160/501] Loss: 7.7806\n",
      "Training: Epoch[020/030] Iteration[170/501] Loss: 8.5715\n",
      "Training: Epoch[020/030] Iteration[180/501] Loss: 7.5481\n",
      "Training: Epoch[020/030] Iteration[190/501] Loss: 5.8646\n",
      "Training: Epoch[020/030] Iteration[200/501] Loss: 6.7757\n",
      "Training: Epoch[020/030] Iteration[210/501] Loss: 6.0954\n",
      "Training: Epoch[020/030] Iteration[220/501] Loss: 9.8190\n",
      "Training: Epoch[020/030] Iteration[230/501] Loss: 8.2087\n",
      "Training: Epoch[020/030] Iteration[240/501] Loss: 7.6920\n",
      "Training: Epoch[020/030] Iteration[250/501] Loss: 7.4864\n",
      "Training: Epoch[020/030] Iteration[260/501] Loss: 7.3975\n",
      "Training: Epoch[020/030] Iteration[270/501] Loss: 8.4640\n",
      "Training: Epoch[020/030] Iteration[280/501] Loss: 8.4487\n",
      "Training: Epoch[020/030] Iteration[290/501] Loss: 7.0640\n",
      "Training: Epoch[020/030] Iteration[300/501] Loss: 7.8882\n",
      "Training: Epoch[020/030] Iteration[310/501] Loss: 8.4696\n",
      "Training: Epoch[020/030] Iteration[320/501] Loss: 9.0651\n",
      "Training: Epoch[020/030] Iteration[330/501] Loss: 6.5034\n",
      "Training: Epoch[020/030] Iteration[340/501] Loss: 5.5016\n",
      "Training: Epoch[020/030] Iteration[350/501] Loss: 7.7169\n",
      "Training: Epoch[020/030] Iteration[360/501] Loss: 8.0790\n",
      "Training: Epoch[020/030] Iteration[370/501] Loss: 7.6017\n",
      "Training: Epoch[020/030] Iteration[380/501] Loss: 5.7237\n",
      "Training: Epoch[020/030] Iteration[390/501] Loss: 6.8694\n",
      "Training: Epoch[020/030] Iteration[400/501] Loss: 7.4027\n",
      "Training: Epoch[020/030] Iteration[410/501] Loss: 5.9687\n",
      "Training: Epoch[020/030] Iteration[420/501] Loss: 7.1412\n",
      "Training: Epoch[020/030] Iteration[430/501] Loss: 5.2912\n",
      "Training: Epoch[020/030] Iteration[440/501] Loss: 6.5077\n",
      "Training: Epoch[020/030] Iteration[450/501] Loss: 8.7434\n",
      "Training: Epoch[020/030] Iteration[460/501] Loss: 7.1633\n",
      "Training: Epoch[020/030] Iteration[470/501] Loss: 8.2194\n",
      "Training: Epoch[020/030] Iteration[480/501] Loss: 9.7637\n",
      "Training: Epoch[020/030] Iteration[490/501] Loss: 6.8041\n",
      "Training: Epoch[020/030] Iteration[500/501] Loss: 9.2547\n",
      "Valid: Epoch[020/030] Iteration[501/501] Loss: 9.6240\n",
      "Training: Epoch[021/030] Iteration[010/501] Loss: 10.3666\n",
      "Training: Epoch[021/030] Iteration[020/501] Loss: 8.3892\n",
      "Training: Epoch[021/030] Iteration[030/501] Loss: 7.3315\n",
      "Training: Epoch[021/030] Iteration[040/501] Loss: 8.4414\n",
      "Training: Epoch[021/030] Iteration[050/501] Loss: 7.2835\n",
      "Training: Epoch[021/030] Iteration[060/501] Loss: 7.2091\n",
      "Training: Epoch[021/030] Iteration[070/501] Loss: 6.9214\n",
      "Training: Epoch[021/030] Iteration[080/501] Loss: 6.4761\n",
      "Training: Epoch[021/030] Iteration[090/501] Loss: 8.0357\n",
      "Training: Epoch[021/030] Iteration[100/501] Loss: 8.8663\n",
      "Training: Epoch[021/030] Iteration[110/501] Loss: 6.3742\n",
      "Training: Epoch[021/030] Iteration[120/501] Loss: 6.4327\n",
      "Training: Epoch[021/030] Iteration[130/501] Loss: 7.5079\n",
      "Training: Epoch[021/030] Iteration[140/501] Loss: 7.6339\n",
      "Training: Epoch[021/030] Iteration[150/501] Loss: 7.3801\n",
      "Training: Epoch[021/030] Iteration[160/501] Loss: 7.6155\n",
      "Training: Epoch[021/030] Iteration[170/501] Loss: 8.1623\n",
      "Training: Epoch[021/030] Iteration[180/501] Loss: 7.0447\n",
      "Training: Epoch[021/030] Iteration[190/501] Loss: 8.0931\n",
      "Training: Epoch[021/030] Iteration[200/501] Loss: 5.2289\n",
      "Training: Epoch[021/030] Iteration[210/501] Loss: 6.3110\n",
      "Training: Epoch[021/030] Iteration[220/501] Loss: 7.3077\n",
      "Training: Epoch[021/030] Iteration[230/501] Loss: 10.6745\n",
      "Training: Epoch[021/030] Iteration[240/501] Loss: 7.7271\n",
      "Training: Epoch[021/030] Iteration[250/501] Loss: 6.2238\n",
      "Training: Epoch[021/030] Iteration[260/501] Loss: 7.9608\n",
      "Training: Epoch[021/030] Iteration[270/501] Loss: 6.7538\n",
      "Training: Epoch[021/030] Iteration[280/501] Loss: 7.7525\n",
      "Training: Epoch[021/030] Iteration[290/501] Loss: 8.3098\n",
      "Training: Epoch[021/030] Iteration[300/501] Loss: 9.3876\n",
      "Training: Epoch[021/030] Iteration[310/501] Loss: 10.1876\n",
      "Training: Epoch[021/030] Iteration[320/501] Loss: 8.0488\n",
      "Training: Epoch[021/030] Iteration[330/501] Loss: 8.2139\n",
      "Training: Epoch[021/030] Iteration[340/501] Loss: 8.9217\n",
      "Training: Epoch[021/030] Iteration[350/501] Loss: 9.1831\n",
      "Training: Epoch[021/030] Iteration[360/501] Loss: 7.1387\n",
      "Training: Epoch[021/030] Iteration[370/501] Loss: 6.8757\n",
      "Training: Epoch[021/030] Iteration[380/501] Loss: 7.4267\n",
      "Training: Epoch[021/030] Iteration[390/501] Loss: 5.7684\n",
      "Training: Epoch[021/030] Iteration[400/501] Loss: 7.2957\n",
      "Training: Epoch[021/030] Iteration[410/501] Loss: 11.3223\n",
      "Training: Epoch[021/030] Iteration[420/501] Loss: 7.2950\n",
      "Training: Epoch[021/030] Iteration[430/501] Loss: 6.1344\n",
      "Training: Epoch[021/030] Iteration[440/501] Loss: 7.8800\n",
      "Training: Epoch[021/030] Iteration[450/501] Loss: 8.7812\n",
      "Training: Epoch[021/030] Iteration[460/501] Loss: 7.1001\n",
      "Training: Epoch[021/030] Iteration[470/501] Loss: 6.5947\n",
      "Training: Epoch[021/030] Iteration[480/501] Loss: 8.3968\n",
      "Training: Epoch[021/030] Iteration[490/501] Loss: 5.6365\n",
      "Training: Epoch[021/030] Iteration[500/501] Loss: 5.6855\n",
      "Valid: Epoch[021/030] Iteration[501/501] Loss: 9.7281\n",
      "Training: Epoch[022/030] Iteration[010/501] Loss: 9.2234\n",
      "Training: Epoch[022/030] Iteration[020/501] Loss: 6.6685\n",
      "Training: Epoch[022/030] Iteration[030/501] Loss: 6.7085\n",
      "Training: Epoch[022/030] Iteration[040/501] Loss: 7.2337\n",
      "Training: Epoch[022/030] Iteration[050/501] Loss: 9.7686\n",
      "Training: Epoch[022/030] Iteration[060/501] Loss: 6.1843\n",
      "Training: Epoch[022/030] Iteration[070/501] Loss: 7.7549\n",
      "Training: Epoch[022/030] Iteration[080/501] Loss: 9.3728\n",
      "Training: Epoch[022/030] Iteration[090/501] Loss: 9.5139\n",
      "Training: Epoch[022/030] Iteration[100/501] Loss: 7.4450\n",
      "Training: Epoch[022/030] Iteration[110/501] Loss: 6.3121\n",
      "Training: Epoch[022/030] Iteration[120/501] Loss: 8.6670\n",
      "Training: Epoch[022/030] Iteration[130/501] Loss: 6.5521\n",
      "Training: Epoch[022/030] Iteration[140/501] Loss: 7.0026\n",
      "Training: Epoch[022/030] Iteration[150/501] Loss: 6.7575\n",
      "Training: Epoch[022/030] Iteration[160/501] Loss: 7.6823\n",
      "Training: Epoch[022/030] Iteration[170/501] Loss: 6.5354\n",
      "Training: Epoch[022/030] Iteration[180/501] Loss: 8.8720\n",
      "Training: Epoch[022/030] Iteration[190/501] Loss: 9.0432\n",
      "Training: Epoch[022/030] Iteration[200/501] Loss: 8.3232\n",
      "Training: Epoch[022/030] Iteration[210/501] Loss: 9.4069\n",
      "Training: Epoch[022/030] Iteration[220/501] Loss: 6.0520\n",
      "Training: Epoch[022/030] Iteration[230/501] Loss: 7.8805\n",
      "Training: Epoch[022/030] Iteration[240/501] Loss: 8.2470\n",
      "Training: Epoch[022/030] Iteration[250/501] Loss: 5.8703\n",
      "Training: Epoch[022/030] Iteration[260/501] Loss: 7.3181\n",
      "Training: Epoch[022/030] Iteration[270/501] Loss: 7.7915\n",
      "Training: Epoch[022/030] Iteration[280/501] Loss: 7.3574\n",
      "Training: Epoch[022/030] Iteration[290/501] Loss: 6.9185\n",
      "Training: Epoch[022/030] Iteration[300/501] Loss: 9.6662\n",
      "Training: Epoch[022/030] Iteration[310/501] Loss: 8.4031\n",
      "Training: Epoch[022/030] Iteration[320/501] Loss: 8.4452\n",
      "Training: Epoch[022/030] Iteration[330/501] Loss: 7.9690\n",
      "Training: Epoch[022/030] Iteration[340/501] Loss: 8.0354\n",
      "Training: Epoch[022/030] Iteration[350/501] Loss: 8.2260\n",
      "Training: Epoch[022/030] Iteration[360/501] Loss: 6.9883\n",
      "Training: Epoch[022/030] Iteration[370/501] Loss: 6.9330\n",
      "Training: Epoch[022/030] Iteration[380/501] Loss: 6.7729\n",
      "Training: Epoch[022/030] Iteration[390/501] Loss: 7.2433\n",
      "Training: Epoch[022/030] Iteration[400/501] Loss: 7.6306\n",
      "Training: Epoch[022/030] Iteration[410/501] Loss: 5.6584\n",
      "Training: Epoch[022/030] Iteration[420/501] Loss: 7.1612\n",
      "Training: Epoch[022/030] Iteration[430/501] Loss: 8.5918\n",
      "Training: Epoch[022/030] Iteration[440/501] Loss: 8.8581\n",
      "Training: Epoch[022/030] Iteration[450/501] Loss: 5.8792\n",
      "Training: Epoch[022/030] Iteration[460/501] Loss: 6.8408\n",
      "Training: Epoch[022/030] Iteration[470/501] Loss: 7.4535\n",
      "Training: Epoch[022/030] Iteration[480/501] Loss: 6.7773\n",
      "Training: Epoch[022/030] Iteration[490/501] Loss: 8.6275\n",
      "Training: Epoch[022/030] Iteration[500/501] Loss: 7.6531\n",
      "Valid: Epoch[022/030] Iteration[501/501] Loss: 9.4481\n",
      "Training: Epoch[023/030] Iteration[010/501] Loss: 6.0281\n",
      "Training: Epoch[023/030] Iteration[020/501] Loss: 8.2373\n",
      "Training: Epoch[023/030] Iteration[030/501] Loss: 5.9993\n",
      "Training: Epoch[023/030] Iteration[040/501] Loss: 7.2635\n",
      "Training: Epoch[023/030] Iteration[050/501] Loss: 8.2702\n",
      "Training: Epoch[023/030] Iteration[060/501] Loss: 9.0892\n",
      "Training: Epoch[023/030] Iteration[070/501] Loss: 7.7746\n",
      "Training: Epoch[023/030] Iteration[080/501] Loss: 7.5784\n",
      "Training: Epoch[023/030] Iteration[090/501] Loss: 7.5601\n",
      "Training: Epoch[023/030] Iteration[100/501] Loss: 10.6733\n",
      "Training: Epoch[023/030] Iteration[110/501] Loss: 8.2907\n",
      "Training: Epoch[023/030] Iteration[120/501] Loss: 9.2473\n",
      "Training: Epoch[023/030] Iteration[130/501] Loss: 7.2578\n",
      "Training: Epoch[023/030] Iteration[140/501] Loss: 6.5281\n",
      "Training: Epoch[023/030] Iteration[150/501] Loss: 7.8006\n",
      "Training: Epoch[023/030] Iteration[160/501] Loss: 9.5780\n",
      "Training: Epoch[023/030] Iteration[170/501] Loss: 5.5679\n",
      "Training: Epoch[023/030] Iteration[180/501] Loss: 7.9405\n",
      "Training: Epoch[023/030] Iteration[190/501] Loss: 7.6679\n",
      "Training: Epoch[023/030] Iteration[200/501] Loss: 7.0745\n",
      "Training: Epoch[023/030] Iteration[210/501] Loss: 6.5961\n",
      "Training: Epoch[023/030] Iteration[220/501] Loss: 10.3991\n",
      "Training: Epoch[023/030] Iteration[230/501] Loss: 6.3366\n",
      "Training: Epoch[023/030] Iteration[240/501] Loss: 9.6999\n",
      "Training: Epoch[023/030] Iteration[250/501] Loss: 6.8766\n",
      "Training: Epoch[023/030] Iteration[260/501] Loss: 7.7421\n",
      "Training: Epoch[023/030] Iteration[270/501] Loss: 6.5402\n",
      "Training: Epoch[023/030] Iteration[280/501] Loss: 6.1200\n",
      "Training: Epoch[023/030] Iteration[290/501] Loss: 10.9644\n",
      "Training: Epoch[023/030] Iteration[300/501] Loss: 6.4182\n",
      "Training: Epoch[023/030] Iteration[310/501] Loss: 7.6799\n",
      "Training: Epoch[023/030] Iteration[320/501] Loss: 8.3264\n",
      "Training: Epoch[023/030] Iteration[330/501] Loss: 8.3343\n",
      "Training: Epoch[023/030] Iteration[340/501] Loss: 8.2509\n",
      "Training: Epoch[023/030] Iteration[350/501] Loss: 6.7948\n",
      "Training: Epoch[023/030] Iteration[360/501] Loss: 6.9637\n",
      "Training: Epoch[023/030] Iteration[370/501] Loss: 5.9628\n",
      "Training: Epoch[023/030] Iteration[380/501] Loss: 7.9600\n",
      "Training: Epoch[023/030] Iteration[390/501] Loss: 9.5470\n",
      "Training: Epoch[023/030] Iteration[400/501] Loss: 6.7502\n",
      "Training: Epoch[023/030] Iteration[410/501] Loss: 7.3422\n",
      "Training: Epoch[023/030] Iteration[420/501] Loss: 8.4107\n",
      "Training: Epoch[023/030] Iteration[430/501] Loss: 7.3693\n",
      "Training: Epoch[023/030] Iteration[440/501] Loss: 8.0176\n",
      "Training: Epoch[023/030] Iteration[450/501] Loss: 8.0810\n",
      "Training: Epoch[023/030] Iteration[460/501] Loss: 7.2190\n",
      "Training: Epoch[023/030] Iteration[470/501] Loss: 5.3931\n",
      "Training: Epoch[023/030] Iteration[480/501] Loss: 5.7685\n",
      "Training: Epoch[023/030] Iteration[490/501] Loss: 7.4994\n",
      "Training: Epoch[023/030] Iteration[500/501] Loss: 7.1951\n",
      "Valid: Epoch[023/030] Iteration[501/501] Loss: 9.5858\n",
      "Training: Epoch[024/030] Iteration[010/501] Loss: 8.7433\n",
      "Training: Epoch[024/030] Iteration[020/501] Loss: 10.4807\n",
      "Training: Epoch[024/030] Iteration[030/501] Loss: 5.5223\n",
      "Training: Epoch[024/030] Iteration[040/501] Loss: 6.9559\n",
      "Training: Epoch[024/030] Iteration[050/501] Loss: 6.6370\n",
      "Training: Epoch[024/030] Iteration[060/501] Loss: 6.5466\n",
      "Training: Epoch[024/030] Iteration[070/501] Loss: 6.1513\n",
      "Training: Epoch[024/030] Iteration[080/501] Loss: 6.4342\n",
      "Training: Epoch[024/030] Iteration[090/501] Loss: 6.3429\n",
      "Training: Epoch[024/030] Iteration[100/501] Loss: 7.3523\n",
      "Training: Epoch[024/030] Iteration[110/501] Loss: 9.6382\n",
      "Training: Epoch[024/030] Iteration[120/501] Loss: 8.2195\n",
      "Training: Epoch[024/030] Iteration[130/501] Loss: 9.5169\n",
      "Training: Epoch[024/030] Iteration[140/501] Loss: 5.9982\n",
      "Training: Epoch[024/030] Iteration[150/501] Loss: 8.2593\n",
      "Training: Epoch[024/030] Iteration[160/501] Loss: 6.0685\n",
      "Training: Epoch[024/030] Iteration[170/501] Loss: 9.6796\n",
      "Training: Epoch[024/030] Iteration[180/501] Loss: 9.0144\n",
      "Training: Epoch[024/030] Iteration[190/501] Loss: 8.6664\n",
      "Training: Epoch[024/030] Iteration[200/501] Loss: 9.6069\n",
      "Training: Epoch[024/030] Iteration[210/501] Loss: 7.0376\n",
      "Training: Epoch[024/030] Iteration[220/501] Loss: 7.9632\n",
      "Training: Epoch[024/030] Iteration[230/501] Loss: 7.5906\n",
      "Training: Epoch[024/030] Iteration[240/501] Loss: 6.4817\n",
      "Training: Epoch[024/030] Iteration[250/501] Loss: 8.1789\n",
      "Training: Epoch[024/030] Iteration[260/501] Loss: 7.8621\n",
      "Training: Epoch[024/030] Iteration[270/501] Loss: 6.9845\n",
      "Training: Epoch[024/030] Iteration[280/501] Loss: 10.1115\n",
      "Training: Epoch[024/030] Iteration[290/501] Loss: 7.9277\n",
      "Training: Epoch[024/030] Iteration[300/501] Loss: 8.3506\n",
      "Training: Epoch[024/030] Iteration[310/501] Loss: 8.3104\n",
      "Training: Epoch[024/030] Iteration[320/501] Loss: 7.5234\n",
      "Training: Epoch[024/030] Iteration[330/501] Loss: 7.5010\n",
      "Training: Epoch[024/030] Iteration[340/501] Loss: 8.3949\n",
      "Training: Epoch[024/030] Iteration[350/501] Loss: 6.3380\n",
      "Training: Epoch[024/030] Iteration[360/501] Loss: 7.2033\n",
      "Training: Epoch[024/030] Iteration[370/501] Loss: 7.0754\n",
      "Training: Epoch[024/030] Iteration[380/501] Loss: 10.2595\n",
      "Training: Epoch[024/030] Iteration[390/501] Loss: 8.7199\n",
      "Training: Epoch[024/030] Iteration[400/501] Loss: 5.7183\n",
      "Training: Epoch[024/030] Iteration[410/501] Loss: 6.2485\n",
      "Training: Epoch[024/030] Iteration[420/501] Loss: 8.9062\n",
      "Training: Epoch[024/030] Iteration[430/501] Loss: 6.6671\n",
      "Training: Epoch[024/030] Iteration[440/501] Loss: 6.6667\n",
      "Training: Epoch[024/030] Iteration[450/501] Loss: 6.6840\n",
      "Training: Epoch[024/030] Iteration[460/501] Loss: 9.1289\n",
      "Training: Epoch[024/030] Iteration[470/501] Loss: 7.4281\n",
      "Training: Epoch[024/030] Iteration[480/501] Loss: 6.4434\n",
      "Training: Epoch[024/030] Iteration[490/501] Loss: 7.8306\n",
      "Training: Epoch[024/030] Iteration[500/501] Loss: 6.4469\n",
      "Valid: Epoch[024/030] Iteration[501/501] Loss: 9.4302\n",
      "Training: Epoch[025/030] Iteration[010/501] Loss: 8.8436\n",
      "Training: Epoch[025/030] Iteration[020/501] Loss: 8.0887\n",
      "Training: Epoch[025/030] Iteration[030/501] Loss: 9.7063\n",
      "Training: Epoch[025/030] Iteration[040/501] Loss: 5.5532\n",
      "Training: Epoch[025/030] Iteration[050/501] Loss: 8.6121\n",
      "Training: Epoch[025/030] Iteration[060/501] Loss: 10.1388\n",
      "Training: Epoch[025/030] Iteration[070/501] Loss: 8.8213\n",
      "Training: Epoch[025/030] Iteration[080/501] Loss: 8.2600\n",
      "Training: Epoch[025/030] Iteration[090/501] Loss: 6.6929\n",
      "Training: Epoch[025/030] Iteration[100/501] Loss: 6.4071\n",
      "Training: Epoch[025/030] Iteration[110/501] Loss: 6.8963\n",
      "Training: Epoch[025/030] Iteration[120/501] Loss: 7.0919\n",
      "Training: Epoch[025/030] Iteration[130/501] Loss: 8.0077\n",
      "Training: Epoch[025/030] Iteration[140/501] Loss: 9.0049\n",
      "Training: Epoch[025/030] Iteration[150/501] Loss: 7.1378\n",
      "Training: Epoch[025/030] Iteration[160/501] Loss: 7.4605\n",
      "Training: Epoch[025/030] Iteration[170/501] Loss: 7.1290\n",
      "Training: Epoch[025/030] Iteration[180/501] Loss: 6.7575\n",
      "Training: Epoch[025/030] Iteration[190/501] Loss: 9.3984\n",
      "Training: Epoch[025/030] Iteration[200/501] Loss: 7.9818\n",
      "Training: Epoch[025/030] Iteration[210/501] Loss: 4.6584\n",
      "Training: Epoch[025/030] Iteration[220/501] Loss: 7.2267\n",
      "Training: Epoch[025/030] Iteration[230/501] Loss: 6.5561\n",
      "Training: Epoch[025/030] Iteration[240/501] Loss: 6.5821\n",
      "Training: Epoch[025/030] Iteration[250/501] Loss: 9.3309\n",
      "Training: Epoch[025/030] Iteration[260/501] Loss: 5.9868\n",
      "Training: Epoch[025/030] Iteration[270/501] Loss: 7.9649\n",
      "Training: Epoch[025/030] Iteration[280/501] Loss: 4.8738\n",
      "Training: Epoch[025/030] Iteration[290/501] Loss: 6.8558\n",
      "Training: Epoch[025/030] Iteration[300/501] Loss: 9.5660\n",
      "Training: Epoch[025/030] Iteration[310/501] Loss: 5.6888\n",
      "Training: Epoch[025/030] Iteration[320/501] Loss: 6.6394\n",
      "Training: Epoch[025/030] Iteration[330/501] Loss: 6.7939\n",
      "Training: Epoch[025/030] Iteration[340/501] Loss: 6.6651\n",
      "Training: Epoch[025/030] Iteration[350/501] Loss: 7.9763\n",
      "Training: Epoch[025/030] Iteration[360/501] Loss: 8.6074\n",
      "Training: Epoch[025/030] Iteration[370/501] Loss: 7.0621\n",
      "Training: Epoch[025/030] Iteration[380/501] Loss: 6.1377\n",
      "Training: Epoch[025/030] Iteration[390/501] Loss: 9.4849\n",
      "Training: Epoch[025/030] Iteration[400/501] Loss: 8.8548\n",
      "Training: Epoch[025/030] Iteration[410/501] Loss: 7.7138\n",
      "Training: Epoch[025/030] Iteration[420/501] Loss: 6.2852\n",
      "Training: Epoch[025/030] Iteration[430/501] Loss: 7.8468\n",
      "Training: Epoch[025/030] Iteration[440/501] Loss: 10.1127\n",
      "Training: Epoch[025/030] Iteration[450/501] Loss: 6.9635\n",
      "Training: Epoch[025/030] Iteration[460/501] Loss: 7.8799\n",
      "Training: Epoch[025/030] Iteration[470/501] Loss: 8.5364\n",
      "Training: Epoch[025/030] Iteration[480/501] Loss: 8.7312\n",
      "Training: Epoch[025/030] Iteration[490/501] Loss: 9.4025\n",
      "Training: Epoch[025/030] Iteration[500/501] Loss: 7.7755\n",
      "Valid: Epoch[025/030] Iteration[501/501] Loss: 9.3027\n",
      "Training: Epoch[026/030] Iteration[010/501] Loss: 8.2330\n",
      "Training: Epoch[026/030] Iteration[020/501] Loss: 7.6905\n",
      "Training: Epoch[026/030] Iteration[030/501] Loss: 6.6329\n",
      "Training: Epoch[026/030] Iteration[040/501] Loss: 7.1910\n",
      "Training: Epoch[026/030] Iteration[050/501] Loss: 7.5011\n",
      "Training: Epoch[026/030] Iteration[060/501] Loss: 7.1823\n",
      "Training: Epoch[026/030] Iteration[070/501] Loss: 7.5534\n",
      "Training: Epoch[026/030] Iteration[080/501] Loss: 5.5725\n",
      "Training: Epoch[026/030] Iteration[090/501] Loss: 7.4000\n",
      "Training: Epoch[026/030] Iteration[100/501] Loss: 6.2531\n",
      "Training: Epoch[026/030] Iteration[110/501] Loss: 5.3854\n",
      "Training: Epoch[026/030] Iteration[120/501] Loss: 7.6877\n",
      "Training: Epoch[026/030] Iteration[130/501] Loss: 10.2557\n",
      "Training: Epoch[026/030] Iteration[140/501] Loss: 6.3884\n",
      "Training: Epoch[026/030] Iteration[150/501] Loss: 7.1594\n",
      "Training: Epoch[026/030] Iteration[160/501] Loss: 9.3513\n",
      "Training: Epoch[026/030] Iteration[170/501] Loss: 5.8327\n",
      "Training: Epoch[026/030] Iteration[180/501] Loss: 7.3343\n",
      "Training: Epoch[026/030] Iteration[190/501] Loss: 7.3047\n",
      "Training: Epoch[026/030] Iteration[200/501] Loss: 10.0555\n",
      "Training: Epoch[026/030] Iteration[210/501] Loss: 6.4838\n",
      "Training: Epoch[026/030] Iteration[220/501] Loss: 6.3933\n",
      "Training: Epoch[026/030] Iteration[230/501] Loss: 8.2151\n",
      "Training: Epoch[026/030] Iteration[240/501] Loss: 7.9472\n",
      "Training: Epoch[026/030] Iteration[250/501] Loss: 6.7150\n",
      "Training: Epoch[026/030] Iteration[260/501] Loss: 7.9434\n",
      "Training: Epoch[026/030] Iteration[270/501] Loss: 7.9404\n",
      "Training: Epoch[026/030] Iteration[280/501] Loss: 6.6756\n",
      "Training: Epoch[026/030] Iteration[290/501] Loss: 10.6932\n",
      "Training: Epoch[026/030] Iteration[300/501] Loss: 6.6263\n",
      "Training: Epoch[026/030] Iteration[310/501] Loss: 9.2353\n",
      "Training: Epoch[026/030] Iteration[320/501] Loss: 6.9390\n",
      "Training: Epoch[026/030] Iteration[330/501] Loss: 8.6242\n",
      "Training: Epoch[026/030] Iteration[340/501] Loss: 7.7801\n",
      "Training: Epoch[026/030] Iteration[350/501] Loss: 7.9444\n",
      "Training: Epoch[026/030] Iteration[360/501] Loss: 10.3698\n",
      "Training: Epoch[026/030] Iteration[370/501] Loss: 6.4058\n",
      "Training: Epoch[026/030] Iteration[380/501] Loss: 12.2936\n",
      "Training: Epoch[026/030] Iteration[390/501] Loss: 6.7110\n",
      "Training: Epoch[026/030] Iteration[400/501] Loss: 7.1487\n",
      "Training: Epoch[026/030] Iteration[410/501] Loss: 8.7335\n",
      "Training: Epoch[026/030] Iteration[420/501] Loss: 6.5207\n",
      "Training: Epoch[026/030] Iteration[430/501] Loss: 9.3045\n",
      "Training: Epoch[026/030] Iteration[440/501] Loss: 8.5279\n",
      "Training: Epoch[026/030] Iteration[450/501] Loss: 5.9539\n",
      "Training: Epoch[026/030] Iteration[460/501] Loss: 9.0378\n",
      "Training: Epoch[026/030] Iteration[470/501] Loss: 8.5559\n",
      "Training: Epoch[026/030] Iteration[480/501] Loss: 7.2766\n",
      "Training: Epoch[026/030] Iteration[490/501] Loss: 7.8101\n",
      "Training: Epoch[026/030] Iteration[500/501] Loss: 5.6256\n",
      "Valid: Epoch[026/030] Iteration[501/501] Loss: 9.3802\n",
      "Training: Epoch[027/030] Iteration[010/501] Loss: 8.0698\n",
      "Training: Epoch[027/030] Iteration[020/501] Loss: 8.5726\n",
      "Training: Epoch[027/030] Iteration[030/501] Loss: 7.0562\n",
      "Training: Epoch[027/030] Iteration[040/501] Loss: 6.9414\n",
      "Training: Epoch[027/030] Iteration[050/501] Loss: 7.9779\n",
      "Training: Epoch[027/030] Iteration[060/501] Loss: 8.2748\n",
      "Training: Epoch[027/030] Iteration[070/501] Loss: 6.7731\n",
      "Training: Epoch[027/030] Iteration[080/501] Loss: 9.5387\n",
      "Training: Epoch[027/030] Iteration[090/501] Loss: 8.2183\n",
      "Training: Epoch[027/030] Iteration[100/501] Loss: 7.5594\n",
      "Training: Epoch[027/030] Iteration[110/501] Loss: 5.8570\n",
      "Training: Epoch[027/030] Iteration[120/501] Loss: 9.9011\n",
      "Training: Epoch[027/030] Iteration[130/501] Loss: 7.3520\n",
      "Training: Epoch[027/030] Iteration[140/501] Loss: 8.3175\n",
      "Training: Epoch[027/030] Iteration[150/501] Loss: 10.3692\n",
      "Training: Epoch[027/030] Iteration[160/501] Loss: 7.0746\n",
      "Training: Epoch[027/030] Iteration[170/501] Loss: 8.8956\n",
      "Training: Epoch[027/030] Iteration[180/501] Loss: 7.4316\n",
      "Training: Epoch[027/030] Iteration[190/501] Loss: 7.6840\n",
      "Training: Epoch[027/030] Iteration[200/501] Loss: 6.6257\n",
      "Training: Epoch[027/030] Iteration[210/501] Loss: 8.1528\n",
      "Training: Epoch[027/030] Iteration[220/501] Loss: 6.9743\n",
      "Training: Epoch[027/030] Iteration[230/501] Loss: 8.0393\n",
      "Training: Epoch[027/030] Iteration[240/501] Loss: 6.7038\n",
      "Training: Epoch[027/030] Iteration[250/501] Loss: 8.6541\n",
      "Training: Epoch[027/030] Iteration[260/501] Loss: 7.9171\n",
      "Training: Epoch[027/030] Iteration[270/501] Loss: 6.7569\n",
      "Training: Epoch[027/030] Iteration[280/501] Loss: 6.5659\n",
      "Training: Epoch[027/030] Iteration[290/501] Loss: 11.7050\n",
      "Training: Epoch[027/030] Iteration[300/501] Loss: 8.5423\n",
      "Training: Epoch[027/030] Iteration[310/501] Loss: 6.5827\n",
      "Training: Epoch[027/030] Iteration[320/501] Loss: 8.3092\n",
      "Training: Epoch[027/030] Iteration[330/501] Loss: 6.8284\n",
      "Training: Epoch[027/030] Iteration[340/501] Loss: 6.4772\n",
      "Training: Epoch[027/030] Iteration[350/501] Loss: 8.7688\n",
      "Training: Epoch[027/030] Iteration[360/501] Loss: 7.8840\n",
      "Training: Epoch[027/030] Iteration[370/501] Loss: 7.3381\n",
      "Training: Epoch[027/030] Iteration[380/501] Loss: 6.3146\n",
      "Training: Epoch[027/030] Iteration[390/501] Loss: 6.4974\n",
      "Training: Epoch[027/030] Iteration[400/501] Loss: 8.0956\n",
      "Training: Epoch[027/030] Iteration[410/501] Loss: 5.4339\n",
      "Training: Epoch[027/030] Iteration[420/501] Loss: 6.7178\n",
      "Training: Epoch[027/030] Iteration[430/501] Loss: 7.1876\n",
      "Training: Epoch[027/030] Iteration[440/501] Loss: 6.9185\n",
      "Training: Epoch[027/030] Iteration[450/501] Loss: 7.3438\n",
      "Training: Epoch[027/030] Iteration[460/501] Loss: 7.4788\n",
      "Training: Epoch[027/030] Iteration[470/501] Loss: 6.9910\n",
      "Training: Epoch[027/030] Iteration[480/501] Loss: 7.1007\n",
      "Training: Epoch[027/030] Iteration[490/501] Loss: 7.7569\n",
      "Training: Epoch[027/030] Iteration[500/501] Loss: 9.9869\n",
      "Valid: Epoch[027/030] Iteration[501/501] Loss: 9.5930\n",
      "Training: Epoch[028/030] Iteration[010/501] Loss: 7.1449\n",
      "Training: Epoch[028/030] Iteration[020/501] Loss: 9.9774\n",
      "Training: Epoch[028/030] Iteration[030/501] Loss: 6.9900\n",
      "Training: Epoch[028/030] Iteration[040/501] Loss: 9.0987\n",
      "Training: Epoch[028/030] Iteration[050/501] Loss: 8.8533\n",
      "Training: Epoch[028/030] Iteration[060/501] Loss: 8.3629\n",
      "Training: Epoch[028/030] Iteration[070/501] Loss: 9.3270\n",
      "Training: Epoch[028/030] Iteration[080/501] Loss: 7.9956\n",
      "Training: Epoch[028/030] Iteration[090/501] Loss: 7.0191\n",
      "Training: Epoch[028/030] Iteration[100/501] Loss: 8.8390\n",
      "Training: Epoch[028/030] Iteration[110/501] Loss: 7.1989\n",
      "Training: Epoch[028/030] Iteration[120/501] Loss: 7.3266\n",
      "Training: Epoch[028/030] Iteration[130/501] Loss: 7.9930\n",
      "Training: Epoch[028/030] Iteration[140/501] Loss: 8.6940\n",
      "Training: Epoch[028/030] Iteration[150/501] Loss: 5.3495\n",
      "Training: Epoch[028/030] Iteration[160/501] Loss: 7.3928\n",
      "Training: Epoch[028/030] Iteration[170/501] Loss: 6.8255\n",
      "Training: Epoch[028/030] Iteration[180/501] Loss: 6.1908\n",
      "Training: Epoch[028/030] Iteration[190/501] Loss: 5.8458\n",
      "Training: Epoch[028/030] Iteration[200/501] Loss: 8.3429\n",
      "Training: Epoch[028/030] Iteration[210/501] Loss: 6.8952\n",
      "Training: Epoch[028/030] Iteration[220/501] Loss: 8.0488\n",
      "Training: Epoch[028/030] Iteration[230/501] Loss: 6.3410\n",
      "Training: Epoch[028/030] Iteration[240/501] Loss: 8.3950\n",
      "Training: Epoch[028/030] Iteration[250/501] Loss: 7.1148\n",
      "Training: Epoch[028/030] Iteration[260/501] Loss: 8.0948\n",
      "Training: Epoch[028/030] Iteration[270/501] Loss: 6.7513\n",
      "Training: Epoch[028/030] Iteration[280/501] Loss: 8.7235\n",
      "Training: Epoch[028/030] Iteration[290/501] Loss: 9.2269\n",
      "Training: Epoch[028/030] Iteration[300/501] Loss: 8.3365\n",
      "Training: Epoch[028/030] Iteration[310/501] Loss: 5.9949\n",
      "Training: Epoch[028/030] Iteration[320/501] Loss: 9.1717\n",
      "Training: Epoch[028/030] Iteration[330/501] Loss: 8.6279\n",
      "Training: Epoch[028/030] Iteration[340/501] Loss: 6.2250\n",
      "Training: Epoch[028/030] Iteration[350/501] Loss: 7.4418\n",
      "Training: Epoch[028/030] Iteration[360/501] Loss: 9.3268\n",
      "Training: Epoch[028/030] Iteration[370/501] Loss: 6.8160\n",
      "Training: Epoch[028/030] Iteration[380/501] Loss: 6.1631\n",
      "Training: Epoch[028/030] Iteration[390/501] Loss: 9.0364\n",
      "Training: Epoch[028/030] Iteration[400/501] Loss: 9.6485\n",
      "Training: Epoch[028/030] Iteration[410/501] Loss: 5.2291\n",
      "Training: Epoch[028/030] Iteration[420/501] Loss: 6.9256\n",
      "Training: Epoch[028/030] Iteration[430/501] Loss: 7.1542\n",
      "Training: Epoch[028/030] Iteration[440/501] Loss: 9.1409\n",
      "Training: Epoch[028/030] Iteration[450/501] Loss: 8.3652\n",
      "Training: Epoch[028/030] Iteration[460/501] Loss: 6.9117\n",
      "Training: Epoch[028/030] Iteration[470/501] Loss: 4.9629\n",
      "Training: Epoch[028/030] Iteration[480/501] Loss: 8.3435\n",
      "Training: Epoch[028/030] Iteration[490/501] Loss: 9.6185\n",
      "Training: Epoch[028/030] Iteration[500/501] Loss: 6.7350\n",
      "Valid: Epoch[028/030] Iteration[501/501] Loss: 9.9592\n",
      "Training: Epoch[029/030] Iteration[010/501] Loss: 6.4175\n",
      "Training: Epoch[029/030] Iteration[020/501] Loss: 8.7989\n",
      "Training: Epoch[029/030] Iteration[030/501] Loss: 7.5862\n",
      "Training: Epoch[029/030] Iteration[040/501] Loss: 7.6615\n",
      "Training: Epoch[029/030] Iteration[050/501] Loss: 9.6413\n",
      "Training: Epoch[029/030] Iteration[060/501] Loss: 9.5858\n",
      "Training: Epoch[029/030] Iteration[070/501] Loss: 9.7152\n",
      "Training: Epoch[029/030] Iteration[080/501] Loss: 7.1178\n",
      "Training: Epoch[029/030] Iteration[090/501] Loss: 7.0572\n",
      "Training: Epoch[029/030] Iteration[100/501] Loss: 7.9160\n",
      "Training: Epoch[029/030] Iteration[110/501] Loss: 8.3076\n",
      "Training: Epoch[029/030] Iteration[120/501] Loss: 8.3373\n",
      "Training: Epoch[029/030] Iteration[130/501] Loss: 6.7405\n",
      "Training: Epoch[029/030] Iteration[140/501] Loss: 7.6989\n",
      "Training: Epoch[029/030] Iteration[150/501] Loss: 8.7184\n",
      "Training: Epoch[029/030] Iteration[160/501] Loss: 5.6281\n",
      "Training: Epoch[029/030] Iteration[170/501] Loss: 10.0840\n",
      "Training: Epoch[029/030] Iteration[180/501] Loss: 9.1479\n",
      "Training: Epoch[029/030] Iteration[190/501] Loss: 6.0135\n",
      "Training: Epoch[029/030] Iteration[200/501] Loss: 6.0071\n",
      "Training: Epoch[029/030] Iteration[210/501] Loss: 6.2284\n",
      "Training: Epoch[029/030] Iteration[220/501] Loss: 7.3186\n",
      "Training: Epoch[029/030] Iteration[230/501] Loss: 7.7163\n",
      "Training: Epoch[029/030] Iteration[240/501] Loss: 7.1612\n",
      "Training: Epoch[029/030] Iteration[250/501] Loss: 9.1594\n",
      "Training: Epoch[029/030] Iteration[260/501] Loss: 6.9628\n",
      "Training: Epoch[029/030] Iteration[270/501] Loss: 7.1509\n",
      "Training: Epoch[029/030] Iteration[280/501] Loss: 7.5207\n",
      "Training: Epoch[029/030] Iteration[290/501] Loss: 6.2452\n",
      "Training: Epoch[029/030] Iteration[300/501] Loss: 10.4702\n",
      "Training: Epoch[029/030] Iteration[310/501] Loss: 8.1794\n",
      "Training: Epoch[029/030] Iteration[320/501] Loss: 5.5055\n",
      "Training: Epoch[029/030] Iteration[330/501] Loss: 8.8029\n",
      "Training: Epoch[029/030] Iteration[340/501] Loss: 6.7161\n",
      "Training: Epoch[029/030] Iteration[350/501] Loss: 6.7336\n",
      "Training: Epoch[029/030] Iteration[360/501] Loss: 6.4504\n",
      "Training: Epoch[029/030] Iteration[370/501] Loss: 9.1975\n",
      "Training: Epoch[029/030] Iteration[380/501] Loss: 8.6214\n",
      "Training: Epoch[029/030] Iteration[390/501] Loss: 9.7528\n",
      "Training: Epoch[029/030] Iteration[400/501] Loss: 5.7107\n",
      "Training: Epoch[029/030] Iteration[410/501] Loss: 5.8710\n",
      "Training: Epoch[029/030] Iteration[420/501] Loss: 6.1815\n",
      "Training: Epoch[029/030] Iteration[430/501] Loss: 7.6192\n",
      "Training: Epoch[029/030] Iteration[440/501] Loss: 7.7295\n",
      "Training: Epoch[029/030] Iteration[450/501] Loss: 7.4763\n",
      "Training: Epoch[029/030] Iteration[460/501] Loss: 6.3261\n",
      "Training: Epoch[029/030] Iteration[470/501] Loss: 8.9258\n",
      "Training: Epoch[029/030] Iteration[480/501] Loss: 6.1199\n",
      "Training: Epoch[029/030] Iteration[490/501] Loss: 9.8902\n",
      "Training: Epoch[029/030] Iteration[500/501] Loss: 6.5842\n",
      "Valid: Epoch[029/030] Iteration[501/501] Loss: 9.5940\n"
     ]
    }
   ],
   "source": [
    "train_curve = list() \n",
    "valid_curve = list()\n",
    "\n",
    "for epoch in range(start_epoch + 1, MAX_EPOCH):\n",
    "\n",
    "    loss_mean = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    res_model.train() # 将模型切换到训练模式，启用 dropout 等操作。\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # 遍历训练数据加载器，data 包含一个批次的 inputs（输入图像）和 labels（对应的标签）。\n",
    "\n",
    "        # forward\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = res_model(inputs)\n",
    "\n",
    "        # 调整标签的形状\n",
    "        labels = labels.view(-1, 1)  # 确保标签的形状与输出一致\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        # 清空上一批次的梯度，防止累积。\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # 通过自动求导计算梯度。\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # 使用优化器根据计算出的梯度更新模型参数。\n",
    "\n",
    "        # 打印训练信息\n",
    "        loss_mean += loss.item()        # 获取当前批次的损失值。\n",
    "        train_curve.append(loss.item()) # 将当前批次的损失值记录到 train_curve。\n",
    "        if (i+1) % log_interval == 0:\n",
    "            loss_mean = loss_mean / log_interval\n",
    "            print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f}\".format(\n",
    "                epoch, MAX_EPOCH, i+1, len(train_dataloader), loss_mean))\n",
    "            loss_mean = 0.\n",
    "\n",
    "    scheduler.step()  # 更新学习率 调用学习率调度器，根据设置调整当前学习率（如按一定步长下降）。\n",
    "\n",
    "    # validate the model\n",
    "    if (epoch+1) % val_interval == 0:\n",
    "\n",
    "        correct_val = 0.\n",
    "        total_val = 0.\n",
    "        loss_val = 0.\n",
    "        res_model.eval() \n",
    "        \n",
    "        # 禁用自动求导，减少内存占用，加速计算。\n",
    "        with torch.no_grad():\n",
    "            for j, data in enumerate(test_dataloader):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = res_model(inputs)\n",
    "               \n",
    "                # 调整标签的形状\n",
    "                labels = labels.view(-1, 1)  # 确保标签的形状与输出一致\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                loss_val += loss.item()\n",
    "\n",
    "            loss_val_mean = loss_val / len(test_dataloader)\n",
    "            valid_curve.append(loss_val_mean)\n",
    "            print(\"Valid: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f}\".format(\n",
    "                epoch, MAX_EPOCH, j+1, len(test_dataloader), loss_val_mean))\n",
    "\n",
    "        res_model.train()  # 将模型切换回训练模式\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a03f3c0f-2f70-4fa0-9b2f-e50b78bbf5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_x = range(len(train_curve))\n",
    "train_y = train_curve\n",
    "\n",
    "train_iters = len(train_dataloader)\n",
    "valid_x = np.arange(1, len(valid_curve)+1) * train_iters*val_interval # 由于valid中记录的是epochloss，需要对记录点进行转换到iterations\n",
    "valid_y = valid_curve\n",
    "\n",
    "plt.plot(train_x, train_y, label='Train')\n",
    "plt.plot(valid_x, valid_y, label='Valid')\n",
    "\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('loss value')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9da9e3-ee1b-4fb0-a3fe-0b18e117089a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
